{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dde9eb84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n",
      "Global seed set to 0\n",
      "WARNING:root:In order to use the mouse gastrulation seqFISH datsets, please install squidpy (see https://github.com/scverse/squidpy).\n",
      "WARNING:root:In order to use sagenet models, please install pytorch geometric (see https://pytorch-geometric.readthedocs.io) and \n",
      " captum (see https://github.com/pytorch/captum).\n",
      "WARNING:root:mvTCR is not installed. To use mvTCR models, please install it first using \"pip install mvtcr\"\n",
      "WARNING:root:multigrate is not installed. To use multigrate models, please install it first using \"pip install multigrate\".\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scanpy as sc\n",
    "import scvi\n",
    "import scarches as sca\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Normal, Poisson\n",
    "import torch.nn.functional as F\n",
    "# from dalib.modules import domain_discriminator\n",
    "# from scvi.nn._base_components import DecoderSCVI\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27300af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_latent = sc.read_h5ad(\"./reference_latent_sample.h5ad\")\n",
    "# query_adata = sc.read_h5ad(\"/home/wyh/liver_atlas/data/Aizarani2019/Aizarani2019_plot_V2.h5ad\")\n",
    "query_adata = sc.read_h5ad(\"/home/wyh/liver_atlas/data/Ramachandran2019/Ramachandran2019_plot_V2.h5ad\")\n",
    "# ref_path = \"./scvi_ref_model/\"\n",
    "ref_path = \"./scvi_ref_model_sample/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67cdf30a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 62210 × 33694\n",
       "    obs: 'orig.ident', 'nCount_RNA', 'nFeature_RNA', 'Cell_ID', 'Sample_ID', 'Original_name_global', 'Original_name_sub', 'Patient_ID', 'Sample_status_original', 'Seq_tech', 'GEO_accession', 'Sample_status', 'Source', 'Gender', 'Age', 'Disease', 'Fibrotic_status', 'Author', 'Aetiology_of_Liver_Disease', 'level1', 'batch', 'leiden'\n",
       "    var: 'highly_variable', 'means', 'dispersions', 'dispersions_norm'\n",
       "    uns: 'hvg', 'leiden', 'level1_colors', 'log1p', 'neighbors', 'pca', 'umap'\n",
       "    obsm: 'X_pca', 'X_umap'\n",
       "    layers: 'counts'\n",
       "    obsp: 'connectivities', 'distances'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e726bd48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2499457/4234022668.py:16: FutureWarning: X.dtype being converted to np.float32 from float64. In the next version of anndata (0.9) conversion will not be automatic. Pass dtype explicitly to avoid this warning. Pass `AnnData(X, dtype=X.dtype, ...)` to get the future behavour.\n",
      "  test_adata = sc.AnnData(new_mtx)\n",
      "/tmp/ipykernel_2499457/4234022668.py:28: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n",
      "  query_adata.obs['sample'] = query_adata.obs['batch']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mINFO    \u001b[0m Using data from adata.layers\u001b[1m[\u001b[0m\u001b[32m\"counts\"\u001b[0m\u001b[1m]\u001b[0m                                              \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wyh/anaconda3/envs/cell2loc_env/lib/python3.9/site-packages/scvi/model/base/_archesmixin.py:95: UserWarning: Query integration should be performed using models trained with version >= 0.8\n",
      "  warnings.warn(\n",
      "/home/wyh/anaconda3/envs/cell2loc_env/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function transfer_anndata_setup is deprecated; This method will be removed in 0.15.0. Please avoid building any new dependencies on it.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "INFO:scvi.data._anndata:Using data from adata.layers[\"counts\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mINFO    \u001b[0m Registered keys:\u001b[1m[\u001b[0m\u001b[32m'X'\u001b[0m, \u001b[32m'batch_indices'\u001b[0m, \u001b[32m'labels'\u001b[0m\u001b[1m]\u001b[0m                                    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:scvi.data._anndata:Registered keys:['X', 'batch_indices', 'labels']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mINFO    \u001b[0m Successfully registered anndata object containing \u001b[1;36m62210\u001b[0m cells, \u001b[1;36m2000\u001b[0m vars, \u001b[1;36m39\u001b[0m        \n",
      "         batches, \u001b[1;36m10\u001b[0m labels, and \u001b[1;36m0\u001b[0m proteins. Also registered \u001b[1;36m0\u001b[0m extra categorical covariates  \n",
      "         and \u001b[1;36m0\u001b[0m extra continuous covariates.                                                  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:scvi.data._anndata:Successfully registered anndata object containing 62210 cells, 2000 vars, 39 batches, 10 labels, and 0 proteins. Also registered 0 extra categorical covariates and 0 extra continuous covariates.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Training: 0it [00:00, ?it/s]"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "def feature_alignment(test_set, gene_list):\n",
    "    test_set.X = test_set.layers['counts']\n",
    "    \n",
    "    # set test_set features as gene_list, zero-filling for missing features\n",
    "    selected_set = set(gene_list)\n",
    "    test_set_genes = set(test_set.var_names)\n",
    "    common_set = selected_set & test_set_genes\n",
    "    gene_extra = selected_set - common_set\n",
    "    n_extra = len(gene_extra)\n",
    "    if n_extra / len(gene_list) > 0.05:\n",
    "        print(\"Warning: %d features not exist in testset.\" % len(gene_extra))\n",
    "\n",
    "    if n_extra > 0:  # fill zeros for missing features\n",
    "        new_mtx = csr_matrix(test_set.X, shape=(test_set.n_obs, test_set.n_vars + n_extra))\n",
    "        test_adata = sc.AnnData(new_mtx)\n",
    "        test_adata.obs = test_set.obs\n",
    "        test_adata.layers['counts'] = test_adata.X\n",
    "        test_adata.obs_names = test_set.obs_names\n",
    "        test_adata.var_names = list(test_set.var_names) + list(gene_extra)\n",
    "        # test_adata.obs = test_set.obs\n",
    "        return test_adata[:, gene_list]\n",
    "    else:\n",
    "        return test_set[:, gene_list]\n",
    "    \n",
    "gene_selected = pd.read_csv(ref_path + \"var_names.csv\", header=None)[0].tolist()\n",
    "query_adata = feature_alignment(query_adata, gene_selected)\n",
    "query_adata.obs['sample'] = query_adata.obs['batch']\n",
    "\n",
    "scvi_model = sca.models.SCVI.load_query_data(\n",
    "    query_adata,\n",
    "    ref_path,\n",
    "    freeze_dropout = True,\n",
    ")\n",
    "\n",
    "scvi_model.train(max_epochs=0, plan_kwargs=dict(weight_decay=0.0))\n",
    "# scvi.data.view_anndata_setup(scvi_model.adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63201d81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wyh/anaconda3/envs/cell2loc_env/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function view_anndata_setup is deprecated; This method will be removed in 0.15.0. Please avoid building any new dependencies on it.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Anndata setup with scvi-tools version <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.14</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Anndata setup with scvi-tools version \u001b[1;36m0.14\u001b[0m.\u001b[1;36m6\u001b[0m.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">              Data Summary              </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">             Data             </span>┃<span style=\"font-weight: bold\"> Count </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">            Cells             </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\"> 62210 </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">             Vars             </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\"> 2000  </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">            Labels            </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">  10   </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">           Batches            </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">  39   </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">           Proteins           </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">   0   </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\"> Extra Categorical Covariates </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">   0   </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\"> Extra Continuous Covariates  </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">   0   </span>│\n",
       "└──────────────────────────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m              Data Summary              \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m            Data            \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mCount\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m           Cells            \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m62210\u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m            Vars            \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m2000 \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m           Labels           \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m 10  \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m          Batches           \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m 39  \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m          Proteins          \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m  0  \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33mExtra Categorical Covariates\u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m  0  \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33mExtra Continuous Covariates \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m  0  \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "└──────────────────────────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">             SCVI Data Registry              </span>\n",
       "┏━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">     Data      </span>┃<span style=\"font-weight: bold\">    scvi-tools Location    </span>┃\n",
       "┡━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">       X       </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">  adata.layers['counts']   </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\"> batch_indices </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\"> adata.obs['_scvi_batch']  </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">    labels     </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\"> adata.obs['_scvi_labels'] </span>│\n",
       "└───────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m             SCVI Data Registry              \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m    Data     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   scvi-tools Location   \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m      X      \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m adata.layers['counts']  \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33mbatch_indices\u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128madata.obs['_scvi_batch'] \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m   labels    \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128madata.obs['_scvi_labels']\u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "└───────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                        Label Categories                        </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">   Source Location   </span>┃<span style=\"font-weight: bold\">    Categories    </span>┃<span style=\"font-weight: bold\"> scvi-tools Encoding </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\"> adata.obs['level1'] </span>│<span style=\"color: #008000; text-decoration-color: #008000\">      B cell      </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">          0          </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">                     </span>│<span style=\"color: #008000; text-decoration-color: #008000\">  Cholangiocyte   </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">          1          </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">                     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> Endothelial cell </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">          2          </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">                     </span>│<span style=\"color: #008000; text-decoration-color: #008000\">    Hepatocyte    </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">          3          </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">                     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> Mesenchymal cell </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">          4          </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">                     </span>│<span style=\"color: #008000; text-decoration-color: #008000\">   Myeloid cell   </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">          5          </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">                     </span>│<span style=\"color: #008000; text-decoration-color: #008000\">    NK/T cell     </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">          6          </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">                     </span>│<span style=\"color: #008000; text-decoration-color: #008000\">  Plasma B cell   </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">          7          </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">                     </span>│<span style=\"color: #008000; text-decoration-color: #008000\">     Cycling      </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">          8          </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">                     </span>│<span style=\"color: #008000; text-decoration-color: #008000\">     Unknown      </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">          9          </span>│\n",
       "└─────────────────────┴──────────────────┴─────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                        Label Categories                        \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m  Source Location  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Categories   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mscvi-tools Encoding\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33madata.obs['level1']\u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m     B cell     \u001b[0m\u001b[32m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m         0         \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m                   \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m Cholangiocyte  \u001b[0m\u001b[32m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m         1         \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m                   \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mEndothelial cell\u001b[0m\u001b[32m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m         2         \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m                   \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m   Hepatocyte   \u001b[0m\u001b[32m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m         3         \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m                   \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mMesenchymal cell\u001b[0m\u001b[32m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m         4         \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m                   \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m  Myeloid cell  \u001b[0m\u001b[32m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m         5         \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m                   \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m   NK/T cell    \u001b[0m\u001b[32m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m         6         \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m                   \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m Plasma B cell  \u001b[0m\u001b[32m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m         7         \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m                   \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m    Cycling     \u001b[0m\u001b[32m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m         8         \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m                   \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m    Unknown     \u001b[0m\u001b[32m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m         9         \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "└─────────────────────┴──────────────────┴─────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                        Batch Categories                        </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">   Source Location   </span>┃<span style=\"font-weight: bold\">    Categories    </span>┃<span style=\"font-weight: bold\"> scvi-tools Encoding </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\"> adata.obs['sample'] </span>│<span style=\"color: #008000; text-decoration-color: #008000\">       ABU8       </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">          0          </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">                     </span>│<span style=\"color: #008000; text-decoration-color: #008000\">      CISE06      </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">          1          </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">                     </span>│<span style=\"color: #008000; text-decoration-color: #008000\">      CISE07      </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">          2          </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">                     </span>│<span style=\"color: #008000; text-decoration-color: #008000\">      CISE08      </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">          3          </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">                     </span>│<span style=\"color: #008000; text-decoration-color: #008000\">      CISE09      </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">          4          </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">                     </span>│<span style=\"color: #008000; text-decoration-color: #008000\">       CS31       </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">          5          </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">                     </span>│<span style=\"color: #008000; text-decoration-color: #008000\">       CS32       </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">          6          </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">                     </span>│<span style=\"color: #008000; text-decoration-color: #008000\">       CS33       </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">          7          </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">                     </span>│<span style=\"color: #008000; text-decoration-color: #008000\">       CS34       </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">          8          </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">                     </span>│<span style=\"color: #008000; text-decoration-color: #008000\">       CS37       </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">          9          </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">                     </span>│<span style=\"color: #008000; text-decoration-color: #008000\">       CS38       </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">         10          </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">                     </span>│<span style=\"color: #008000; text-decoration-color: #008000\">       CS41       </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">         11          </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">                     </span>│<span style=\"color: #008000; text-decoration-color: #008000\">       CS42       </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">         12          </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">                     </span>│<span style=\"color: #008000; text-decoration-color: #008000\">       CS43       </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">         13          </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">                     </span>│<span style=\"color: #008000; text-decoration-color: #008000\">       CS44       </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">         14          </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">                     </span>│<span style=\"color: #008000; text-decoration-color: #008000\">       CS46       </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">         15          </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">                     </span>│<span style=\"color: #008000; text-decoration-color: #008000\">       CS71       </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">         16          </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">                     </span>│<span style=\"color: #008000; text-decoration-color: #008000\">       CS73       </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">         17          </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">                     </span>│<span style=\"color: #008000; text-decoration-color: #008000\">       CS81       </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">         18          </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">                     </span>│<span style=\"color: #008000; text-decoration-color: #008000\">       CS83       </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">         19          </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">                     </span>│<span style=\"color: #008000; text-decoration-color: #008000\">       CS85       </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">         20          </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">                     </span>│<span style=\"color: #008000; text-decoration-color: #008000\">       CS87       </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">         21          </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">                     </span>│<span style=\"color: #008000; text-decoration-color: #008000\">      CS101       </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">         22          </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">                     </span>│<span style=\"color: #008000; text-decoration-color: #008000\">      CS108       </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">         23          </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">                     </span>│<span style=\"color: #008000; text-decoration-color: #008000\">      CS109       </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">         24          </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">                     </span>│<span style=\"color: #008000; text-decoration-color: #008000\">      CS110       </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">         25          </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">                     </span>│<span style=\"color: #008000; text-decoration-color: #008000\">      CS111       </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">         26          </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">                     </span>│<span style=\"color: #008000; text-decoration-color: #008000\">      CS112       </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">         27          </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">                     </span>│<span style=\"color: #008000; text-decoration-color: #008000\">      CS126       </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">         28          </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">                     </span>│<span style=\"color: #008000; text-decoration-color: #008000\">      CS127       </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">         29          </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">                     </span>│<span style=\"color: #008000; text-decoration-color: #008000\">      CS161       </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">         30          </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">                     </span>│<span style=\"color: #008000; text-decoration-color: #008000\">      CS162       </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">         31          </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">                     </span>│<span style=\"color: #008000; text-decoration-color: #008000\">      CS164       </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">         32          </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">                     </span>│<span style=\"color: #008000; text-decoration-color: #008000\">      CS166       </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">         33          </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">                     </span>│<span style=\"color: #008000; text-decoration-color: #008000\">      CS167       </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">         34          </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">                     </span>│<span style=\"color: #008000; text-decoration-color: #008000\">      CS169       </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">         35          </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">                     </span>│<span style=\"color: #008000; text-decoration-color: #008000\">      CS170       </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">         36          </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">                     </span>│<span style=\"color: #008000; text-decoration-color: #008000\">      CS171       </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">         37          </span>│\n",
       "│<span style=\"color: #0087ff; text-decoration-color: #0087ff\">                     </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> Ramachandran2019 </span>│<span style=\"color: #af00d7; text-decoration-color: #af00d7\">         38          </span>│\n",
       "└─────────────────────┴──────────────────┴─────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                        Batch Categories                        \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m  Source Location  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Categories   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mscvi-tools Encoding\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33madata.obs['sample']\u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m      ABU8      \u001b[0m\u001b[32m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m         0         \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m                   \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m     CISE06     \u001b[0m\u001b[32m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m         1         \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m                   \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m     CISE07     \u001b[0m\u001b[32m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m         2         \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m                   \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m     CISE08     \u001b[0m\u001b[32m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m         3         \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m                   \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m     CISE09     \u001b[0m\u001b[32m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m         4         \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m                   \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m      CS31      \u001b[0m\u001b[32m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m         5         \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m                   \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m      CS32      \u001b[0m\u001b[32m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m         6         \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m                   \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m      CS33      \u001b[0m\u001b[32m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m         7         \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m                   \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m      CS34      \u001b[0m\u001b[32m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m         8         \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m                   \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m      CS37      \u001b[0m\u001b[32m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m         9         \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m                   \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m      CS38      \u001b[0m\u001b[32m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m        10         \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m                   \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m      CS41      \u001b[0m\u001b[32m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m        11         \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m                   \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m      CS42      \u001b[0m\u001b[32m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m        12         \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m                   \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m      CS43      \u001b[0m\u001b[32m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m        13         \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m                   \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m      CS44      \u001b[0m\u001b[32m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m        14         \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m                   \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m      CS46      \u001b[0m\u001b[32m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m        15         \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m                   \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m      CS71      \u001b[0m\u001b[32m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m        16         \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m                   \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m      CS73      \u001b[0m\u001b[32m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m        17         \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m                   \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m      CS81      \u001b[0m\u001b[32m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m        18         \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m                   \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m      CS83      \u001b[0m\u001b[32m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m        19         \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m                   \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m      CS85      \u001b[0m\u001b[32m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m        20         \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m                   \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m      CS87      \u001b[0m\u001b[32m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m        21         \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m                   \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m     CS101      \u001b[0m\u001b[32m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m        22         \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m                   \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m     CS108      \u001b[0m\u001b[32m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m        23         \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m                   \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m     CS109      \u001b[0m\u001b[32m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m        24         \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m                   \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m     CS110      \u001b[0m\u001b[32m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m        25         \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m                   \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m     CS111      \u001b[0m\u001b[32m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m        26         \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m                   \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m     CS112      \u001b[0m\u001b[32m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m        27         \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m                   \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m     CS126      \u001b[0m\u001b[32m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m        28         \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m                   \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m     CS127      \u001b[0m\u001b[32m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m        29         \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m                   \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m     CS161      \u001b[0m\u001b[32m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m        30         \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m                   \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m     CS162      \u001b[0m\u001b[32m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m        31         \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m                   \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m     CS164      \u001b[0m\u001b[32m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m        32         \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m                   \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m     CS166      \u001b[0m\u001b[32m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m        33         \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m                   \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m     CS167      \u001b[0m\u001b[32m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m        34         \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m                   \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m     CS169      \u001b[0m\u001b[32m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m        35         \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m                   \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m     CS170      \u001b[0m\u001b[32m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m        36         \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m                   \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m     CS171      \u001b[0m\u001b[32m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m        37         \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "│\u001b[38;5;33m \u001b[0m\u001b[38;5;33m                   \u001b[0m\u001b[38;5;33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mRamachandran2019\u001b[0m\u001b[32m \u001b[0m│\u001b[38;5;128m \u001b[0m\u001b[38;5;128m        38         \u001b[0m\u001b[38;5;128m \u001b[0m│\n",
       "└─────────────────────┴──────────────────┴─────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scvi.data.view_anndata_setup(scvi_model.adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10ecb6a1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_id = scvi_model.adata.obs['_scvi_batch'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db753c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_origin = sc.AnnData(scvi_model.get_latent_representation())\n",
    "latent_origin.obs['cell_type'] = scvi_model.adata.obs['level1'].tolist()\n",
    "latent_origin.obs['batch'] = scvi_model.adata.obs['batch'].tolist()\n",
    "latent_origin.write(\"ramachandran_latent_origin.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df562671",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransferMappingModule(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_latent=10, \n",
    "        n_gene=2000, \n",
    "        n_hidden=[64, 32],\n",
    "        dropout_rate=0.1,\n",
    "        extra_decoder_kwargs=None\n",
    "    ):\n",
    "        \n",
    "        \n",
    "        super(TransferMappingModule, self).__init__()\n",
    "        \n",
    "#         # decoder (froze param)\n",
    "#         self.decoder = Decoder(\n",
    "#             n_latent=n_latent, \n",
    "#             n_out=n_gene, \n",
    "#         )\n",
    "\n",
    "        # transfer\n",
    "        self.fc11 = nn.Linear(n_latent, n_hidden[0])\n",
    "        self.fc12 = nn.Linear(n_hidden[0], n_hidden[1])      \n",
    "        self.fc2 = nn.Linear(n_hidden[1], n_latent)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        \n",
    "    def query_transfer(self, z):\n",
    "        h = F.relu(self.fc11(z))\n",
    "        h = self.dropout(h)\n",
    "        h = F.relu(self.fc12(h))\n",
    "        h = self.dropout(h)\n",
    "        return self.fc2(h)\n",
    "    \n",
    "#     def generative(self, z):\n",
    "#         categorical_input = ()\n",
    "#         size_factor = None\n",
    "# #         px_scale, px_r, px_rate, px_dropout = self.decoder(\n",
    "# #                 self.dispersion,\n",
    "# #                 self.query_transfer(z),\n",
    "# #                 size_factor,\n",
    "# #                 *categorical_input,\n",
    "# #                 y,\n",
    "# #             )\n",
    "# #         px = NegativeBinomial(mu=px_rate, theta=px_r, scale=px_scale)\n",
    "#         px_scale, px_dropout = self.decoder(z)\n",
    "#         poisson = Poisson(torch.exp(library) * px_scale + self.eps)\n",
    "#         decoder_poisson = Poisson(px_scale + self.eps)\n",
    "#         return px\n",
    "    \n",
    "    def forward(self, z):\n",
    "        return self.query_transfer(z)\n",
    "\n",
    "# 定义discriminator类\n",
    "class DomainDiscriminator(nn.Module):\n",
    "    def __init__(self, input_dim=30, hidden_dim=64, dropout_rate=0.1):\n",
    "        super(DomainDiscriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, dropout_rate=0.1):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return self.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fcceda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 自定义dataset类\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, adata, X_key):\n",
    "        self.data = adata\n",
    "        if X_key == \"X\":\n",
    "            self.X = adata.X\n",
    "        else:\n",
    "            self.X = adata.layers[X_key]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data.n_obs\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx,:].toarray())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9496d6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(discriminator, ref_latent, query_latent, mode):\n",
    "    if mode == \"trainD\":\n",
    "        ref_labels = torch.zeros(ref_latent.shape[0], 1)\n",
    "        query_labels = torch.ones(query_latent.shape[0], 1)\n",
    "        labels = torch.cat((ref_labels, query_labels), dim=0).to(device)\n",
    "        \n",
    "    elif mode == \"trainM\":\n",
    "        labels = torch.zeros(ref_latent.shape[0] + query_latent.shape[0], 1).to(device)\n",
    "    else:\n",
    "        print(\"No such mode for discriminator loss.\")\n",
    "\n",
    "    # 将源领域和目标领域数据合并并输入到域鉴别器中\n",
    "    data = torch.cat((ref_latent, query_latent), dim=0).to(device)\n",
    "    predictions = discriminator(data)\n",
    "\n",
    "    # 计算域鉴别器的损失\n",
    "    criterion = nn.BCELoss()\n",
    "    loss = criterion(predictions, labels)\n",
    "#     print(mode)\n",
    "#     print(predictions[-10:])\n",
    "#     print(labels[-10:])\n",
    "#     print(loss)\n",
    "    return loss #* 1e3\n",
    "\n",
    "def reconstruction_loss(scvi_model, transfer_mapping_module, X, batch_id):\n",
    "    n_sample = X.shape[0]\n",
    "    batch_index = torch.full((n_sample, 1), batch_id)\n",
    "    inference_output = scvi_model.module.inference(\n",
    "        X, \n",
    "        batch_index=batch_index\n",
    "    )\n",
    "    z = inference_output['z']\n",
    "    transferred_z = transfer_mapping_module(z)\n",
    "    generative_output = scvi_model.module.generative(\n",
    "        z=transferred_z,\n",
    "        library=inference_output['library'], \n",
    "        batch_index=batch_index\n",
    "    )    \n",
    "    reconst_loss = scvi_model.module.get_reconstruction_loss(\n",
    "        X, \n",
    "        generative_output['px_rate'], \n",
    "        generative_output['px_r'], \n",
    "        generative_output['px_dropout']\n",
    "    )\n",
    "#     loss = -generative_x.log_prob(x).sum(-1)\n",
    "    return torch.mean(reconst_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "badaf589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "dataset = MyDataset(query_adata, \"counts\")\n",
    "\n",
    "# hyper-parameters\n",
    "learning_rate = 0.01\n",
    "batch_size = 256\n",
    "num_workers = 4\n",
    "shuffle = True\n",
    "num_epoch = 250\n",
    "\n",
    "# Instantiate trainloader\n",
    "train_loader = DataLoader(dataset, \n",
    "                        batch_size=batch_size, \n",
    "                        num_workers=num_workers, \n",
    "                        shuffle=shuffle)\n",
    "# Instantiate mapping module\n",
    "mapping_module = TransferMappingModule(n_latent=30, n_gene=2000,  \n",
    "                                       dropout_rate=0.1).to(device)\n",
    "# Instantiate discriminator\n",
    "discriminator = DomainDiscriminator(input_dim=30, \n",
    "                                    dropout_rate=0.1).to(device)\n",
    "\n",
    "# criterion = nn.BCELoss()\n",
    "optimizerD = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "optimizerM = optim.Adam(mapping_module.parameters(), lr=learning_rate)\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "schedulerD = StepLR(optimizerD, step_size=50, gamma=0.5)\n",
    "schedulerM = StepLR(optimizerM, step_size=50, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1ce6ff3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wyh/anaconda3/envs/cell2loc_env/lib/python3.9/site-packages/scvi/distributions/_negative_binomial.py:97: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /home/wyh/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/cuda/jit_utils.cpp:860.)\n",
      "  + torch.lgamma(x + theta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/250], step [1/244], D_loss: 0.7648, M_loss: 2.0078, M_d_loss: 0.6395, M_r_loss: 1368.3115\n",
      "Epoch [1/250], step [41/244], D_loss: 0.0392, M_loss: 0.6529, M_d_loss: 0.0307, M_r_loss: 622.2471\n",
      "Epoch [1/250], step [81/244], D_loss: 0.1358, M_loss: 0.6260, M_d_loss: 0.0416, M_r_loss: 584.4247\n",
      "Epoch [1/250], step [121/244], D_loss: 0.0651, M_loss: 0.6175, M_d_loss: 0.0235, M_r_loss: 594.0700\n",
      "Epoch [1/250], step [161/244], D_loss: 0.1304, M_loss: 0.6688, M_d_loss: 0.0622, M_r_loss: 606.6487\n",
      "Epoch [1/250], step [201/244], D_loss: 0.0487, M_loss: 0.6529, M_d_loss: 0.0381, M_r_loss: 614.7516\n",
      "Epoch [1/250], step [241/244], D_loss: 0.0540, M_loss: 0.5459, M_d_loss: 0.0325, M_r_loss: 513.4702\n",
      "Epoch [2/250], step [1/244], D_loss: 0.0509, M_loss: 0.5717, M_d_loss: 0.0300, M_r_loss: 541.7722\n",
      "Epoch [2/250], step [41/244], D_loss: 0.0700, M_loss: 0.5859, M_d_loss: 0.0354, M_r_loss: 550.4769\n",
      "Epoch [2/250], step [81/244], D_loss: 0.0535, M_loss: 0.5690, M_d_loss: 0.0249, M_r_loss: 544.0588\n",
      "Epoch [2/250], step [121/244], D_loss: 0.0653, M_loss: 0.5478, M_d_loss: 0.0342, M_r_loss: 513.5917\n",
      "Epoch [2/250], step [161/244], D_loss: 0.0778, M_loss: 0.6375, M_d_loss: 0.0475, M_r_loss: 589.9761\n",
      "Epoch [2/250], step [201/244], D_loss: 0.0583, M_loss: 0.6205, M_d_loss: 0.0293, M_r_loss: 591.2247\n",
      "Epoch [2/250], step [241/244], D_loss: 0.0821, M_loss: 0.5842, M_d_loss: 0.0378, M_r_loss: 546.3946\n",
      "Epoch [3/250], step [1/244], D_loss: 0.0952, M_loss: 0.6122, M_d_loss: 0.0498, M_r_loss: 562.3696\n",
      "Epoch [3/250], step [41/244], D_loss: 0.0737, M_loss: 0.6648, M_d_loss: 0.0425, M_r_loss: 622.3042\n",
      "Epoch [3/250], step [81/244], D_loss: 0.0907, M_loss: 0.6419, M_d_loss: 0.0462, M_r_loss: 595.7073\n",
      "Epoch [3/250], step [121/244], D_loss: 0.0676, M_loss: 0.5898, M_d_loss: 0.0363, M_r_loss: 553.4775\n",
      "Epoch [3/250], step [161/244], D_loss: 0.0654, M_loss: 0.5589, M_d_loss: 0.0357, M_r_loss: 523.1746\n",
      "Epoch [3/250], step [201/244], D_loss: 0.0510, M_loss: 0.6115, M_d_loss: 0.0454, M_r_loss: 566.0398\n",
      "Epoch [3/250], step [241/244], D_loss: 0.0825, M_loss: 0.6218, M_d_loss: 0.0515, M_r_loss: 570.3237\n",
      "Epoch [4/250], step [1/244], D_loss: 0.0880, M_loss: 0.6329, M_d_loss: 0.0494, M_r_loss: 583.4902\n",
      "Epoch [4/250], step [41/244], D_loss: 0.0483, M_loss: 0.6001, M_d_loss: 0.0289, M_r_loss: 571.1876\n",
      "Epoch [4/250], step [81/244], D_loss: 0.0743, M_loss: 0.6315, M_d_loss: 0.0395, M_r_loss: 591.9722\n",
      "Epoch [4/250], step [121/244], D_loss: 0.0534, M_loss: 0.5272, M_d_loss: 0.0326, M_r_loss: 494.5547\n",
      "Epoch [4/250], step [161/244], D_loss: 0.0599, M_loss: 0.6143, M_d_loss: 0.0355, M_r_loss: 578.7479\n",
      "Epoch [4/250], step [201/244], D_loss: 0.0652, M_loss: 0.5931, M_d_loss: 0.0361, M_r_loss: 556.9404\n",
      "Epoch [4/250], step [241/244], D_loss: 0.0632, M_loss: 0.5797, M_d_loss: 0.0389, M_r_loss: 540.7991\n",
      "Epoch [5/250], step [1/244], D_loss: 0.0549, M_loss: 0.5971, M_d_loss: 0.0384, M_r_loss: 558.6649\n",
      "Epoch [5/250], step [41/244], D_loss: 0.0640, M_loss: 0.6396, M_d_loss: 0.0421, M_r_loss: 597.4909\n",
      "Epoch [5/250], step [81/244], D_loss: 0.0437, M_loss: 0.6172, M_d_loss: 0.0333, M_r_loss: 583.9527\n",
      "Epoch [5/250], step [121/244], D_loss: 0.0450, M_loss: 0.5980, M_d_loss: 0.0331, M_r_loss: 564.8213\n",
      "Epoch [5/250], step [161/244], D_loss: 0.0566, M_loss: 0.5864, M_d_loss: 0.0414, M_r_loss: 544.9061\n",
      "Epoch [5/250], step [201/244], D_loss: 0.0516, M_loss: 0.6062, M_d_loss: 0.0362, M_r_loss: 570.0165\n",
      "Epoch [5/250], step [241/244], D_loss: 0.0480, M_loss: 0.5803, M_d_loss: 0.0344, M_r_loss: 545.8549\n",
      "Epoch [6/250], step [1/244], D_loss: 0.0471, M_loss: 0.5671, M_d_loss: 0.0403, M_r_loss: 526.8047\n",
      "Epoch [6/250], step [41/244], D_loss: 0.0726, M_loss: 0.6057, M_d_loss: 0.0492, M_r_loss: 556.5161\n",
      "Epoch [6/250], step [81/244], D_loss: 0.0648, M_loss: 0.6107, M_d_loss: 0.0402, M_r_loss: 570.4790\n",
      "Epoch [6/250], step [121/244], D_loss: 0.0623, M_loss: 0.6068, M_d_loss: 0.0306, M_r_loss: 576.1472\n",
      "Epoch [6/250], step [161/244], D_loss: 0.0655, M_loss: 0.5946, M_d_loss: 0.0427, M_r_loss: 551.8874\n",
      "Epoch [6/250], step [201/244], D_loss: 0.0793, M_loss: 0.5977, M_d_loss: 0.0579, M_r_loss: 539.8033\n",
      "Epoch [6/250], step [241/244], D_loss: 0.1047, M_loss: 0.6142, M_d_loss: 0.0588, M_r_loss: 555.4022\n",
      "Epoch [7/250], step [1/244], D_loss: 0.0937, M_loss: 0.6218, M_d_loss: 0.0548, M_r_loss: 566.9792\n",
      "Epoch [7/250], step [41/244], D_loss: 0.0540, M_loss: 0.6070, M_d_loss: 0.0344, M_r_loss: 572.5787\n",
      "Epoch [7/250], step [81/244], D_loss: 0.0680, M_loss: 0.5719, M_d_loss: 0.0327, M_r_loss: 539.2072\n",
      "Epoch [7/250], step [121/244], D_loss: 0.0543, M_loss: 0.5577, M_d_loss: 0.0301, M_r_loss: 527.6458\n",
      "Epoch [7/250], step [161/244], D_loss: 0.0452, M_loss: 0.6136, M_d_loss: 0.0353, M_r_loss: 578.3348\n",
      "Epoch [7/250], step [201/244], D_loss: 0.0522, M_loss: 0.5410, M_d_loss: 0.0355, M_r_loss: 505.5594\n",
      "Epoch [7/250], step [241/244], D_loss: 0.0397, M_loss: 0.5434, M_d_loss: 0.0289, M_r_loss: 514.5293\n",
      "Epoch [8/250], step [1/244], D_loss: 0.0423, M_loss: 0.5759, M_d_loss: 0.0304, M_r_loss: 545.5624\n",
      "Epoch [8/250], step [41/244], D_loss: 0.0797, M_loss: 0.5695, M_d_loss: 0.0358, M_r_loss: 533.6461\n",
      "Epoch [8/250], step [81/244], D_loss: 0.0479, M_loss: 0.5792, M_d_loss: 0.0294, M_r_loss: 549.7732\n",
      "Epoch [8/250], step [121/244], D_loss: 0.0676, M_loss: 0.6223, M_d_loss: 0.0413, M_r_loss: 581.0031\n",
      "Epoch [8/250], step [161/244], D_loss: 0.0767, M_loss: 0.6127, M_d_loss: 0.0416, M_r_loss: 571.1022\n",
      "Epoch [8/250], step [201/244], D_loss: 0.0552, M_loss: 0.5850, M_d_loss: 0.0398, M_r_loss: 545.2457\n",
      "Epoch [8/250], step [241/244], D_loss: 0.0832, M_loss: 0.6469, M_d_loss: 0.0712, M_r_loss: 575.7018\n",
      "Epoch [9/250], step [1/244], D_loss: 0.1042, M_loss: 0.6653, M_d_loss: 0.0561, M_r_loss: 609.2325\n",
      "Epoch [9/250], step [41/244], D_loss: 0.0550, M_loss: 0.6064, M_d_loss: 0.0404, M_r_loss: 566.0054\n",
      "Epoch [9/250], step [81/244], D_loss: 0.0533, M_loss: 0.5764, M_d_loss: 0.0359, M_r_loss: 540.4338\n",
      "Epoch [9/250], step [121/244], D_loss: 0.0538, M_loss: 0.5434, M_d_loss: 0.0356, M_r_loss: 507.7958\n",
      "Epoch [9/250], step [161/244], D_loss: 0.0536, M_loss: 0.5624, M_d_loss: 0.0338, M_r_loss: 528.5939\n",
      "Epoch [9/250], step [201/244], D_loss: 0.0518, M_loss: 0.5872, M_d_loss: 0.0352, M_r_loss: 552.0448\n",
      "Epoch [9/250], step [241/244], D_loss: 0.0776, M_loss: 0.6131, M_d_loss: 0.0354, M_r_loss: 577.6747\n",
      "Epoch [10/250], step [1/244], D_loss: 0.0783, M_loss: 0.6122, M_d_loss: 0.0471, M_r_loss: 565.0712\n",
      "Epoch [10/250], step [41/244], D_loss: 0.0509, M_loss: 0.5829, M_d_loss: 0.0323, M_r_loss: 550.6011\n",
      "Epoch [10/250], step [81/244], D_loss: 0.0640, M_loss: 0.6471, M_d_loss: 0.0388, M_r_loss: 608.2949\n",
      "Epoch [10/250], step [121/244], D_loss: 0.0780, M_loss: 0.5780, M_d_loss: 0.0360, M_r_loss: 542.0534\n",
      "Epoch [10/250], step [161/244], D_loss: 0.0705, M_loss: 0.5956, M_d_loss: 0.0389, M_r_loss: 556.6754\n",
      "Epoch [10/250], step [201/244], D_loss: 0.0655, M_loss: 0.5818, M_d_loss: 0.0386, M_r_loss: 543.1789\n",
      "Epoch [10/250], step [241/244], D_loss: 0.0891, M_loss: 0.5704, M_d_loss: 0.0354, M_r_loss: 534.9678\n",
      "Epoch [11/250], step [1/244], D_loss: 0.0858, M_loss: 0.5850, M_d_loss: 0.0447, M_r_loss: 540.3196\n",
      "Epoch [11/250], step [41/244], D_loss: 0.0533, M_loss: 0.5711, M_d_loss: 0.0343, M_r_loss: 536.8096\n",
      "Epoch [11/250], step [81/244], D_loss: 0.0703, M_loss: 0.5773, M_d_loss: 0.0376, M_r_loss: 539.6835\n",
      "Epoch [11/250], step [121/244], D_loss: 0.0541, M_loss: 0.6175, M_d_loss: 0.0347, M_r_loss: 582.7924\n",
      "Epoch [11/250], step [161/244], D_loss: 0.0462, M_loss: 0.5607, M_d_loss: 0.0340, M_r_loss: 526.6454\n",
      "Epoch [11/250], step [201/244], D_loss: 0.0735, M_loss: 0.5885, M_d_loss: 0.0408, M_r_loss: 547.7206\n",
      "Epoch [11/250], step [241/244], D_loss: 0.0500, M_loss: 0.5758, M_d_loss: 0.0311, M_r_loss: 544.6481\n",
      "Epoch [12/250], step [1/244], D_loss: 0.0477, M_loss: 0.6008, M_d_loss: 0.0431, M_r_loss: 557.7352\n",
      "Epoch [12/250], step [41/244], D_loss: 0.0575, M_loss: 0.5593, M_d_loss: 0.0373, M_r_loss: 521.9512\n",
      "Epoch [12/250], step [81/244], D_loss: 0.0390, M_loss: 0.5616, M_d_loss: 0.0313, M_r_loss: 530.3431\n",
      "Epoch [12/250], step [121/244], D_loss: 0.0756, M_loss: 0.5989, M_d_loss: 0.0326, M_r_loss: 566.2650\n",
      "Epoch [12/250], step [161/244], D_loss: 0.0703, M_loss: 0.6160, M_d_loss: 0.0454, M_r_loss: 570.6082\n",
      "Epoch [12/250], step [201/244], D_loss: 0.0621, M_loss: 0.6015, M_d_loss: 0.0400, M_r_loss: 561.4961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/250], step [241/244], D_loss: 0.0743, M_loss: 0.5985, M_d_loss: 0.0404, M_r_loss: 558.1320\n",
      "Epoch [13/250], step [1/244], D_loss: 0.0894, M_loss: 0.6307, M_d_loss: 0.0418, M_r_loss: 588.8945\n",
      "Epoch [13/250], step [41/244], D_loss: 0.0795, M_loss: 0.6280, M_d_loss: 0.0414, M_r_loss: 586.5339\n",
      "Epoch [13/250], step [81/244], D_loss: 0.0670, M_loss: 0.6467, M_d_loss: 0.0414, M_r_loss: 605.3606\n",
      "Epoch [13/250], step [121/244], D_loss: 0.0482, M_loss: 0.6118, M_d_loss: 0.0286, M_r_loss: 583.1709\n",
      "Epoch [13/250], step [161/244], D_loss: 0.0541, M_loss: 0.6119, M_d_loss: 0.0327, M_r_loss: 579.2499\n",
      "Epoch [13/250], step [201/244], D_loss: 0.0694, M_loss: 0.6079, M_d_loss: 0.0467, M_r_loss: 561.2422\n",
      "Epoch [13/250], step [241/244], D_loss: 0.0665, M_loss: 0.6605, M_d_loss: 0.0598, M_r_loss: 600.7540\n",
      "Epoch [14/250], step [1/244], D_loss: 0.0903, M_loss: 0.6248, M_d_loss: 0.0575, M_r_loss: 567.2628\n",
      "Epoch [14/250], step [41/244], D_loss: 0.0663, M_loss: 0.5958, M_d_loss: 0.0360, M_r_loss: 559.8171\n",
      "Epoch [14/250], step [81/244], D_loss: 0.0696, M_loss: 0.5636, M_d_loss: 0.0369, M_r_loss: 526.6756\n",
      "Epoch [14/250], step [121/244], D_loss: 0.0464, M_loss: 0.5840, M_d_loss: 0.0393, M_r_loss: 544.7147\n",
      "Epoch [14/250], step [161/244], D_loss: 0.0546, M_loss: 0.5760, M_d_loss: 0.0364, M_r_loss: 539.6538\n",
      "Epoch [14/250], step [201/244], D_loss: 0.0723, M_loss: 0.5401, M_d_loss: 0.0295, M_r_loss: 510.6201\n",
      "Epoch [14/250], step [241/244], D_loss: 0.0621, M_loss: 0.6123, M_d_loss: 0.0332, M_r_loss: 579.0591\n",
      "Epoch [15/250], step [1/244], D_loss: 0.0556, M_loss: 0.5924, M_d_loss: 0.0358, M_r_loss: 556.5927\n",
      "Epoch [15/250], step [41/244], D_loss: 0.0525, M_loss: 0.6225, M_d_loss: 0.0426, M_r_loss: 579.9064\n",
      "Epoch [15/250], step [81/244], D_loss: 0.0717, M_loss: 0.5591, M_d_loss: 0.0369, M_r_loss: 522.2108\n",
      "Epoch [15/250], step [121/244], D_loss: 0.0867, M_loss: 0.5637, M_d_loss: 0.0333, M_r_loss: 530.4724\n",
      "Epoch [15/250], step [161/244], D_loss: 0.0795, M_loss: 0.5853, M_d_loss: 0.0504, M_r_loss: 534.8990\n",
      "Epoch [15/250], step [201/244], D_loss: 0.0477, M_loss: 0.5823, M_d_loss: 0.0366, M_r_loss: 545.6479\n",
      "Epoch [15/250], step [241/244], D_loss: 0.0711, M_loss: 0.6183, M_d_loss: 0.0378, M_r_loss: 580.4922\n",
      "Epoch [16/250], step [1/244], D_loss: 0.0656, M_loss: 0.5953, M_d_loss: 0.0381, M_r_loss: 557.2458\n",
      "Epoch [16/250], step [41/244], D_loss: 0.0577, M_loss: 0.6260, M_d_loss: 0.0436, M_r_loss: 582.4048\n",
      "Epoch [16/250], step [81/244], D_loss: 0.0527, M_loss: 0.5912, M_d_loss: 0.0441, M_r_loss: 547.1299\n",
      "Epoch [16/250], step [121/244], D_loss: 0.0452, M_loss: 0.5672, M_d_loss: 0.0354, M_r_loss: 531.7808\n",
      "Epoch [16/250], step [161/244], D_loss: 0.0466, M_loss: 0.5888, M_d_loss: 0.0347, M_r_loss: 554.1331\n",
      "Epoch [16/250], step [201/244], D_loss: 0.0583, M_loss: 0.5600, M_d_loss: 0.0389, M_r_loss: 521.1082\n",
      "Epoch [16/250], step [241/244], D_loss: 0.0563, M_loss: 0.5655, M_d_loss: 0.0352, M_r_loss: 530.2977\n",
      "Epoch [17/250], step [1/244], D_loss: 0.0650, M_loss: 0.5833, M_d_loss: 0.0341, M_r_loss: 549.2114\n",
      "Epoch [17/250], step [41/244], D_loss: 0.0666, M_loss: 0.6082, M_d_loss: 0.0402, M_r_loss: 568.0167\n",
      "Epoch [17/250], step [81/244], D_loss: 0.0767, M_loss: 0.5801, M_d_loss: 0.0418, M_r_loss: 538.2357\n",
      "Epoch [17/250], step [121/244], D_loss: 0.0455, M_loss: 0.6081, M_d_loss: 0.0342, M_r_loss: 573.8699\n",
      "Epoch [17/250], step [161/244], D_loss: 0.0728, M_loss: 0.6179, M_d_loss: 0.0473, M_r_loss: 570.5723\n",
      "Epoch [17/250], step [201/244], D_loss: 0.0538, M_loss: 0.6035, M_d_loss: 0.0378, M_r_loss: 565.7198\n",
      "Epoch [17/250], step [241/244], D_loss: 0.0584, M_loss: 0.5684, M_d_loss: 0.0314, M_r_loss: 536.9707\n",
      "Epoch [18/250], step [1/244], D_loss: 0.0676, M_loss: 0.6017, M_d_loss: 0.0391, M_r_loss: 562.5342\n",
      "Epoch [18/250], step [41/244], D_loss: 0.0396, M_loss: 0.6122, M_d_loss: 0.0359, M_r_loss: 576.2972\n",
      "Epoch [18/250], step [81/244], D_loss: 0.0670, M_loss: 0.5501, M_d_loss: 0.0326, M_r_loss: 517.5033\n",
      "Epoch [18/250], step [121/244], D_loss: 0.0779, M_loss: 0.5659, M_d_loss: 0.0318, M_r_loss: 534.0577\n",
      "Epoch [18/250], step [161/244], D_loss: 0.0640, M_loss: 0.6132, M_d_loss: 0.0435, M_r_loss: 569.7170\n",
      "Epoch [18/250], step [201/244], D_loss: 0.0763, M_loss: 0.5591, M_d_loss: 0.0419, M_r_loss: 517.1826\n",
      "Epoch [18/250], step [241/244], D_loss: 0.0604, M_loss: 0.5953, M_d_loss: 0.0403, M_r_loss: 554.9556\n",
      "Epoch [19/250], step [1/244], D_loss: 0.0739, M_loss: 0.6386, M_d_loss: 0.0473, M_r_loss: 591.2289\n",
      "Epoch [19/250], step [41/244], D_loss: 0.0693, M_loss: 0.5890, M_d_loss: 0.0390, M_r_loss: 549.9930\n",
      "Epoch [19/250], step [81/244], D_loss: 0.0544, M_loss: 0.5372, M_d_loss: 0.0289, M_r_loss: 508.3832\n",
      "Epoch [19/250], step [121/244], D_loss: 0.0623, M_loss: 0.5832, M_d_loss: 0.0370, M_r_loss: 546.1469\n",
      "Epoch [19/250], step [161/244], D_loss: 0.0479, M_loss: 0.5702, M_d_loss: 0.0320, M_r_loss: 538.2549\n",
      "Epoch [19/250], step [201/244], D_loss: 0.0663, M_loss: 0.5870, M_d_loss: 0.0373, M_r_loss: 549.6919\n",
      "Epoch [19/250], step [241/244], D_loss: 0.0569, M_loss: 0.5648, M_d_loss: 0.0411, M_r_loss: 523.7139\n",
      "Epoch [20/250], step [1/244], D_loss: 0.0684, M_loss: 0.6008, M_d_loss: 0.0418, M_r_loss: 558.9343\n",
      "Epoch [20/250], step [41/244], D_loss: 0.0553, M_loss: 0.6248, M_d_loss: 0.0346, M_r_loss: 590.1877\n",
      "Epoch [20/250], step [81/244], D_loss: 0.0466, M_loss: 0.5487, M_d_loss: 0.0364, M_r_loss: 512.3118\n",
      "Epoch [20/250], step [121/244], D_loss: 0.0455, M_loss: 0.5763, M_d_loss: 0.0381, M_r_loss: 538.1752\n",
      "Epoch [20/250], step [161/244], D_loss: 0.0538, M_loss: 0.5859, M_d_loss: 0.0416, M_r_loss: 544.2638\n",
      "Epoch [20/250], step [201/244], D_loss: 0.0955, M_loss: 0.5893, M_d_loss: 0.0458, M_r_loss: 543.5198\n",
      "Epoch [20/250], step [241/244], D_loss: 0.1015, M_loss: 0.6440, M_d_loss: 0.0464, M_r_loss: 597.6660\n",
      "Epoch [21/250], step [1/244], D_loss: 0.0927, M_loss: 0.5970, M_d_loss: 0.0509, M_r_loss: 546.1368\n",
      "Epoch [21/250], step [41/244], D_loss: 0.0441, M_loss: 0.5662, M_d_loss: 0.0354, M_r_loss: 530.7486\n",
      "Epoch [21/250], step [81/244], D_loss: 0.0733, M_loss: 0.6610, M_d_loss: 0.0826, M_r_loss: 578.3700\n",
      "Epoch [21/250], step [121/244], D_loss: 0.0569, M_loss: 0.5914, M_d_loss: 0.0465, M_r_loss: 544.9772\n",
      "Epoch [21/250], step [161/244], D_loss: 0.0727, M_loss: 0.6330, M_d_loss: 0.0472, M_r_loss: 585.8527\n",
      "Epoch [21/250], step [201/244], D_loss: 0.0941, M_loss: 0.6004, M_d_loss: 0.0439, M_r_loss: 556.5300\n",
      "Epoch [21/250], step [241/244], D_loss: 0.0701, M_loss: 0.5849, M_d_loss: 0.0416, M_r_loss: 543.2926\n",
      "Epoch [22/250], step [1/244], D_loss: 0.0747, M_loss: 0.6226, M_d_loss: 0.0433, M_r_loss: 579.3560\n",
      "Epoch [22/250], step [41/244], D_loss: 0.0453, M_loss: 0.5734, M_d_loss: 0.0373, M_r_loss: 536.1657\n",
      "Epoch [22/250], step [81/244], D_loss: 0.0356, M_loss: 0.6462, M_d_loss: 0.0379, M_r_loss: 608.3100\n",
      "Epoch [22/250], step [121/244], D_loss: 0.0591, M_loss: 0.5715, M_d_loss: 0.0409, M_r_loss: 530.6304\n",
      "Epoch [22/250], step [161/244], D_loss: 0.0674, M_loss: 0.6136, M_d_loss: 0.0393, M_r_loss: 574.2753\n",
      "Epoch [22/250], step [201/244], D_loss: 0.0430, M_loss: 0.6452, M_d_loss: 0.0317, M_r_loss: 613.4819\n",
      "Epoch [22/250], step [241/244], D_loss: 0.0487, M_loss: 0.6045, M_d_loss: 0.0352, M_r_loss: 569.3092\n",
      "Epoch [23/250], step [1/244], D_loss: 0.0582, M_loss: 0.5928, M_d_loss: 0.0412, M_r_loss: 551.6147\n",
      "Epoch [23/250], step [41/244], D_loss: 0.0654, M_loss: 0.6150, M_d_loss: 0.0383, M_r_loss: 576.7278\n",
      "Epoch [23/250], step [81/244], D_loss: 0.0609, M_loss: 0.5924, M_d_loss: 0.0434, M_r_loss: 548.9946\n",
      "Epoch [23/250], step [121/244], D_loss: 0.0535, M_loss: 0.6098, M_d_loss: 0.0387, M_r_loss: 571.1672\n",
      "Epoch [23/250], step [161/244], D_loss: 0.0676, M_loss: 0.6110, M_d_loss: 0.0468, M_r_loss: 564.2019\n",
      "Epoch [23/250], step [201/244], D_loss: 0.0806, M_loss: 0.5884, M_d_loss: 0.0401, M_r_loss: 548.3448\n",
      "Epoch [23/250], step [241/244], D_loss: 0.0680, M_loss: 0.5994, M_d_loss: 0.0367, M_r_loss: 562.7362\n",
      "Epoch [24/250], step [1/244], D_loss: 0.0747, M_loss: 0.5996, M_d_loss: 0.0392, M_r_loss: 560.4018\n",
      "Epoch [24/250], step [41/244], D_loss: 0.0714, M_loss: 0.6242, M_d_loss: 0.0374, M_r_loss: 586.7759\n",
      "Epoch [24/250], step [81/244], D_loss: 0.0632, M_loss: 0.5639, M_d_loss: 0.0289, M_r_loss: 534.9624\n",
      "Epoch [24/250], step [121/244], D_loss: 0.0605, M_loss: 0.5566, M_d_loss: 0.0472, M_r_loss: 509.3408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/250], step [161/244], D_loss: 0.0684, M_loss: 0.6562, M_d_loss: 0.0423, M_r_loss: 613.8911\n",
      "Epoch [24/250], step [201/244], D_loss: 0.0553, M_loss: 0.5835, M_d_loss: 0.0377, M_r_loss: 545.7734\n",
      "Epoch [24/250], step [241/244], D_loss: 0.0503, M_loss: 0.5758, M_d_loss: 0.0416, M_r_loss: 534.1534\n",
      "Epoch [25/250], step [1/244], D_loss: 0.0536, M_loss: 0.5602, M_d_loss: 0.0384, M_r_loss: 521.7930\n",
      "Epoch [25/250], step [41/244], D_loss: 0.0735, M_loss: 0.5882, M_d_loss: 0.0361, M_r_loss: 552.0939\n",
      "Epoch [25/250], step [81/244], D_loss: 0.0596, M_loss: 0.5977, M_d_loss: 0.0434, M_r_loss: 554.3300\n",
      "Epoch [25/250], step [121/244], D_loss: 0.0758, M_loss: 0.6087, M_d_loss: 0.0368, M_r_loss: 571.8636\n",
      "Epoch [25/250], step [161/244], D_loss: 0.0919, M_loss: 0.5948, M_d_loss: 0.0498, M_r_loss: 545.0219\n",
      "Epoch [25/250], step [201/244], D_loss: 0.0692, M_loss: 0.5828, M_d_loss: 0.0430, M_r_loss: 539.8155\n",
      "Epoch [25/250], step [241/244], D_loss: 0.0695, M_loss: 0.5743, M_d_loss: 0.0442, M_r_loss: 530.1179\n",
      "Epoch [26/250], step [1/244], D_loss: 0.0839, M_loss: 0.5987, M_d_loss: 0.0540, M_r_loss: 544.7094\n",
      "Epoch [26/250], step [41/244], D_loss: 0.0771, M_loss: 0.6305, M_d_loss: 0.0371, M_r_loss: 593.3474\n",
      "Epoch [26/250], step [81/244], D_loss: 0.0803, M_loss: 0.5520, M_d_loss: 0.0316, M_r_loss: 520.4369\n",
      "Epoch [26/250], step [121/244], D_loss: 0.0593, M_loss: 0.6313, M_d_loss: 0.0363, M_r_loss: 595.0364\n",
      "Epoch [26/250], step [161/244], D_loss: 0.0463, M_loss: 0.5636, M_d_loss: 0.0322, M_r_loss: 531.3834\n",
      "Epoch [26/250], step [201/244], D_loss: 0.0508, M_loss: 0.6102, M_d_loss: 0.0468, M_r_loss: 563.3234\n",
      "Epoch [26/250], step [241/244], D_loss: 0.0437, M_loss: 0.5804, M_d_loss: 0.0402, M_r_loss: 540.2493\n",
      "Epoch [27/250], step [1/244], D_loss: 0.0538, M_loss: 0.6087, M_d_loss: 0.0462, M_r_loss: 562.4919\n",
      "Epoch [27/250], step [41/244], D_loss: 0.0530, M_loss: 0.5946, M_d_loss: 0.0342, M_r_loss: 560.4462\n",
      "Epoch [27/250], step [81/244], D_loss: 0.0713, M_loss: 0.6036, M_d_loss: 0.0411, M_r_loss: 562.4723\n",
      "Epoch [27/250], step [121/244], D_loss: 0.0698, M_loss: 0.5743, M_d_loss: 0.0412, M_r_loss: 533.0499\n",
      "Epoch [27/250], step [161/244], D_loss: 0.0609, M_loss: 0.5723, M_d_loss: 0.0367, M_r_loss: 535.5750\n",
      "Epoch [27/250], step [201/244], D_loss: 0.0701, M_loss: 0.6029, M_d_loss: 0.0417, M_r_loss: 561.2642\n",
      "Epoch [27/250], step [241/244], D_loss: 0.0613, M_loss: 0.6053, M_d_loss: 0.0459, M_r_loss: 559.4077\n",
      "Epoch [28/250], step [1/244], D_loss: 0.0683, M_loss: 0.6369, M_d_loss: 0.0551, M_r_loss: 581.8241\n",
      "Epoch [28/250], step [41/244], D_loss: 0.0704, M_loss: 0.6045, M_d_loss: 0.0460, M_r_loss: 558.4741\n",
      "Epoch [28/250], step [81/244], D_loss: 0.0536, M_loss: 0.6217, M_d_loss: 0.0388, M_r_loss: 582.8821\n",
      "Epoch [28/250], step [121/244], D_loss: 0.0736, M_loss: 0.5901, M_d_loss: 0.0359, M_r_loss: 554.2139\n",
      "Epoch [28/250], step [161/244], D_loss: 0.0580, M_loss: 0.5929, M_d_loss: 0.0339, M_r_loss: 559.0439\n",
      "Epoch [28/250], step [201/244], D_loss: 0.0550, M_loss: 0.5855, M_d_loss: 0.0322, M_r_loss: 553.3028\n",
      "Epoch [28/250], step [241/244], D_loss: 0.0438, M_loss: 0.6058, M_d_loss: 0.0327, M_r_loss: 573.1641\n",
      "Epoch [29/250], step [1/244], D_loss: 0.0504, M_loss: 0.6018, M_d_loss: 0.0362, M_r_loss: 565.5616\n",
      "Epoch [29/250], step [41/244], D_loss: 0.0459, M_loss: 0.5764, M_d_loss: 0.0400, M_r_loss: 536.3857\n",
      "Epoch [29/250], step [81/244], D_loss: 0.0749, M_loss: 0.5703, M_d_loss: 0.0401, M_r_loss: 530.2571\n",
      "Epoch [29/250], step [121/244], D_loss: 0.0704, M_loss: 0.5938, M_d_loss: 0.0294, M_r_loss: 564.3967\n",
      "Epoch [29/250], step [161/244], D_loss: 0.0667, M_loss: 0.6017, M_d_loss: 0.0383, M_r_loss: 563.3408\n",
      "Epoch [29/250], step [201/244], D_loss: 0.0898, M_loss: 0.5914, M_d_loss: 0.0395, M_r_loss: 551.9541\n",
      "Epoch [29/250], step [241/244], D_loss: 0.0490, M_loss: 0.5823, M_d_loss: 0.0346, M_r_loss: 547.7419\n",
      "Epoch [30/250], step [1/244], D_loss: 0.0599, M_loss: 0.5833, M_d_loss: 0.0362, M_r_loss: 547.1161\n",
      "Epoch [30/250], step [41/244], D_loss: 0.0578, M_loss: 0.5650, M_d_loss: 0.0350, M_r_loss: 530.0314\n",
      "Epoch [30/250], step [81/244], D_loss: 0.0567, M_loss: 0.5897, M_d_loss: 0.0392, M_r_loss: 550.5125\n",
      "Epoch [30/250], step [121/244], D_loss: 0.0700, M_loss: 0.5654, M_d_loss: 0.0420, M_r_loss: 523.3922\n",
      "Epoch [30/250], step [161/244], D_loss: 0.0569, M_loss: 0.6128, M_d_loss: 0.0373, M_r_loss: 575.4839\n",
      "Epoch [30/250], step [201/244], D_loss: 0.0691, M_loss: 0.6228, M_d_loss: 0.0382, M_r_loss: 584.6140\n",
      "Epoch [30/250], step [241/244], D_loss: 0.0592, M_loss: 0.5907, M_d_loss: 0.0385, M_r_loss: 552.1584\n",
      "Epoch [31/250], step [1/244], D_loss: 0.0671, M_loss: 0.5873, M_d_loss: 0.0384, M_r_loss: 548.8810\n",
      "Epoch [31/250], step [41/244], D_loss: 0.0496, M_loss: 0.5593, M_d_loss: 0.0287, M_r_loss: 530.5942\n",
      "Epoch [31/250], step [81/244], D_loss: 0.0888, M_loss: 0.5538, M_d_loss: 0.0383, M_r_loss: 515.4982\n",
      "Epoch [31/250], step [121/244], D_loss: 0.0478, M_loss: 0.5725, M_d_loss: 0.0288, M_r_loss: 543.7179\n",
      "Epoch [31/250], step [161/244], D_loss: 0.0493, M_loss: 0.5979, M_d_loss: 0.0330, M_r_loss: 564.8244\n",
      "Epoch [31/250], step [201/244], D_loss: 0.0656, M_loss: 0.5334, M_d_loss: 0.0388, M_r_loss: 494.5265\n",
      "Epoch [31/250], step [241/244], D_loss: 0.0666, M_loss: 0.5824, M_d_loss: 0.0419, M_r_loss: 540.5243\n",
      "Epoch [32/250], step [1/244], D_loss: 0.0655, M_loss: 0.6228, M_d_loss: 0.0395, M_r_loss: 583.3607\n",
      "Epoch [32/250], step [41/244], D_loss: 0.0679, M_loss: 0.6019, M_d_loss: 0.0387, M_r_loss: 563.1905\n",
      "Epoch [32/250], step [81/244], D_loss: 0.0474, M_loss: 0.5908, M_d_loss: 0.0376, M_r_loss: 553.2195\n",
      "Epoch [32/250], step [121/244], D_loss: 0.0476, M_loss: 0.5870, M_d_loss: 0.0363, M_r_loss: 550.6934\n",
      "Epoch [32/250], step [161/244], D_loss: 0.0485, M_loss: 0.6103, M_d_loss: 0.0431, M_r_loss: 567.2098\n",
      "Epoch [32/250], step [201/244], D_loss: 0.0781, M_loss: 0.5749, M_d_loss: 0.0357, M_r_loss: 539.1826\n",
      "Epoch [32/250], step [241/244], D_loss: 0.0776, M_loss: 0.5545, M_d_loss: 0.0411, M_r_loss: 513.3837\n",
      "Epoch [33/250], step [1/244], D_loss: 0.0777, M_loss: 0.6647, M_d_loss: 0.0511, M_r_loss: 613.5605\n",
      "Epoch [33/250], step [41/244], D_loss: 0.0647, M_loss: 0.6003, M_d_loss: 0.0400, M_r_loss: 560.3668\n",
      "Epoch [33/250], step [81/244], D_loss: 0.0667, M_loss: 0.6161, M_d_loss: 0.0403, M_r_loss: 575.7928\n",
      "Epoch [33/250], step [121/244], D_loss: 0.0637, M_loss: 0.6075, M_d_loss: 0.0421, M_r_loss: 565.3762\n",
      "Epoch [33/250], step [161/244], D_loss: 0.0611, M_loss: 0.6086, M_d_loss: 0.0373, M_r_loss: 571.3122\n",
      "Epoch [33/250], step [201/244], D_loss: 0.0653, M_loss: 0.5763, M_d_loss: 0.0377, M_r_loss: 538.6057\n",
      "Epoch [33/250], step [241/244], D_loss: 0.0427, M_loss: 0.5740, M_d_loss: 0.0317, M_r_loss: 542.2213\n",
      "Epoch [34/250], step [1/244], D_loss: 0.0579, M_loss: 0.5826, M_d_loss: 0.0416, M_r_loss: 540.9533\n",
      "Epoch [34/250], step [41/244], D_loss: 0.0543, M_loss: 0.5880, M_d_loss: 0.0346, M_r_loss: 553.3308\n",
      "Epoch [34/250], step [81/244], D_loss: 0.0449, M_loss: 0.6092, M_d_loss: 0.0417, M_r_loss: 567.4629\n",
      "Epoch [34/250], step [121/244], D_loss: 0.0551, M_loss: 0.5479, M_d_loss: 0.0341, M_r_loss: 513.8412\n",
      "Epoch [34/250], step [161/244], D_loss: 0.0523, M_loss: 0.5590, M_d_loss: 0.0392, M_r_loss: 519.7792\n",
      "Epoch [34/250], step [201/244], D_loss: 0.0745, M_loss: 0.6051, M_d_loss: 0.0499, M_r_loss: 555.2736\n",
      "Epoch [34/250], step [241/244], D_loss: 0.0844, M_loss: 0.5953, M_d_loss: 0.0380, M_r_loss: 557.3727\n",
      "Epoch [35/250], step [1/244], D_loss: 0.0687, M_loss: 0.6160, M_d_loss: 0.0492, M_r_loss: 566.7700\n",
      "Epoch [35/250], step [41/244], D_loss: 0.0661, M_loss: 0.5940, M_d_loss: 0.0351, M_r_loss: 558.9497\n",
      "Epoch [35/250], step [81/244], D_loss: 0.0844, M_loss: 0.6349, M_d_loss: 0.0474, M_r_loss: 587.5334\n",
      "Epoch [35/250], step [121/244], D_loss: 0.0706, M_loss: 0.5759, M_d_loss: 0.0353, M_r_loss: 540.5619\n",
      "Epoch [35/250], step [161/244], D_loss: 0.0468, M_loss: 0.5806, M_d_loss: 0.0315, M_r_loss: 549.1192\n",
      "Epoch [35/250], step [201/244], D_loss: 0.0725, M_loss: 0.5446, M_d_loss: 0.0388, M_r_loss: 505.8601\n",
      "Epoch [35/250], step [241/244], D_loss: 0.0603, M_loss: 0.5818, M_d_loss: 0.0377, M_r_loss: 544.1344\n",
      "Epoch [36/250], step [1/244], D_loss: 0.0723, M_loss: 0.5619, M_d_loss: 0.0437, M_r_loss: 518.1637\n",
      "Epoch [36/250], step [41/244], D_loss: 0.0576, M_loss: 0.6211, M_d_loss: 0.0385, M_r_loss: 582.6241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/250], step [81/244], D_loss: 0.0425, M_loss: 0.5526, M_d_loss: 0.0391, M_r_loss: 513.4408\n",
      "Epoch [36/250], step [121/244], D_loss: 0.0636, M_loss: 0.5799, M_d_loss: 0.0425, M_r_loss: 537.3907\n",
      "Epoch [36/250], step [161/244], D_loss: 0.0942, M_loss: 0.6299, M_d_loss: 0.0479, M_r_loss: 582.0370\n",
      "Epoch [36/250], step [201/244], D_loss: 0.0657, M_loss: 0.6607, M_d_loss: 0.0391, M_r_loss: 621.6284\n",
      "Epoch [36/250], step [241/244], D_loss: 0.0524, M_loss: 0.5926, M_d_loss: 0.0364, M_r_loss: 556.1967\n",
      "Epoch [37/250], step [1/244], D_loss: 0.0623, M_loss: 0.6153, M_d_loss: 0.0397, M_r_loss: 575.6472\n",
      "Epoch [37/250], step [41/244], D_loss: 0.0484, M_loss: 0.5556, M_d_loss: 0.0410, M_r_loss: 514.5375\n",
      "Epoch [37/250], step [81/244], D_loss: 0.0503, M_loss: 0.6000, M_d_loss: 0.0415, M_r_loss: 558.4802\n",
      "Epoch [37/250], step [121/244], D_loss: 0.0409, M_loss: 0.5766, M_d_loss: 0.0418, M_r_loss: 534.7502\n",
      "Epoch [37/250], step [161/244], D_loss: 0.0611, M_loss: 0.5742, M_d_loss: 0.0363, M_r_loss: 537.8933\n",
      "Epoch [37/250], step [201/244], D_loss: 0.0602, M_loss: 0.5733, M_d_loss: 0.0360, M_r_loss: 537.3075\n",
      "Epoch [37/250], step [241/244], D_loss: 0.0769, M_loss: 0.6221, M_d_loss: 0.0473, M_r_loss: 574.7316\n",
      "Epoch [38/250], step [1/244], D_loss: 0.0610, M_loss: 0.6367, M_d_loss: 0.0574, M_r_loss: 579.2655\n",
      "Epoch [38/250], step [41/244], D_loss: 0.0560, M_loss: 0.5549, M_d_loss: 0.0332, M_r_loss: 521.7332\n",
      "Epoch [38/250], step [81/244], D_loss: 0.0588, M_loss: 0.5533, M_d_loss: 0.0329, M_r_loss: 520.3407\n",
      "Epoch [38/250], step [121/244], D_loss: 0.0548, M_loss: 0.6018, M_d_loss: 0.0453, M_r_loss: 556.5084\n",
      "Epoch [38/250], step [161/244], D_loss: 0.0705, M_loss: 0.5994, M_d_loss: 0.0454, M_r_loss: 554.0453\n",
      "Epoch [38/250], step [201/244], D_loss: 0.0599, M_loss: 0.5710, M_d_loss: 0.0446, M_r_loss: 526.4846\n",
      "Epoch [38/250], step [241/244], D_loss: 0.0879, M_loss: 0.6166, M_d_loss: 0.0542, M_r_loss: 562.4230\n",
      "Epoch [39/250], step [1/244], D_loss: 0.0730, M_loss: 0.6241, M_d_loss: 0.0618, M_r_loss: 562.3270\n",
      "Epoch [39/250], step [41/244], D_loss: 0.0560, M_loss: 0.5774, M_d_loss: 0.0339, M_r_loss: 543.5104\n",
      "Epoch [39/250], step [81/244], D_loss: 0.0436, M_loss: 0.5750, M_d_loss: 0.0341, M_r_loss: 540.9004\n",
      "Epoch [39/250], step [121/244], D_loss: 0.0659, M_loss: 0.6003, M_d_loss: 0.0370, M_r_loss: 563.2242\n",
      "Epoch [39/250], step [161/244], D_loss: 0.0452, M_loss: 0.5561, M_d_loss: 0.0370, M_r_loss: 519.1115\n",
      "Epoch [39/250], step [201/244], D_loss: 0.0451, M_loss: 0.6214, M_d_loss: 0.0416, M_r_loss: 579.7473\n",
      "Epoch [39/250], step [241/244], D_loss: 0.0562, M_loss: 0.6127, M_d_loss: 0.0469, M_r_loss: 565.8334\n",
      "Epoch [40/250], step [1/244], D_loss: 0.0703, M_loss: 0.6035, M_d_loss: 0.0530, M_r_loss: 550.4550\n",
      "Epoch [40/250], step [41/244], D_loss: 0.0553, M_loss: 0.5729, M_d_loss: 0.0368, M_r_loss: 536.0985\n",
      "Epoch [40/250], step [81/244], D_loss: 0.0470, M_loss: 0.5746, M_d_loss: 0.0341, M_r_loss: 540.5403\n",
      "Epoch [40/250], step [121/244], D_loss: 0.0788, M_loss: 0.6160, M_d_loss: 0.0419, M_r_loss: 574.0974\n",
      "Epoch [40/250], step [161/244], D_loss: 0.0649, M_loss: 0.6273, M_d_loss: 0.0484, M_r_loss: 578.8583\n",
      "Epoch [40/250], step [201/244], D_loss: 0.0775, M_loss: 0.6014, M_d_loss: 0.0443, M_r_loss: 557.0825\n",
      "Epoch [40/250], step [241/244], D_loss: 0.0470, M_loss: 0.6300, M_d_loss: 0.0454, M_r_loss: 584.6232\n",
      "Epoch [41/250], step [1/244], D_loss: 0.0436, M_loss: 0.5612, M_d_loss: 0.0350, M_r_loss: 526.1850\n",
      "Epoch [41/250], step [41/244], D_loss: 0.0595, M_loss: 0.5805, M_d_loss: 0.0340, M_r_loss: 546.5570\n",
      "Epoch [41/250], step [81/244], D_loss: 0.0602, M_loss: 0.6069, M_d_loss: 0.0361, M_r_loss: 570.8094\n",
      "Epoch [41/250], step [121/244], D_loss: 0.0512, M_loss: 0.6230, M_d_loss: 0.0412, M_r_loss: 581.7658\n",
      "Epoch [41/250], step [161/244], D_loss: 0.0638, M_loss: 0.5929, M_d_loss: 0.0317, M_r_loss: 561.1795\n",
      "Epoch [41/250], step [201/244], D_loss: 0.0726, M_loss: 0.6127, M_d_loss: 0.0435, M_r_loss: 569.1928\n",
      "Epoch [41/250], step [241/244], D_loss: 0.0655, M_loss: 0.6118, M_d_loss: 0.0449, M_r_loss: 566.9130\n",
      "Epoch [42/250], step [1/244], D_loss: 0.0763, M_loss: 0.6555, M_d_loss: 0.0442, M_r_loss: 611.2479\n",
      "Epoch [42/250], step [41/244], D_loss: 0.0685, M_loss: 0.5801, M_d_loss: 0.0405, M_r_loss: 539.6277\n",
      "Epoch [42/250], step [81/244], D_loss: 0.0475, M_loss: 0.5757, M_d_loss: 0.0349, M_r_loss: 540.7991\n",
      "Epoch [42/250], step [121/244], D_loss: 0.0570, M_loss: 0.6113, M_d_loss: 0.0473, M_r_loss: 563.9963\n",
      "Epoch [42/250], step [161/244], D_loss: 0.0755, M_loss: 0.6668, M_d_loss: 0.0534, M_r_loss: 613.3546\n",
      "Epoch [42/250], step [201/244], D_loss: 0.0821, M_loss: 0.5681, M_d_loss: 0.0419, M_r_loss: 526.2666\n",
      "Epoch [42/250], step [241/244], D_loss: 0.0626, M_loss: 0.6184, M_d_loss: 0.0363, M_r_loss: 582.0304\n",
      "Epoch [43/250], step [1/244], D_loss: 0.0583, M_loss: 0.6385, M_d_loss: 0.0412, M_r_loss: 597.3146\n",
      "Epoch [43/250], step [41/244], D_loss: 0.0731, M_loss: 0.5581, M_d_loss: 0.0289, M_r_loss: 529.1971\n",
      "Epoch [43/250], step [81/244], D_loss: 0.0419, M_loss: 0.5707, M_d_loss: 0.0371, M_r_loss: 533.6184\n",
      "Epoch [43/250], step [121/244], D_loss: 0.0638, M_loss: 0.5726, M_d_loss: 0.0347, M_r_loss: 537.9695\n",
      "Epoch [43/250], step [161/244], D_loss: 0.0493, M_loss: 0.6039, M_d_loss: 0.0404, M_r_loss: 563.4497\n",
      "Epoch [43/250], step [201/244], D_loss: 0.0672, M_loss: 0.5932, M_d_loss: 0.0344, M_r_loss: 558.7509\n",
      "Epoch [43/250], step [241/244], D_loss: 0.0824, M_loss: 0.6008, M_d_loss: 0.0537, M_r_loss: 547.0521\n",
      "Epoch [44/250], step [1/244], D_loss: 0.0754, M_loss: 0.6418, M_d_loss: 0.0650, M_r_loss: 576.8065\n",
      "Epoch [44/250], step [41/244], D_loss: 0.0627, M_loss: 0.6018, M_d_loss: 0.0453, M_r_loss: 556.4913\n",
      "Epoch [44/250], step [81/244], D_loss: 0.0413, M_loss: 0.5986, M_d_loss: 0.0318, M_r_loss: 566.8373\n",
      "Epoch [44/250], step [121/244], D_loss: 0.0396, M_loss: 0.5951, M_d_loss: 0.0333, M_r_loss: 561.8398\n",
      "Epoch [44/250], step [161/244], D_loss: 0.0854, M_loss: 0.5629, M_d_loss: 0.0430, M_r_loss: 519.9481\n",
      "Epoch [44/250], step [201/244], D_loss: 0.0560, M_loss: 0.6191, M_d_loss: 0.0433, M_r_loss: 575.8081\n",
      "Epoch [44/250], step [241/244], D_loss: 0.0568, M_loss: 0.6385, M_d_loss: 0.0474, M_r_loss: 591.0668\n",
      "Epoch [45/250], step [1/244], D_loss: 0.0629, M_loss: 0.6137, M_d_loss: 0.0459, M_r_loss: 567.7836\n",
      "Epoch [45/250], step [41/244], D_loss: 0.0809, M_loss: 0.5975, M_d_loss: 0.0550, M_r_loss: 542.4800\n",
      "Epoch [45/250], step [81/244], D_loss: 0.0499, M_loss: 0.5344, M_d_loss: 0.0377, M_r_loss: 496.6722\n",
      "Epoch [45/250], step [121/244], D_loss: 0.0459, M_loss: 0.6222, M_d_loss: 0.0352, M_r_loss: 587.0452\n",
      "Epoch [45/250], step [161/244], D_loss: 0.0556, M_loss: 0.6268, M_d_loss: 0.0388, M_r_loss: 587.9371\n",
      "Epoch [45/250], step [201/244], D_loss: 0.0610, M_loss: 0.5954, M_d_loss: 0.0361, M_r_loss: 559.3002\n",
      "Epoch [45/250], step [241/244], D_loss: 0.0400, M_loss: 0.5950, M_d_loss: 0.0393, M_r_loss: 555.6965\n",
      "Epoch [46/250], step [1/244], D_loss: 0.0570, M_loss: 0.6120, M_d_loss: 0.0519, M_r_loss: 560.1508\n",
      "Epoch [46/250], step [41/244], D_loss: 0.0606, M_loss: 0.6232, M_d_loss: 0.0530, M_r_loss: 570.1407\n",
      "Epoch [46/250], step [81/244], D_loss: 0.0644, M_loss: 0.5868, M_d_loss: 0.0368, M_r_loss: 549.9850\n",
      "Epoch [46/250], step [121/244], D_loss: 0.0498, M_loss: 0.5668, M_d_loss: 0.0357, M_r_loss: 531.1378\n",
      "Epoch [46/250], step [161/244], D_loss: 0.0685, M_loss: 0.5691, M_d_loss: 0.0321, M_r_loss: 537.0484\n",
      "Epoch [46/250], step [201/244], D_loss: 0.0710, M_loss: 0.6158, M_d_loss: 0.0332, M_r_loss: 582.6133\n",
      "Epoch [46/250], step [241/244], D_loss: 0.0534, M_loss: 0.5793, M_d_loss: 0.0353, M_r_loss: 543.9960\n",
      "Epoch [47/250], step [1/244], D_loss: 0.0581, M_loss: 0.5922, M_d_loss: 0.0386, M_r_loss: 553.5967\n",
      "Epoch [47/250], step [41/244], D_loss: 0.0667, M_loss: 0.6101, M_d_loss: 0.0355, M_r_loss: 574.5951\n",
      "Epoch [47/250], step [81/244], D_loss: 0.0620, M_loss: 0.6233, M_d_loss: 0.0413, M_r_loss: 581.9539\n",
      "Epoch [47/250], step [121/244], D_loss: 0.0563, M_loss: 0.5626, M_d_loss: 0.0355, M_r_loss: 527.0314\n",
      "Epoch [47/250], step [161/244], D_loss: 0.0548, M_loss: 0.5771, M_d_loss: 0.0334, M_r_loss: 543.6565\n",
      "Epoch [47/250], step [201/244], D_loss: 0.0635, M_loss: 0.5816, M_d_loss: 0.0386, M_r_loss: 543.0830\n",
      "Epoch [47/250], step [241/244], D_loss: 0.0460, M_loss: 0.6051, M_d_loss: 0.0438, M_r_loss: 561.3851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [48/250], step [1/244], D_loss: 0.0505, M_loss: 0.6090, M_d_loss: 0.0437, M_r_loss: 565.3163\n",
      "Epoch [48/250], step [41/244], D_loss: 0.0530, M_loss: 0.6328, M_d_loss: 0.0408, M_r_loss: 591.9965\n",
      "Epoch [48/250], step [81/244], D_loss: 0.0429, M_loss: 0.6064, M_d_loss: 0.0357, M_r_loss: 570.7272\n",
      "Epoch [48/250], step [121/244], D_loss: 0.0438, M_loss: 0.6156, M_d_loss: 0.0385, M_r_loss: 577.1026\n",
      "Epoch [48/250], step [161/244], D_loss: 0.1106, M_loss: 0.5880, M_d_loss: 0.0486, M_r_loss: 539.3951\n",
      "Epoch [48/250], step [201/244], D_loss: 0.0471, M_loss: 0.6206, M_d_loss: 0.0413, M_r_loss: 579.2974\n",
      "Epoch [48/250], step [241/244], D_loss: 0.0513, M_loss: 0.6022, M_d_loss: 0.0389, M_r_loss: 563.3142\n",
      "Epoch [49/250], step [1/244], D_loss: 0.0619, M_loss: 0.6153, M_d_loss: 0.0442, M_r_loss: 571.1532\n",
      "Epoch [49/250], step [41/244], D_loss: 0.0837, M_loss: 0.6162, M_d_loss: 0.0517, M_r_loss: 564.5035\n",
      "Epoch [49/250], step [81/244], D_loss: 0.0874, M_loss: 0.5936, M_d_loss: 0.0418, M_r_loss: 551.8368\n",
      "Epoch [49/250], step [121/244], D_loss: 0.0726, M_loss: 0.5733, M_d_loss: 0.0337, M_r_loss: 539.5762\n",
      "Epoch [49/250], step [161/244], D_loss: 0.0637, M_loss: 0.5506, M_d_loss: 0.0375, M_r_loss: 513.0378\n",
      "Epoch [49/250], step [201/244], D_loss: 0.0438, M_loss: 0.6367, M_d_loss: 0.0420, M_r_loss: 594.7004\n",
      "Epoch [49/250], step [241/244], D_loss: 0.0400, M_loss: 0.6136, M_d_loss: 0.0379, M_r_loss: 575.7262\n",
      "Epoch [50/250], step [1/244], D_loss: 0.0410, M_loss: 0.5839, M_d_loss: 0.0421, M_r_loss: 541.8137\n",
      "Epoch [50/250], step [41/244], D_loss: 0.0588, M_loss: 0.6662, M_d_loss: 0.0397, M_r_loss: 626.5070\n",
      "Epoch [50/250], step [81/244], D_loss: 0.1093, M_loss: 0.6258, M_d_loss: 0.0518, M_r_loss: 574.0241\n",
      "Epoch [50/250], step [121/244], D_loss: 0.0574, M_loss: 0.5830, M_d_loss: 0.0328, M_r_loss: 550.2194\n",
      "Epoch [50/250], step [161/244], D_loss: 0.0420, M_loss: 0.5689, M_d_loss: 0.0361, M_r_loss: 532.7760\n",
      "Epoch [50/250], step [201/244], D_loss: 0.0576, M_loss: 0.5798, M_d_loss: 0.0395, M_r_loss: 540.2675\n",
      "Epoch [50/250], step [241/244], D_loss: 0.0599, M_loss: 0.6010, M_d_loss: 0.0430, M_r_loss: 557.9384\n",
      "Epoch [51/250], step [1/244], D_loss: 0.0654, M_loss: 0.5991, M_d_loss: 0.0468, M_r_loss: 552.2975\n",
      "Epoch [51/250], step [41/244], D_loss: 0.0650, M_loss: 0.6258, M_d_loss: 0.0455, M_r_loss: 580.2596\n",
      "Epoch [51/250], step [81/244], D_loss: 0.0534, M_loss: 0.5742, M_d_loss: 0.0340, M_r_loss: 540.2159\n",
      "Epoch [51/250], step [121/244], D_loss: 0.0497, M_loss: 0.5857, M_d_loss: 0.0394, M_r_loss: 546.2363\n",
      "Epoch [51/250], step [161/244], D_loss: 0.0473, M_loss: 0.5990, M_d_loss: 0.0404, M_r_loss: 558.6045\n",
      "Epoch [51/250], step [201/244], D_loss: 0.0521, M_loss: 0.6384, M_d_loss: 0.0418, M_r_loss: 596.5849\n",
      "Epoch [51/250], step [241/244], D_loss: 0.0486, M_loss: 0.6052, M_d_loss: 0.0321, M_r_loss: 573.0063\n",
      "Epoch [52/250], step [1/244], D_loss: 0.0617, M_loss: 0.5608, M_d_loss: 0.0439, M_r_loss: 516.9596\n",
      "Epoch [52/250], step [41/244], D_loss: 0.0602, M_loss: 0.5616, M_d_loss: 0.0340, M_r_loss: 527.5439\n",
      "Epoch [52/250], step [81/244], D_loss: 0.0582, M_loss: 0.5759, M_d_loss: 0.0401, M_r_loss: 535.7743\n",
      "Epoch [52/250], step [121/244], D_loss: 0.0625, M_loss: 0.5715, M_d_loss: 0.0412, M_r_loss: 530.2369\n",
      "Epoch [52/250], step [161/244], D_loss: 0.0552, M_loss: 0.5858, M_d_loss: 0.0435, M_r_loss: 542.3788\n",
      "Epoch [52/250], step [201/244], D_loss: 0.0588, M_loss: 0.5857, M_d_loss: 0.0451, M_r_loss: 540.6458\n",
      "Epoch [52/250], step [241/244], D_loss: 0.0612, M_loss: 0.6056, M_d_loss: 0.0447, M_r_loss: 560.8947\n",
      "Epoch [53/250], step [1/244], D_loss: 0.0651, M_loss: 0.5968, M_d_loss: 0.0411, M_r_loss: 555.6815\n",
      "Epoch [53/250], step [41/244], D_loss: 0.0547, M_loss: 0.6220, M_d_loss: 0.0425, M_r_loss: 579.4944\n",
      "Epoch [53/250], step [81/244], D_loss: 0.0545, M_loss: 0.5346, M_d_loss: 0.0385, M_r_loss: 496.0702\n",
      "Epoch [53/250], step [121/244], D_loss: 0.0533, M_loss: 0.6144, M_d_loss: 0.0367, M_r_loss: 577.6869\n",
      "Epoch [53/250], step [161/244], D_loss: 0.0535, M_loss: 0.5630, M_d_loss: 0.0352, M_r_loss: 527.7687\n",
      "Epoch [53/250], step [201/244], D_loss: 0.0500, M_loss: 0.5596, M_d_loss: 0.0422, M_r_loss: 517.4021\n",
      "Epoch [53/250], step [241/244], D_loss: 0.0532, M_loss: 0.5724, M_d_loss: 0.0441, M_r_loss: 528.2386\n",
      "Epoch [54/250], step [1/244], D_loss: 0.0554, M_loss: 0.5683, M_d_loss: 0.0462, M_r_loss: 522.1223\n",
      "Epoch [54/250], step [41/244], D_loss: 0.0547, M_loss: 0.6271, M_d_loss: 0.0440, M_r_loss: 583.1614\n",
      "Epoch [54/250], step [81/244], D_loss: 0.0529, M_loss: 0.6247, M_d_loss: 0.0422, M_r_loss: 582.4845\n",
      "Epoch [54/250], step [121/244], D_loss: 0.0642, M_loss: 0.5523, M_d_loss: 0.0390, M_r_loss: 513.3288\n",
      "Epoch [54/250], step [161/244], D_loss: 0.0492, M_loss: 0.5826, M_d_loss: 0.0356, M_r_loss: 547.0080\n",
      "Epoch [54/250], step [201/244], D_loss: 0.0530, M_loss: 0.6282, M_d_loss: 0.0449, M_r_loss: 583.3654\n",
      "Epoch [54/250], step [241/244], D_loss: 0.0574, M_loss: 0.6221, M_d_loss: 0.0426, M_r_loss: 579.5228\n",
      "Epoch [55/250], step [1/244], D_loss: 0.0635, M_loss: 0.6070, M_d_loss: 0.0479, M_r_loss: 559.1545\n",
      "Epoch [55/250], step [41/244], D_loss: 0.0588, M_loss: 0.5358, M_d_loss: 0.0369, M_r_loss: 498.9536\n",
      "Epoch [55/250], step [81/244], D_loss: 0.0575, M_loss: 0.6090, M_d_loss: 0.0447, M_r_loss: 564.3057\n",
      "Epoch [55/250], step [121/244], D_loss: 0.0576, M_loss: 0.5752, M_d_loss: 0.0439, M_r_loss: 531.3325\n",
      "Epoch [55/250], step [161/244], D_loss: 0.0504, M_loss: 0.6483, M_d_loss: 0.0431, M_r_loss: 605.2083\n",
      "Epoch [55/250], step [201/244], D_loss: 0.0649, M_loss: 0.5863, M_d_loss: 0.0339, M_r_loss: 552.4089\n",
      "Epoch [55/250], step [241/244], D_loss: 0.0537, M_loss: 0.5997, M_d_loss: 0.0386, M_r_loss: 561.1022\n",
      "Epoch [56/250], step [1/244], D_loss: 0.0603, M_loss: 0.6223, M_d_loss: 0.0441, M_r_loss: 578.2790\n",
      "Epoch [56/250], step [41/244], D_loss: 0.0457, M_loss: 0.5990, M_d_loss: 0.0422, M_r_loss: 556.8510\n",
      "Epoch [56/250], step [81/244], D_loss: 0.0617, M_loss: 0.5532, M_d_loss: 0.0397, M_r_loss: 513.5691\n",
      "Epoch [56/250], step [121/244], D_loss: 0.0617, M_loss: 0.5755, M_d_loss: 0.0422, M_r_loss: 533.2899\n",
      "Epoch [56/250], step [161/244], D_loss: 0.0558, M_loss: 0.6290, M_d_loss: 0.0420, M_r_loss: 587.0811\n",
      "Epoch [56/250], step [201/244], D_loss: 0.0495, M_loss: 0.5745, M_d_loss: 0.0375, M_r_loss: 537.0248\n",
      "Epoch [56/250], step [241/244], D_loss: 0.0500, M_loss: 0.5826, M_d_loss: 0.0404, M_r_loss: 542.1649\n",
      "Epoch [57/250], step [1/244], D_loss: 0.0535, M_loss: 0.6022, M_d_loss: 0.0407, M_r_loss: 561.4631\n",
      "Epoch [57/250], step [41/244], D_loss: 0.0682, M_loss: 0.5976, M_d_loss: 0.0399, M_r_loss: 557.6791\n",
      "Epoch [57/250], step [81/244], D_loss: 0.0493, M_loss: 0.6164, M_d_loss: 0.0389, M_r_loss: 577.4679\n",
      "Epoch [57/250], step [121/244], D_loss: 0.0527, M_loss: 0.5669, M_d_loss: 0.0407, M_r_loss: 526.2136\n",
      "Epoch [57/250], step [161/244], D_loss: 0.0503, M_loss: 0.5969, M_d_loss: 0.0412, M_r_loss: 555.7253\n",
      "Epoch [57/250], step [201/244], D_loss: 0.0635, M_loss: 0.5814, M_d_loss: 0.0424, M_r_loss: 538.9802\n",
      "Epoch [57/250], step [241/244], D_loss: 0.0820, M_loss: 0.5620, M_d_loss: 0.0428, M_r_loss: 519.1943\n",
      "Epoch [58/250], step [1/244], D_loss: 0.0720, M_loss: 0.5881, M_d_loss: 0.0496, M_r_loss: 538.5056\n",
      "Epoch [58/250], step [41/244], D_loss: 0.0543, M_loss: 0.5791, M_d_loss: 0.0358, M_r_loss: 543.3220\n",
      "Epoch [58/250], step [81/244], D_loss: 0.0559, M_loss: 0.5706, M_d_loss: 0.0352, M_r_loss: 535.4094\n",
      "Epoch [58/250], step [121/244], D_loss: 0.0540, M_loss: 0.5610, M_d_loss: 0.0368, M_r_loss: 524.1999\n",
      "Epoch [58/250], step [161/244], D_loss: 0.0550, M_loss: 0.5639, M_d_loss: 0.0331, M_r_loss: 530.7732\n",
      "Epoch [58/250], step [201/244], D_loss: 0.0689, M_loss: 0.5953, M_d_loss: 0.0451, M_r_loss: 550.2726\n",
      "Epoch [58/250], step [241/244], D_loss: 0.0703, M_loss: 0.6036, M_d_loss: 0.0387, M_r_loss: 564.8817\n",
      "Epoch [59/250], step [1/244], D_loss: 0.0736, M_loss: 0.6065, M_d_loss: 0.0421, M_r_loss: 564.3140\n",
      "Epoch [59/250], step [41/244], D_loss: 0.0591, M_loss: 0.5714, M_d_loss: 0.0379, M_r_loss: 533.4242\n",
      "Epoch [59/250], step [81/244], D_loss: 0.0562, M_loss: 0.6027, M_d_loss: 0.0377, M_r_loss: 565.0178\n",
      "Epoch [59/250], step [121/244], D_loss: 0.0640, M_loss: 0.5427, M_d_loss: 0.0393, M_r_loss: 503.3732\n",
      "Epoch [59/250], step [161/244], D_loss: 0.0598, M_loss: 0.5858, M_d_loss: 0.0328, M_r_loss: 553.0812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [59/250], step [201/244], D_loss: 0.0834, M_loss: 0.5948, M_d_loss: 0.0472, M_r_loss: 547.6044\n",
      "Epoch [59/250], step [241/244], D_loss: 0.0646, M_loss: 0.5904, M_d_loss: 0.0434, M_r_loss: 546.9684\n",
      "Epoch [60/250], step [1/244], D_loss: 0.0666, M_loss: 0.6155, M_d_loss: 0.0519, M_r_loss: 563.5981\n",
      "Epoch [60/250], step [41/244], D_loss: 0.0551, M_loss: 0.6220, M_d_loss: 0.0441, M_r_loss: 577.9005\n",
      "Epoch [60/250], step [81/244], D_loss: 0.0528, M_loss: 0.5562, M_d_loss: 0.0322, M_r_loss: 523.9988\n",
      "Epoch [60/250], step [121/244], D_loss: 0.0660, M_loss: 0.5897, M_d_loss: 0.0376, M_r_loss: 552.0881\n",
      "Epoch [60/250], step [161/244], D_loss: 0.0689, M_loss: 0.5728, M_d_loss: 0.0449, M_r_loss: 527.8964\n",
      "Epoch [60/250], step [201/244], D_loss: 0.0521, M_loss: 0.5372, M_d_loss: 0.0383, M_r_loss: 498.8884\n",
      "Epoch [60/250], step [241/244], D_loss: 0.0688, M_loss: 0.5864, M_d_loss: 0.0391, M_r_loss: 547.2357\n",
      "Epoch [61/250], step [1/244], D_loss: 0.0566, M_loss: 0.5798, M_d_loss: 0.0417, M_r_loss: 538.1240\n",
      "Epoch [61/250], step [41/244], D_loss: 0.0613, M_loss: 0.6297, M_d_loss: 0.0471, M_r_loss: 582.5764\n",
      "Epoch [61/250], step [81/244], D_loss: 0.0543, M_loss: 0.6139, M_d_loss: 0.0430, M_r_loss: 570.8409\n",
      "Epoch [61/250], step [121/244], D_loss: 0.0467, M_loss: 0.5985, M_d_loss: 0.0355, M_r_loss: 562.9645\n",
      "Epoch [61/250], step [161/244], D_loss: 0.0683, M_loss: 0.6403, M_d_loss: 0.0458, M_r_loss: 594.4993\n",
      "Epoch [61/250], step [201/244], D_loss: 0.0695, M_loss: 0.6081, M_d_loss: 0.0448, M_r_loss: 563.2574\n",
      "Epoch [61/250], step [241/244], D_loss: 0.0582, M_loss: 0.5955, M_d_loss: 0.0463, M_r_loss: 549.1914\n",
      "Epoch [62/250], step [1/244], D_loss: 0.0535, M_loss: 0.5988, M_d_loss: 0.0508, M_r_loss: 548.0318\n",
      "Epoch [62/250], step [41/244], D_loss: 0.0614, M_loss: 0.5944, M_d_loss: 0.0460, M_r_loss: 548.4519\n",
      "Epoch [62/250], step [81/244], D_loss: 0.0636, M_loss: 0.5854, M_d_loss: 0.0418, M_r_loss: 543.5153\n",
      "Epoch [62/250], step [121/244], D_loss: 0.0630, M_loss: 0.5809, M_d_loss: 0.0372, M_r_loss: 543.6998\n",
      "Epoch [62/250], step [161/244], D_loss: 0.0553, M_loss: 0.5795, M_d_loss: 0.0372, M_r_loss: 542.2510\n",
      "Epoch [62/250], step [201/244], D_loss: 0.0496, M_loss: 0.5365, M_d_loss: 0.0340, M_r_loss: 502.5113\n",
      "Epoch [62/250], step [241/244], D_loss: 0.0552, M_loss: 0.6114, M_d_loss: 0.0404, M_r_loss: 570.9645\n",
      "Epoch [63/250], step [1/244], D_loss: 0.0621, M_loss: 0.5941, M_d_loss: 0.0446, M_r_loss: 549.5165\n",
      "Epoch [63/250], step [41/244], D_loss: 0.0516, M_loss: 0.5761, M_d_loss: 0.0439, M_r_loss: 532.1917\n",
      "Epoch [63/250], step [81/244], D_loss: 0.0545, M_loss: 0.5788, M_d_loss: 0.0393, M_r_loss: 539.4732\n",
      "Epoch [63/250], step [121/244], D_loss: 0.0622, M_loss: 0.5791, M_d_loss: 0.0426, M_r_loss: 536.5681\n",
      "Epoch [63/250], step [161/244], D_loss: 0.0788, M_loss: 0.6505, M_d_loss: 0.0477, M_r_loss: 602.7359\n",
      "Epoch [63/250], step [201/244], D_loss: 0.0564, M_loss: 0.6198, M_d_loss: 0.0474, M_r_loss: 572.4106\n",
      "Epoch [63/250], step [241/244], D_loss: 0.0498, M_loss: 0.6012, M_d_loss: 0.0392, M_r_loss: 561.9343\n",
      "Epoch [64/250], step [1/244], D_loss: 0.0534, M_loss: 0.6111, M_d_loss: 0.0413, M_r_loss: 569.7921\n",
      "Epoch [64/250], step [41/244], D_loss: 0.0606, M_loss: 0.6097, M_d_loss: 0.0398, M_r_loss: 569.9399\n",
      "Epoch [64/250], step [81/244], D_loss: 0.0549, M_loss: 0.6024, M_d_loss: 0.0451, M_r_loss: 557.2820\n",
      "Epoch [64/250], step [121/244], D_loss: 0.0554, M_loss: 0.6041, M_d_loss: 0.0444, M_r_loss: 559.7504\n",
      "Epoch [64/250], step [161/244], D_loss: 0.0548, M_loss: 0.6115, M_d_loss: 0.0379, M_r_loss: 573.5876\n",
      "Epoch [64/250], step [201/244], D_loss: 0.0624, M_loss: 0.6239, M_d_loss: 0.0401, M_r_loss: 583.8453\n",
      "Epoch [64/250], step [241/244], D_loss: 0.0711, M_loss: 0.5579, M_d_loss: 0.0413, M_r_loss: 516.6522\n",
      "Epoch [65/250], step [1/244], D_loss: 0.0675, M_loss: 0.6608, M_d_loss: 0.0538, M_r_loss: 607.0018\n",
      "Epoch [65/250], step [41/244], D_loss: 0.0808, M_loss: 0.6153, M_d_loss: 0.0489, M_r_loss: 566.4542\n",
      "Epoch [65/250], step [81/244], D_loss: 0.0583, M_loss: 0.6113, M_d_loss: 0.0387, M_r_loss: 572.5897\n",
      "Epoch [65/250], step [121/244], D_loss: 0.0503, M_loss: 0.5728, M_d_loss: 0.0348, M_r_loss: 538.0336\n",
      "Epoch [65/250], step [161/244], D_loss: 0.0479, M_loss: 0.5802, M_d_loss: 0.0370, M_r_loss: 543.1690\n",
      "Epoch [65/250], step [201/244], D_loss: 0.0466, M_loss: 0.5963, M_d_loss: 0.0431, M_r_loss: 553.1864\n",
      "Epoch [65/250], step [241/244], D_loss: 0.0513, M_loss: 0.6203, M_d_loss: 0.0426, M_r_loss: 577.7297\n",
      "Epoch [66/250], step [1/244], D_loss: 0.0482, M_loss: 0.5821, M_d_loss: 0.0456, M_r_loss: 536.5072\n",
      "Epoch [66/250], step [41/244], D_loss: 0.0419, M_loss: 0.5922, M_d_loss: 0.0394, M_r_loss: 552.7980\n",
      "Epoch [66/250], step [81/244], D_loss: 0.0475, M_loss: 0.6181, M_d_loss: 0.0418, M_r_loss: 576.2819\n",
      "Epoch [66/250], step [121/244], D_loss: 0.0604, M_loss: 0.6073, M_d_loss: 0.0448, M_r_loss: 562.4949\n",
      "Epoch [66/250], step [161/244], D_loss: 0.0551, M_loss: 0.6286, M_d_loss: 0.0487, M_r_loss: 579.8384\n",
      "Epoch [66/250], step [201/244], D_loss: 0.0714, M_loss: 0.6287, M_d_loss: 0.0485, M_r_loss: 580.1549\n",
      "Epoch [66/250], step [241/244], D_loss: 0.0588, M_loss: 0.5960, M_d_loss: 0.0350, M_r_loss: 561.0291\n",
      "Epoch [67/250], step [1/244], D_loss: 0.0584, M_loss: 0.5582, M_d_loss: 0.0362, M_r_loss: 521.9965\n",
      "Epoch [67/250], step [41/244], D_loss: 0.0727, M_loss: 0.5681, M_d_loss: 0.0413, M_r_loss: 526.8312\n",
      "Epoch [67/250], step [81/244], D_loss: 0.0575, M_loss: 0.6148, M_d_loss: 0.0448, M_r_loss: 569.9958\n",
      "Epoch [67/250], step [121/244], D_loss: 0.0622, M_loss: 0.6680, M_d_loss: 0.0474, M_r_loss: 620.5533\n",
      "Epoch [67/250], step [161/244], D_loss: 0.0594, M_loss: 0.5929, M_d_loss: 0.0380, M_r_loss: 554.9242\n",
      "Epoch [67/250], step [201/244], D_loss: 0.0435, M_loss: 0.5680, M_d_loss: 0.0338, M_r_loss: 534.1921\n",
      "Epoch [67/250], step [241/244], D_loss: 0.0599, M_loss: 0.6367, M_d_loss: 0.0426, M_r_loss: 594.1167\n",
      "Epoch [68/250], step [1/244], D_loss: 0.0744, M_loss: 0.5969, M_d_loss: 0.0466, M_r_loss: 550.2892\n",
      "Epoch [68/250], step [41/244], D_loss: 0.0498, M_loss: 0.5595, M_d_loss: 0.0388, M_r_loss: 520.7197\n",
      "Epoch [68/250], step [81/244], D_loss: 0.0546, M_loss: 0.5702, M_d_loss: 0.0388, M_r_loss: 531.4521\n",
      "Epoch [68/250], step [121/244], D_loss: 0.0545, M_loss: 0.6027, M_d_loss: 0.0443, M_r_loss: 558.4059\n",
      "Epoch [68/250], step [161/244], D_loss: 0.0624, M_loss: 0.6018, M_d_loss: 0.0389, M_r_loss: 562.9480\n",
      "Epoch [68/250], step [201/244], D_loss: 0.0473, M_loss: 0.5558, M_d_loss: 0.0352, M_r_loss: 520.5923\n",
      "Epoch [68/250], step [241/244], D_loss: 0.0599, M_loss: 0.6177, M_d_loss: 0.0458, M_r_loss: 571.8730\n",
      "Epoch [69/250], step [1/244], D_loss: 0.0687, M_loss: 0.5739, M_d_loss: 0.0415, M_r_loss: 532.4002\n",
      "Epoch [69/250], step [41/244], D_loss: 0.0513, M_loss: 0.5843, M_d_loss: 0.0397, M_r_loss: 544.5746\n",
      "Epoch [69/250], step [81/244], D_loss: 0.0604, M_loss: 0.5914, M_d_loss: 0.0461, M_r_loss: 545.2662\n",
      "Epoch [69/250], step [121/244], D_loss: 0.0606, M_loss: 0.6236, M_d_loss: 0.0447, M_r_loss: 578.9490\n",
      "Epoch [69/250], step [161/244], D_loss: 0.0725, M_loss: 0.5715, M_d_loss: 0.0387, M_r_loss: 532.8506\n",
      "Epoch [69/250], step [201/244], D_loss: 0.0525, M_loss: 0.5960, M_d_loss: 0.0395, M_r_loss: 556.5137\n",
      "Epoch [69/250], step [241/244], D_loss: 0.0619, M_loss: 0.5852, M_d_loss: 0.0455, M_r_loss: 539.7076\n",
      "Epoch [70/250], step [1/244], D_loss: 0.0791, M_loss: 0.5873, M_d_loss: 0.0443, M_r_loss: 542.9636\n",
      "Epoch [70/250], step [41/244], D_loss: 0.0518, M_loss: 0.5884, M_d_loss: 0.0363, M_r_loss: 552.1147\n",
      "Epoch [70/250], step [81/244], D_loss: 0.0491, M_loss: 0.5985, M_d_loss: 0.0436, M_r_loss: 554.8849\n",
      "Epoch [70/250], step [121/244], D_loss: 0.0480, M_loss: 0.5633, M_d_loss: 0.0383, M_r_loss: 525.0263\n",
      "Epoch [70/250], step [161/244], D_loss: 0.0620, M_loss: 0.6160, M_d_loss: 0.0474, M_r_loss: 568.5615\n",
      "Epoch [70/250], step [201/244], D_loss: 0.0478, M_loss: 0.5829, M_d_loss: 0.0342, M_r_loss: 548.6567\n",
      "Epoch [70/250], step [241/244], D_loss: 0.0670, M_loss: 0.5860, M_d_loss: 0.0363, M_r_loss: 549.7305\n",
      "Epoch [71/250], step [1/244], D_loss: 0.0665, M_loss: 0.5828, M_d_loss: 0.0448, M_r_loss: 538.0392\n",
      "Epoch [71/250], step [41/244], D_loss: 0.0639, M_loss: 0.6122, M_d_loss: 0.0497, M_r_loss: 562.4944\n",
      "Epoch [71/250], step [81/244], D_loss: 0.0562, M_loss: 0.5871, M_d_loss: 0.0433, M_r_loss: 543.8109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [71/250], step [121/244], D_loss: 0.0642, M_loss: 0.5926, M_d_loss: 0.0355, M_r_loss: 557.1335\n",
      "Epoch [71/250], step [161/244], D_loss: 0.0730, M_loss: 0.6262, M_d_loss: 0.0428, M_r_loss: 583.4647\n",
      "Epoch [71/250], step [201/244], D_loss: 0.0564, M_loss: 0.5816, M_d_loss: 0.0366, M_r_loss: 545.0110\n",
      "Epoch [71/250], step [241/244], D_loss: 0.0531, M_loss: 0.6123, M_d_loss: 0.0464, M_r_loss: 565.8957\n",
      "Epoch [72/250], step [1/244], D_loss: 0.0511, M_loss: 0.5723, M_d_loss: 0.0469, M_r_loss: 525.4279\n",
      "Epoch [72/250], step [41/244], D_loss: 0.0654, M_loss: 0.6215, M_d_loss: 0.0517, M_r_loss: 569.8307\n",
      "Epoch [72/250], step [81/244], D_loss: 0.0567, M_loss: 0.5733, M_d_loss: 0.0404, M_r_loss: 532.9430\n",
      "Epoch [72/250], step [121/244], D_loss: 0.0649, M_loss: 0.6172, M_d_loss: 0.0366, M_r_loss: 580.5189\n",
      "Epoch [72/250], step [161/244], D_loss: 0.0511, M_loss: 0.5862, M_d_loss: 0.0402, M_r_loss: 546.0593\n",
      "Epoch [72/250], step [201/244], D_loss: 0.0538, M_loss: 0.5318, M_d_loss: 0.0401, M_r_loss: 491.7751\n",
      "Epoch [72/250], step [241/244], D_loss: 0.0478, M_loss: 0.5706, M_d_loss: 0.0392, M_r_loss: 531.4307\n",
      "Epoch [73/250], step [1/244], D_loss: 0.0464, M_loss: 0.6212, M_d_loss: 0.0447, M_r_loss: 576.5480\n",
      "Epoch [73/250], step [41/244], D_loss: 0.0499, M_loss: 0.6082, M_d_loss: 0.0424, M_r_loss: 565.7267\n",
      "Epoch [73/250], step [81/244], D_loss: 0.0618, M_loss: 0.5958, M_d_loss: 0.0414, M_r_loss: 554.4279\n",
      "Epoch [73/250], step [121/244], D_loss: 0.0705, M_loss: 0.6016, M_d_loss: 0.0425, M_r_loss: 559.0817\n",
      "Epoch [73/250], step [161/244], D_loss: 0.0538, M_loss: 0.5661, M_d_loss: 0.0367, M_r_loss: 529.3834\n",
      "Epoch [73/250], step [201/244], D_loss: 0.0442, M_loss: 0.5859, M_d_loss: 0.0388, M_r_loss: 547.1880\n",
      "Epoch [73/250], step [241/244], D_loss: 0.0552, M_loss: 0.5597, M_d_loss: 0.0382, M_r_loss: 521.5185\n",
      "Epoch [74/250], step [1/244], D_loss: 0.0556, M_loss: 0.5992, M_d_loss: 0.0398, M_r_loss: 559.3162\n",
      "Epoch [74/250], step [41/244], D_loss: 0.0492, M_loss: 0.5870, M_d_loss: 0.0363, M_r_loss: 550.7168\n",
      "Epoch [74/250], step [81/244], D_loss: 0.0542, M_loss: 0.5962, M_d_loss: 0.0460, M_r_loss: 550.1840\n",
      "Epoch [74/250], step [121/244], D_loss: 0.0695, M_loss: 0.5882, M_d_loss: 0.0368, M_r_loss: 551.3794\n",
      "Epoch [74/250], step [161/244], D_loss: 0.0662, M_loss: 0.5583, M_d_loss: 0.0368, M_r_loss: 521.4968\n",
      "Epoch [74/250], step [201/244], D_loss: 0.0628, M_loss: 0.5907, M_d_loss: 0.0354, M_r_loss: 555.3529\n",
      "Epoch [74/250], step [241/244], D_loss: 0.0571, M_loss: 0.5775, M_d_loss: 0.0373, M_r_loss: 540.1490\n",
      "Epoch [75/250], step [1/244], D_loss: 0.0702, M_loss: 0.5881, M_d_loss: 0.0399, M_r_loss: 548.1328\n",
      "Epoch [75/250], step [41/244], D_loss: 0.0583, M_loss: 0.5923, M_d_loss: 0.0484, M_r_loss: 543.8672\n",
      "Epoch [75/250], step [81/244], D_loss: 0.0601, M_loss: 0.6213, M_d_loss: 0.0434, M_r_loss: 577.8243\n",
      "Epoch [75/250], step [121/244], D_loss: 0.0657, M_loss: 0.5949, M_d_loss: 0.0374, M_r_loss: 557.5094\n",
      "Epoch [75/250], step [161/244], D_loss: 0.0475, M_loss: 0.6214, M_d_loss: 0.0369, M_r_loss: 584.4674\n",
      "Epoch [75/250], step [201/244], D_loss: 0.0789, M_loss: 0.5736, M_d_loss: 0.0381, M_r_loss: 535.4811\n",
      "Epoch [75/250], step [241/244], D_loss: 0.0551, M_loss: 0.5999, M_d_loss: 0.0408, M_r_loss: 559.0482\n",
      "Epoch [76/250], step [1/244], D_loss: 0.0727, M_loss: 0.6612, M_d_loss: 0.0630, M_r_loss: 598.2679\n",
      "Epoch [76/250], step [41/244], D_loss: 0.0675, M_loss: 0.5977, M_d_loss: 0.0410, M_r_loss: 556.7316\n",
      "Epoch [76/250], step [81/244], D_loss: 0.0618, M_loss: 0.5480, M_d_loss: 0.0367, M_r_loss: 511.3087\n",
      "Epoch [76/250], step [121/244], D_loss: 0.0630, M_loss: 0.6158, M_d_loss: 0.0455, M_r_loss: 570.3467\n",
      "Epoch [76/250], step [161/244], D_loss: 0.0553, M_loss: 0.6464, M_d_loss: 0.0404, M_r_loss: 605.9941\n",
      "Epoch [76/250], step [201/244], D_loss: 0.0495, M_loss: 0.5892, M_d_loss: 0.0434, M_r_loss: 545.8184\n",
      "Epoch [76/250], step [241/244], D_loss: 0.0425, M_loss: 0.5794, M_d_loss: 0.0448, M_r_loss: 534.5645\n",
      "Epoch [77/250], step [1/244], D_loss: 0.0556, M_loss: 0.5679, M_d_loss: 0.0533, M_r_loss: 514.5572\n",
      "Epoch [77/250], step [41/244], D_loss: 0.0556, M_loss: 0.5836, M_d_loss: 0.0400, M_r_loss: 543.5908\n",
      "Epoch [77/250], step [81/244], D_loss: 0.0576, M_loss: 0.5747, M_d_loss: 0.0451, M_r_loss: 529.6412\n",
      "Epoch [77/250], step [121/244], D_loss: 0.0598, M_loss: 0.5943, M_d_loss: 0.0373, M_r_loss: 556.9919\n",
      "Epoch [77/250], step [161/244], D_loss: 0.0741, M_loss: 0.5522, M_d_loss: 0.0377, M_r_loss: 514.4604\n",
      "Epoch [77/250], step [201/244], D_loss: 0.0548, M_loss: 0.5926, M_d_loss: 0.0388, M_r_loss: 553.8790\n",
      "Epoch [77/250], step [241/244], D_loss: 0.0549, M_loss: 0.6048, M_d_loss: 0.0436, M_r_loss: 561.2310\n",
      "Epoch [78/250], step [1/244], D_loss: 0.0656, M_loss: 0.5872, M_d_loss: 0.0473, M_r_loss: 539.8381\n",
      "Epoch [78/250], step [41/244], D_loss: 0.0633, M_loss: 0.6512, M_d_loss: 0.0413, M_r_loss: 609.9481\n",
      "Epoch [78/250], step [81/244], D_loss: 0.0481, M_loss: 0.6140, M_d_loss: 0.0385, M_r_loss: 575.4951\n",
      "Epoch [78/250], step [121/244], D_loss: 0.0579, M_loss: 0.5601, M_d_loss: 0.0376, M_r_loss: 522.5177\n",
      "Epoch [78/250], step [161/244], D_loss: 0.0528, M_loss: 0.5948, M_d_loss: 0.0370, M_r_loss: 557.8116\n",
      "Epoch [78/250], step [201/244], D_loss: 0.0556, M_loss: 0.5912, M_d_loss: 0.0401, M_r_loss: 551.1072\n",
      "Epoch [78/250], step [241/244], D_loss: 0.0734, M_loss: 0.5775, M_d_loss: 0.0395, M_r_loss: 538.0479\n",
      "Epoch [79/250], step [1/244], D_loss: 0.0703, M_loss: 0.5831, M_d_loss: 0.0485, M_r_loss: 534.6448\n",
      "Epoch [79/250], step [41/244], D_loss: 0.0643, M_loss: 0.5927, M_d_loss: 0.0423, M_r_loss: 550.3932\n",
      "Epoch [79/250], step [81/244], D_loss: 0.0617, M_loss: 0.6208, M_d_loss: 0.0463, M_r_loss: 574.4956\n",
      "Epoch [79/250], step [121/244], D_loss: 0.0673, M_loss: 0.5905, M_d_loss: 0.0398, M_r_loss: 550.6866\n",
      "Epoch [79/250], step [161/244], D_loss: 0.0486, M_loss: 0.5633, M_d_loss: 0.0383, M_r_loss: 524.9684\n",
      "Epoch [79/250], step [201/244], D_loss: 0.0587, M_loss: 0.5745, M_d_loss: 0.0385, M_r_loss: 535.9945\n",
      "Epoch [79/250], step [241/244], D_loss: 0.0686, M_loss: 0.5680, M_d_loss: 0.0376, M_r_loss: 530.3398\n",
      "Epoch [80/250], step [1/244], D_loss: 0.0634, M_loss: 0.5823, M_d_loss: 0.0435, M_r_loss: 538.7837\n",
      "Epoch [80/250], step [41/244], D_loss: 0.0721, M_loss: 0.6059, M_d_loss: 0.0369, M_r_loss: 569.0929\n",
      "Epoch [80/250], step [81/244], D_loss: 0.0617, M_loss: 0.6222, M_d_loss: 0.0422, M_r_loss: 580.0105\n",
      "Epoch [80/250], step [121/244], D_loss: 0.0496, M_loss: 0.5877, M_d_loss: 0.0388, M_r_loss: 548.9126\n",
      "Epoch [80/250], step [161/244], D_loss: 0.0602, M_loss: 0.5779, M_d_loss: 0.0399, M_r_loss: 537.9807\n",
      "Epoch [80/250], step [201/244], D_loss: 0.0473, M_loss: 0.5570, M_d_loss: 0.0402, M_r_loss: 516.7784\n",
      "Epoch [80/250], step [241/244], D_loss: 0.0685, M_loss: 0.5876, M_d_loss: 0.0422, M_r_loss: 545.4395\n",
      "Epoch [81/250], step [1/244], D_loss: 0.0658, M_loss: 0.5930, M_d_loss: 0.0448, M_r_loss: 548.2424\n",
      "Epoch [81/250], step [41/244], D_loss: 0.0687, M_loss: 0.6162, M_d_loss: 0.0392, M_r_loss: 576.9999\n",
      "Epoch [81/250], step [81/244], D_loss: 0.0619, M_loss: 0.5920, M_d_loss: 0.0391, M_r_loss: 552.8790\n",
      "Epoch [81/250], step [121/244], D_loss: 0.0590, M_loss: 0.5750, M_d_loss: 0.0384, M_r_loss: 536.5732\n",
      "Epoch [81/250], step [161/244], D_loss: 0.0604, M_loss: 0.5951, M_d_loss: 0.0369, M_r_loss: 558.2261\n",
      "Epoch [81/250], step [201/244], D_loss: 0.0553, M_loss: 0.5783, M_d_loss: 0.0327, M_r_loss: 545.6048\n",
      "Epoch [81/250], step [241/244], D_loss: 0.0522, M_loss: 0.6249, M_d_loss: 0.0409, M_r_loss: 584.0516\n",
      "Epoch [82/250], step [1/244], D_loss: 0.0678, M_loss: 0.5956, M_d_loss: 0.0384, M_r_loss: 557.1395\n",
      "Epoch [82/250], step [41/244], D_loss: 0.0749, M_loss: 0.5739, M_d_loss: 0.0424, M_r_loss: 531.5055\n",
      "Epoch [82/250], step [81/244], D_loss: 0.0634, M_loss: 0.5997, M_d_loss: 0.0391, M_r_loss: 560.5424\n",
      "Epoch [82/250], step [121/244], D_loss: 0.0622, M_loss: 0.5556, M_d_loss: 0.0394, M_r_loss: 516.1549\n",
      "Epoch [82/250], step [161/244], D_loss: 0.0552, M_loss: 0.5422, M_d_loss: 0.0356, M_r_loss: 506.5245\n",
      "Epoch [82/250], step [201/244], D_loss: 0.0724, M_loss: 0.5612, M_d_loss: 0.0390, M_r_loss: 522.1845\n",
      "Epoch [82/250], step [241/244], D_loss: 0.0589, M_loss: 0.5776, M_d_loss: 0.0372, M_r_loss: 540.3350\n",
      "Epoch [83/250], step [1/244], D_loss: 0.0661, M_loss: 0.5711, M_d_loss: 0.0451, M_r_loss: 525.9851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [83/250], step [41/244], D_loss: 0.0487, M_loss: 0.5552, M_d_loss: 0.0376, M_r_loss: 517.5939\n",
      "Epoch [83/250], step [81/244], D_loss: 0.0514, M_loss: 0.5817, M_d_loss: 0.0404, M_r_loss: 541.3330\n",
      "Epoch [83/250], step [121/244], D_loss: 0.0544, M_loss: 0.5785, M_d_loss: 0.0414, M_r_loss: 537.1562\n",
      "Epoch [83/250], step [161/244], D_loss: 0.0554, M_loss: 0.5843, M_d_loss: 0.0418, M_r_loss: 542.5175\n",
      "Epoch [83/250], step [201/244], D_loss: 0.0522, M_loss: 0.5807, M_d_loss: 0.0396, M_r_loss: 541.0649\n",
      "Epoch [83/250], step [241/244], D_loss: 0.0530, M_loss: 0.5745, M_d_loss: 0.0372, M_r_loss: 537.3685\n",
      "Epoch [84/250], step [1/244], D_loss: 0.0519, M_loss: 0.5653, M_d_loss: 0.0366, M_r_loss: 528.6191\n",
      "Epoch [84/250], step [41/244], D_loss: 0.0787, M_loss: 0.5788, M_d_loss: 0.0422, M_r_loss: 536.6461\n",
      "Epoch [84/250], step [81/244], D_loss: 0.0782, M_loss: 0.5976, M_d_loss: 0.0449, M_r_loss: 552.6653\n",
      "Epoch [84/250], step [121/244], D_loss: 0.0677, M_loss: 0.6092, M_d_loss: 0.0383, M_r_loss: 570.8843\n",
      "Epoch [84/250], step [161/244], D_loss: 0.0507, M_loss: 0.6023, M_d_loss: 0.0371, M_r_loss: 565.1895\n",
      "Epoch [84/250], step [201/244], D_loss: 0.0481, M_loss: 0.5646, M_d_loss: 0.0359, M_r_loss: 528.6910\n",
      "Epoch [84/250], step [241/244], D_loss: 0.0501, M_loss: 0.6289, M_d_loss: 0.0483, M_r_loss: 580.5984\n",
      "Epoch [85/250], step [1/244], D_loss: 0.0527, M_loss: 0.5574, M_d_loss: 0.0557, M_r_loss: 501.7161\n",
      "Epoch [85/250], step [41/244], D_loss: 0.0692, M_loss: 0.5741, M_d_loss: 0.0403, M_r_loss: 533.7805\n",
      "Epoch [85/250], step [81/244], D_loss: 0.0753, M_loss: 0.5581, M_d_loss: 0.0382, M_r_loss: 519.9538\n",
      "Epoch [85/250], step [121/244], D_loss: 0.0644, M_loss: 0.5911, M_d_loss: 0.0411, M_r_loss: 550.0320\n",
      "Epoch [85/250], step [161/244], D_loss: 0.0618, M_loss: 0.5734, M_d_loss: 0.0432, M_r_loss: 530.2311\n",
      "Epoch [85/250], step [201/244], D_loss: 0.0477, M_loss: 0.6129, M_d_loss: 0.0410, M_r_loss: 571.9005\n",
      "Epoch [85/250], step [241/244], D_loss: 0.0483, M_loss: 0.5402, M_d_loss: 0.0324, M_r_loss: 507.7887\n",
      "Epoch [86/250], step [1/244], D_loss: 0.0492, M_loss: 0.5801, M_d_loss: 0.0347, M_r_loss: 545.3705\n",
      "Epoch [86/250], step [41/244], D_loss: 0.0582, M_loss: 0.5818, M_d_loss: 0.0360, M_r_loss: 545.8239\n",
      "Epoch [86/250], step [81/244], D_loss: 0.0721, M_loss: 0.6168, M_d_loss: 0.0424, M_r_loss: 574.4459\n",
      "Epoch [86/250], step [121/244], D_loss: 0.0707, M_loss: 0.5709, M_d_loss: 0.0433, M_r_loss: 527.6069\n",
      "Epoch [86/250], step [161/244], D_loss: 0.0645, M_loss: 0.6186, M_d_loss: 0.0392, M_r_loss: 579.3610\n",
      "Epoch [86/250], step [201/244], D_loss: 0.0562, M_loss: 0.5502, M_d_loss: 0.0365, M_r_loss: 513.6756\n",
      "Epoch [86/250], step [241/244], D_loss: 0.0607, M_loss: 0.6009, M_d_loss: 0.0421, M_r_loss: 558.8359\n",
      "Epoch [87/250], step [1/244], D_loss: 0.0645, M_loss: 0.5956, M_d_loss: 0.0478, M_r_loss: 547.7538\n",
      "Epoch [87/250], step [41/244], D_loss: 0.0448, M_loss: 0.5637, M_d_loss: 0.0402, M_r_loss: 523.4916\n",
      "Epoch [87/250], step [81/244], D_loss: 0.0604, M_loss: 0.6218, M_d_loss: 0.0493, M_r_loss: 572.5312\n",
      "Epoch [87/250], step [121/244], D_loss: 0.0422, M_loss: 0.5826, M_d_loss: 0.0369, M_r_loss: 545.6777\n",
      "Epoch [87/250], step [161/244], D_loss: 0.0722, M_loss: 0.5837, M_d_loss: 0.0362, M_r_loss: 547.5519\n",
      "Epoch [87/250], step [201/244], D_loss: 0.0699, M_loss: 0.6011, M_d_loss: 0.0456, M_r_loss: 555.4922\n",
      "Epoch [87/250], step [241/244], D_loss: 0.0554, M_loss: 0.5791, M_d_loss: 0.0364, M_r_loss: 542.6790\n",
      "Epoch [88/250], step [1/244], D_loss: 0.0636, M_loss: 0.6146, M_d_loss: 0.0497, M_r_loss: 564.8677\n",
      "Epoch [88/250], step [41/244], D_loss: 0.0608, M_loss: 0.5880, M_d_loss: 0.0386, M_r_loss: 549.3624\n",
      "Epoch [88/250], step [81/244], D_loss: 0.0390, M_loss: 0.6334, M_d_loss: 0.0406, M_r_loss: 592.7751\n",
      "Epoch [88/250], step [121/244], D_loss: 0.0579, M_loss: 0.6385, M_d_loss: 0.0477, M_r_loss: 590.8035\n",
      "Epoch [88/250], step [161/244], D_loss: 0.0516, M_loss: 0.5711, M_d_loss: 0.0380, M_r_loss: 533.1321\n",
      "Epoch [88/250], step [201/244], D_loss: 0.0502, M_loss: 0.5996, M_d_loss: 0.0417, M_r_loss: 557.8782\n",
      "Epoch [88/250], step [241/244], D_loss: 0.0486, M_loss: 0.5767, M_d_loss: 0.0370, M_r_loss: 539.7760\n",
      "Epoch [89/250], step [1/244], D_loss: 0.0542, M_loss: 0.6051, M_d_loss: 0.0405, M_r_loss: 564.6186\n",
      "Epoch [89/250], step [41/244], D_loss: 0.0608, M_loss: 0.5891, M_d_loss: 0.0411, M_r_loss: 548.0447\n",
      "Epoch [89/250], step [81/244], D_loss: 0.0588, M_loss: 0.5906, M_d_loss: 0.0464, M_r_loss: 544.2337\n",
      "Epoch [89/250], step [121/244], D_loss: 0.0521, M_loss: 0.5753, M_d_loss: 0.0382, M_r_loss: 537.0914\n",
      "Epoch [89/250], step [161/244], D_loss: 0.0648, M_loss: 0.5680, M_d_loss: 0.0451, M_r_loss: 522.9225\n",
      "Epoch [89/250], step [201/244], D_loss: 0.0544, M_loss: 0.5867, M_d_loss: 0.0442, M_r_loss: 542.5210\n",
      "Epoch [89/250], step [241/244], D_loss: 0.0646, M_loss: 0.5896, M_d_loss: 0.0448, M_r_loss: 544.8103\n",
      "Epoch [90/250], step [1/244], D_loss: 0.0813, M_loss: 0.6211, M_d_loss: 0.0547, M_r_loss: 566.3165\n",
      "Epoch [90/250], step [41/244], D_loss: 0.0510, M_loss: 0.5799, M_d_loss: 0.0446, M_r_loss: 535.3375\n",
      "Epoch [90/250], step [81/244], D_loss: 0.0763, M_loss: 0.6525, M_d_loss: 0.0456, M_r_loss: 606.9639\n",
      "Epoch [90/250], step [121/244], D_loss: 0.0494, M_loss: 0.5873, M_d_loss: 0.0383, M_r_loss: 548.9751\n",
      "Epoch [90/250], step [161/244], D_loss: 0.0613, M_loss: 0.5845, M_d_loss: 0.0428, M_r_loss: 541.7148\n",
      "Epoch [90/250], step [201/244], D_loss: 0.0776, M_loss: 0.6614, M_d_loss: 0.0529, M_r_loss: 608.4585\n",
      "Epoch [90/250], step [241/244], D_loss: 0.0645, M_loss: 0.6040, M_d_loss: 0.0457, M_r_loss: 558.3711\n",
      "Epoch [91/250], step [1/244], D_loss: 0.0692, M_loss: 0.6198, M_d_loss: 0.0506, M_r_loss: 569.2194\n",
      "Epoch [91/250], step [41/244], D_loss: 0.0555, M_loss: 0.5896, M_d_loss: 0.0424, M_r_loss: 547.2361\n",
      "Epoch [91/250], step [81/244], D_loss: 0.0647, M_loss: 0.5842, M_d_loss: 0.0330, M_r_loss: 551.2249\n",
      "Epoch [91/250], step [121/244], D_loss: 0.0478, M_loss: 0.5373, M_d_loss: 0.0365, M_r_loss: 500.8210\n",
      "Epoch [91/250], step [161/244], D_loss: 0.0485, M_loss: 0.6193, M_d_loss: 0.0412, M_r_loss: 578.1016\n",
      "Epoch [91/250], step [201/244], D_loss: 0.0483, M_loss: 0.6135, M_d_loss: 0.0343, M_r_loss: 579.1750\n",
      "Epoch [91/250], step [241/244], D_loss: 0.0745, M_loss: 0.5548, M_d_loss: 0.0365, M_r_loss: 518.2281\n",
      "Epoch [92/250], step [1/244], D_loss: 0.0707, M_loss: 0.5983, M_d_loss: 0.0421, M_r_loss: 556.1639\n",
      "Epoch [92/250], step [41/244], D_loss: 0.0552, M_loss: 0.5818, M_d_loss: 0.0382, M_r_loss: 543.5477\n",
      "Epoch [92/250], step [81/244], D_loss: 0.0578, M_loss: 0.5543, M_d_loss: 0.0401, M_r_loss: 514.1974\n",
      "Epoch [92/250], step [121/244], D_loss: 0.0652, M_loss: 0.5910, M_d_loss: 0.0431, M_r_loss: 547.8725\n",
      "Epoch [92/250], step [161/244], D_loss: 0.0533, M_loss: 0.6700, M_d_loss: 0.0539, M_r_loss: 616.0746\n",
      "Epoch [92/250], step [201/244], D_loss: 0.0750, M_loss: 0.5879, M_d_loss: 0.0377, M_r_loss: 550.2511\n",
      "Epoch [92/250], step [241/244], D_loss: 0.0519, M_loss: 0.6095, M_d_loss: 0.0432, M_r_loss: 566.2549\n",
      "Epoch [93/250], step [1/244], D_loss: 0.0616, M_loss: 0.6278, M_d_loss: 0.0491, M_r_loss: 578.6406\n",
      "Epoch [93/250], step [41/244], D_loss: 0.0639, M_loss: 0.6443, M_d_loss: 0.0515, M_r_loss: 592.8024\n",
      "Epoch [93/250], step [81/244], D_loss: 0.0491, M_loss: 0.5747, M_d_loss: 0.0371, M_r_loss: 537.5787\n",
      "Epoch [93/250], step [121/244], D_loss: 0.0461, M_loss: 0.5943, M_d_loss: 0.0383, M_r_loss: 556.0480\n",
      "Epoch [93/250], step [161/244], D_loss: 0.0474, M_loss: 0.6281, M_d_loss: 0.0447, M_r_loss: 583.4652\n",
      "Epoch [93/250], step [201/244], D_loss: 0.0589, M_loss: 0.5933, M_d_loss: 0.0416, M_r_loss: 551.7018\n",
      "Epoch [93/250], step [241/244], D_loss: 0.0688, M_loss: 0.5805, M_d_loss: 0.0438, M_r_loss: 536.6992\n",
      "Epoch [94/250], step [1/244], D_loss: 0.0691, M_loss: 0.6335, M_d_loss: 0.0483, M_r_loss: 585.1581\n",
      "Epoch [94/250], step [41/244], D_loss: 0.0561, M_loss: 0.6167, M_d_loss: 0.0452, M_r_loss: 571.4341\n",
      "Epoch [94/250], step [81/244], D_loss: 0.0785, M_loss: 0.6107, M_d_loss: 0.0338, M_r_loss: 576.9009\n",
      "Epoch [94/250], step [121/244], D_loss: 0.0511, M_loss: 0.5714, M_d_loss: 0.0349, M_r_loss: 536.5637\n",
      "Epoch [94/250], step [161/244], D_loss: 0.0600, M_loss: 0.6218, M_d_loss: 0.0508, M_r_loss: 571.0469\n",
      "Epoch [94/250], step [201/244], D_loss: 0.0415, M_loss: 0.5653, M_d_loss: 0.0397, M_r_loss: 525.5743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [94/250], step [241/244], D_loss: 0.0607, M_loss: 0.6113, M_d_loss: 0.0403, M_r_loss: 571.0897\n",
      "Epoch [95/250], step [1/244], D_loss: 0.0629, M_loss: 0.6603, M_d_loss: 0.0429, M_r_loss: 617.3903\n",
      "Epoch [95/250], step [41/244], D_loss: 0.0638, M_loss: 0.5999, M_d_loss: 0.0433, M_r_loss: 556.5562\n",
      "Epoch [95/250], step [81/244], D_loss: 0.0581, M_loss: 0.6171, M_d_loss: 0.0457, M_r_loss: 571.4084\n",
      "Epoch [95/250], step [121/244], D_loss: 0.0574, M_loss: 0.6172, M_d_loss: 0.0403, M_r_loss: 576.8842\n",
      "Epoch [95/250], step [161/244], D_loss: 0.0473, M_loss: 0.6204, M_d_loss: 0.0394, M_r_loss: 581.0077\n",
      "Epoch [95/250], step [201/244], D_loss: 0.0554, M_loss: 0.6059, M_d_loss: 0.0466, M_r_loss: 559.2424\n",
      "Epoch [95/250], step [241/244], D_loss: 0.0615, M_loss: 0.6033, M_d_loss: 0.0515, M_r_loss: 551.7844\n",
      "Epoch [96/250], step [1/244], D_loss: 0.0580, M_loss: 0.6013, M_d_loss: 0.0579, M_r_loss: 543.3103\n",
      "Epoch [96/250], step [41/244], D_loss: 0.0826, M_loss: 0.6186, M_d_loss: 0.0634, M_r_loss: 555.2329\n",
      "Epoch [96/250], step [81/244], D_loss: 0.0781, M_loss: 0.6315, M_d_loss: 0.0488, M_r_loss: 582.6743\n",
      "Epoch [96/250], step [121/244], D_loss: 0.0528, M_loss: 0.5669, M_d_loss: 0.0356, M_r_loss: 531.3352\n",
      "Epoch [96/250], step [161/244], D_loss: 0.0609, M_loss: 0.6165, M_d_loss: 0.0421, M_r_loss: 574.3882\n",
      "Epoch [96/250], step [201/244], D_loss: 0.0451, M_loss: 0.5829, M_d_loss: 0.0414, M_r_loss: 541.5212\n",
      "Epoch [96/250], step [241/244], D_loss: 0.0467, M_loss: 0.5685, M_d_loss: 0.0432, M_r_loss: 525.2532\n",
      "Epoch [97/250], step [1/244], D_loss: 0.0527, M_loss: 0.5489, M_d_loss: 0.0441, M_r_loss: 504.7638\n",
      "Epoch [97/250], step [41/244], D_loss: 0.0362, M_loss: 0.5809, M_d_loss: 0.0375, M_r_loss: 543.4390\n",
      "Epoch [97/250], step [81/244], D_loss: 0.0385, M_loss: 0.5723, M_d_loss: 0.0390, M_r_loss: 533.2948\n",
      "Epoch [97/250], step [121/244], D_loss: 0.0640, M_loss: 0.5928, M_d_loss: 0.0434, M_r_loss: 549.4066\n",
      "Epoch [97/250], step [161/244], D_loss: 0.0540, M_loss: 0.5398, M_d_loss: 0.0423, M_r_loss: 497.5034\n",
      "Epoch [97/250], step [201/244], D_loss: 0.0494, M_loss: 0.5686, M_d_loss: 0.0398, M_r_loss: 528.7683\n",
      "Epoch [97/250], step [241/244], D_loss: 0.0611, M_loss: 0.5623, M_d_loss: 0.0383, M_r_loss: 524.0044\n",
      "Epoch [98/250], step [1/244], D_loss: 0.0632, M_loss: 0.6197, M_d_loss: 0.0460, M_r_loss: 573.6775\n",
      "Epoch [98/250], step [41/244], D_loss: 0.0783, M_loss: 0.5972, M_d_loss: 0.0472, M_r_loss: 550.0527\n",
      "Epoch [98/250], step [81/244], D_loss: 0.0566, M_loss: 0.5871, M_d_loss: 0.0402, M_r_loss: 546.8294\n",
      "Epoch [98/250], step [121/244], D_loss: 0.0538, M_loss: 0.5479, M_d_loss: 0.0391, M_r_loss: 508.8582\n",
      "Epoch [98/250], step [161/244], D_loss: 0.0599, M_loss: 0.5939, M_d_loss: 0.0416, M_r_loss: 552.2964\n",
      "Epoch [98/250], step [201/244], D_loss: 0.0473, M_loss: 0.5987, M_d_loss: 0.0473, M_r_loss: 551.3323\n",
      "Epoch [98/250], step [241/244], D_loss: 0.0582, M_loss: 0.5796, M_d_loss: 0.0431, M_r_loss: 536.5533\n",
      "Epoch [99/250], step [1/244], D_loss: 0.0580, M_loss: 0.6000, M_d_loss: 0.0565, M_r_loss: 543.5681\n",
      "Epoch [99/250], step [41/244], D_loss: 0.0603, M_loss: 0.6403, M_d_loss: 0.0516, M_r_loss: 588.6960\n",
      "Epoch [99/250], step [81/244], D_loss: 0.0637, M_loss: 0.5505, M_d_loss: 0.0373, M_r_loss: 513.1379\n",
      "Epoch [99/250], step [121/244], D_loss: 0.0720, M_loss: 0.5938, M_d_loss: 0.0429, M_r_loss: 550.9760\n",
      "Epoch [99/250], step [161/244], D_loss: 0.0640, M_loss: 0.5799, M_d_loss: 0.0507, M_r_loss: 529.1918\n",
      "Epoch [99/250], step [201/244], D_loss: 0.0482, M_loss: 0.5961, M_d_loss: 0.0332, M_r_loss: 562.8842\n",
      "Epoch [99/250], step [241/244], D_loss: 0.0478, M_loss: 0.5710, M_d_loss: 0.0350, M_r_loss: 535.9738\n",
      "Epoch [100/250], step [1/244], D_loss: 0.0490, M_loss: 0.6003, M_d_loss: 0.0426, M_r_loss: 557.7683\n",
      "Epoch [100/250], step [41/244], D_loss: 0.0515, M_loss: 0.5992, M_d_loss: 0.0488, M_r_loss: 550.3481\n",
      "Epoch [100/250], step [81/244], D_loss: 0.0482, M_loss: 0.5634, M_d_loss: 0.0419, M_r_loss: 521.5785\n",
      "Epoch [100/250], step [121/244], D_loss: 0.0569, M_loss: 0.5918, M_d_loss: 0.0441, M_r_loss: 547.7255\n",
      "Epoch [100/250], step [161/244], D_loss: 0.0447, M_loss: 0.6347, M_d_loss: 0.0484, M_r_loss: 586.2743\n",
      "Epoch [100/250], step [201/244], D_loss: 0.0786, M_loss: 0.6089, M_d_loss: 0.0495, M_r_loss: 559.3990\n",
      "Epoch [100/250], step [241/244], D_loss: 0.0566, M_loss: 0.5761, M_d_loss: 0.0334, M_r_loss: 542.7815\n",
      "Epoch [101/250], step [1/244], D_loss: 0.0629, M_loss: 0.6297, M_d_loss: 0.0425, M_r_loss: 587.1288\n",
      "Epoch [101/250], step [41/244], D_loss: 0.0544, M_loss: 0.6005, M_d_loss: 0.0404, M_r_loss: 560.1103\n",
      "Epoch [101/250], step [81/244], D_loss: 0.0591, M_loss: 0.5431, M_d_loss: 0.0391, M_r_loss: 504.0722\n",
      "Epoch [101/250], step [121/244], D_loss: 0.0612, M_loss: 0.5924, M_d_loss: 0.0384, M_r_loss: 553.9177\n",
      "Epoch [101/250], step [161/244], D_loss: 0.0559, M_loss: 0.5808, M_d_loss: 0.0401, M_r_loss: 540.6840\n",
      "Epoch [101/250], step [201/244], D_loss: 0.0465, M_loss: 0.5626, M_d_loss: 0.0378, M_r_loss: 524.8085\n",
      "Epoch [101/250], step [241/244], D_loss: 0.0449, M_loss: 0.5619, M_d_loss: 0.0351, M_r_loss: 526.8026\n",
      "Epoch [102/250], step [1/244], D_loss: 0.0518, M_loss: 0.5883, M_d_loss: 0.0398, M_r_loss: 548.4594\n",
      "Epoch [102/250], step [41/244], D_loss: 0.0462, M_loss: 0.5925, M_d_loss: 0.0380, M_r_loss: 554.5057\n",
      "Epoch [102/250], step [81/244], D_loss: 0.0468, M_loss: 0.6055, M_d_loss: 0.0376, M_r_loss: 567.8886\n",
      "Epoch [102/250], step [121/244], D_loss: 0.0440, M_loss: 0.5541, M_d_loss: 0.0368, M_r_loss: 517.2428\n",
      "Epoch [102/250], step [161/244], D_loss: 0.0588, M_loss: 0.5905, M_d_loss: 0.0429, M_r_loss: 547.6553\n",
      "Epoch [102/250], step [201/244], D_loss: 0.0630, M_loss: 0.6020, M_d_loss: 0.0395, M_r_loss: 562.5104\n",
      "Epoch [102/250], step [241/244], D_loss: 0.0491, M_loss: 0.5804, M_d_loss: 0.0433, M_r_loss: 537.0295\n",
      "Epoch [103/250], step [1/244], D_loss: 0.0414, M_loss: 0.6034, M_d_loss: 0.0423, M_r_loss: 561.0948\n",
      "Epoch [103/250], step [41/244], D_loss: 0.0531, M_loss: 0.5682, M_d_loss: 0.0380, M_r_loss: 530.1540\n",
      "Epoch [103/250], step [81/244], D_loss: 0.0541, M_loss: 0.5920, M_d_loss: 0.0360, M_r_loss: 555.9283\n",
      "Epoch [103/250], step [121/244], D_loss: 0.0579, M_loss: 0.5848, M_d_loss: 0.0425, M_r_loss: 542.3296\n",
      "Epoch [103/250], step [161/244], D_loss: 0.0510, M_loss: 0.5607, M_d_loss: 0.0386, M_r_loss: 522.0837\n",
      "Epoch [103/250], step [201/244], D_loss: 0.0639, M_loss: 0.5653, M_d_loss: 0.0421, M_r_loss: 523.2546\n",
      "Epoch [103/250], step [241/244], D_loss: 0.0509, M_loss: 0.6106, M_d_loss: 0.0451, M_r_loss: 565.5754\n",
      "Epoch [104/250], step [1/244], D_loss: 0.0451, M_loss: 0.5891, M_d_loss: 0.0456, M_r_loss: 543.4466\n",
      "Epoch [104/250], step [41/244], D_loss: 0.0462, M_loss: 0.6112, M_d_loss: 0.0412, M_r_loss: 569.9525\n",
      "Epoch [104/250], step [81/244], D_loss: 0.0602, M_loss: 0.6067, M_d_loss: 0.0461, M_r_loss: 560.6099\n",
      "Epoch [104/250], step [121/244], D_loss: 0.0599, M_loss: 0.5679, M_d_loss: 0.0432, M_r_loss: 524.7131\n",
      "Epoch [104/250], step [161/244], D_loss: 0.0606, M_loss: 0.5912, M_d_loss: 0.0465, M_r_loss: 544.7529\n",
      "Epoch [104/250], step [201/244], D_loss: 0.0710, M_loss: 0.6005, M_d_loss: 0.0454, M_r_loss: 555.0851\n",
      "Epoch [104/250], step [241/244], D_loss: 0.0506, M_loss: 0.5902, M_d_loss: 0.0391, M_r_loss: 551.0710\n",
      "Epoch [105/250], step [1/244], D_loss: 0.0651, M_loss: 0.5732, M_d_loss: 0.0509, M_r_loss: 522.3240\n",
      "Epoch [105/250], step [41/244], D_loss: 0.0612, M_loss: 0.5911, M_d_loss: 0.0357, M_r_loss: 555.4241\n",
      "Epoch [105/250], step [81/244], D_loss: 0.0569, M_loss: 0.6175, M_d_loss: 0.0404, M_r_loss: 577.0958\n",
      "Epoch [105/250], step [121/244], D_loss: 0.0620, M_loss: 0.5552, M_d_loss: 0.0364, M_r_loss: 518.8325\n",
      "Epoch [105/250], step [161/244], D_loss: 0.0476, M_loss: 0.6095, M_d_loss: 0.0384, M_r_loss: 571.1292\n",
      "Epoch [105/250], step [201/244], D_loss: 0.0496, M_loss: 0.5836, M_d_loss: 0.0374, M_r_loss: 546.2382\n",
      "Epoch [105/250], step [241/244], D_loss: 0.0446, M_loss: 0.5884, M_d_loss: 0.0391, M_r_loss: 549.3206\n",
      "Epoch [106/250], step [1/244], D_loss: 0.0481, M_loss: 0.6237, M_d_loss: 0.0456, M_r_loss: 578.0760\n",
      "Epoch [106/250], step [41/244], D_loss: 0.0478, M_loss: 0.5793, M_d_loss: 0.0411, M_r_loss: 538.2158\n",
      "Epoch [106/250], step [81/244], D_loss: 0.0670, M_loss: 0.5804, M_d_loss: 0.0418, M_r_loss: 538.6548\n",
      "Epoch [106/250], step [121/244], D_loss: 0.0591, M_loss: 0.6220, M_d_loss: 0.0441, M_r_loss: 577.9482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [106/250], step [161/244], D_loss: 0.0549, M_loss: 0.5797, M_d_loss: 0.0381, M_r_loss: 541.5780\n",
      "Epoch [106/250], step [201/244], D_loss: 0.0650, M_loss: 0.6187, M_d_loss: 0.0419, M_r_loss: 576.7544\n",
      "Epoch [106/250], step [241/244], D_loss: 0.0482, M_loss: 0.6107, M_d_loss: 0.0421, M_r_loss: 568.6124\n",
      "Epoch [107/250], step [1/244], D_loss: 0.0573, M_loss: 0.5891, M_d_loss: 0.0432, M_r_loss: 545.8333\n",
      "Epoch [107/250], step [41/244], D_loss: 0.0554, M_loss: 0.5418, M_d_loss: 0.0384, M_r_loss: 503.4662\n",
      "Epoch [107/250], step [81/244], D_loss: 0.0545, M_loss: 0.5922, M_d_loss: 0.0384, M_r_loss: 553.7303\n",
      "Epoch [107/250], step [121/244], D_loss: 0.0485, M_loss: 0.6066, M_d_loss: 0.0412, M_r_loss: 565.4333\n",
      "Epoch [107/250], step [161/244], D_loss: 0.0569, M_loss: 0.5971, M_d_loss: 0.0416, M_r_loss: 555.4312\n",
      "Epoch [107/250], step [201/244], D_loss: 0.0496, M_loss: 0.6106, M_d_loss: 0.0383, M_r_loss: 572.2623\n",
      "Epoch [107/250], step [241/244], D_loss: 0.0612, M_loss: 0.5685, M_d_loss: 0.0442, M_r_loss: 524.3396\n",
      "Epoch [108/250], step [1/244], D_loss: 0.0635, M_loss: 0.6495, M_d_loss: 0.0478, M_r_loss: 601.6607\n",
      "Epoch [108/250], step [41/244], D_loss: 0.0557, M_loss: 0.6025, M_d_loss: 0.0384, M_r_loss: 564.0283\n",
      "Epoch [108/250], step [81/244], D_loss: 0.0712, M_loss: 0.5679, M_d_loss: 0.0414, M_r_loss: 526.5370\n",
      "Epoch [108/250], step [121/244], D_loss: 0.0583, M_loss: 0.5541, M_d_loss: 0.0369, M_r_loss: 517.1169\n",
      "Epoch [108/250], step [161/244], D_loss: 0.0518, M_loss: 0.6132, M_d_loss: 0.0423, M_r_loss: 570.9390\n",
      "Epoch [108/250], step [201/244], D_loss: 0.0429, M_loss: 0.6099, M_d_loss: 0.0406, M_r_loss: 569.3426\n",
      "Epoch [108/250], step [241/244], D_loss: 0.0527, M_loss: 0.5711, M_d_loss: 0.0427, M_r_loss: 528.4668\n",
      "Epoch [109/250], step [1/244], D_loss: 0.0577, M_loss: 0.6006, M_d_loss: 0.0473, M_r_loss: 553.2812\n",
      "Epoch [109/250], step [41/244], D_loss: 0.0605, M_loss: 0.5695, M_d_loss: 0.0405, M_r_loss: 528.9612\n",
      "Epoch [109/250], step [81/244], D_loss: 0.0736, M_loss: 0.5947, M_d_loss: 0.0363, M_r_loss: 558.4495\n",
      "Epoch [109/250], step [121/244], D_loss: 0.0724, M_loss: 0.6011, M_d_loss: 0.0410, M_r_loss: 560.1741\n",
      "Epoch [109/250], step [161/244], D_loss: 0.0524, M_loss: 0.6344, M_d_loss: 0.0490, M_r_loss: 585.4341\n",
      "Epoch [109/250], step [201/244], D_loss: 0.0598, M_loss: 0.6126, M_d_loss: 0.0408, M_r_loss: 571.8120\n",
      "Epoch [109/250], step [241/244], D_loss: 0.0592, M_loss: 0.5735, M_d_loss: 0.0435, M_r_loss: 529.9640\n",
      "Epoch [110/250], step [1/244], D_loss: 0.0632, M_loss: 0.6358, M_d_loss: 0.0449, M_r_loss: 590.8976\n",
      "Epoch [110/250], step [41/244], D_loss: 0.0527, M_loss: 0.6279, M_d_loss: 0.0469, M_r_loss: 580.9882\n",
      "Epoch [110/250], step [81/244], D_loss: 0.0644, M_loss: 0.6221, M_d_loss: 0.0489, M_r_loss: 573.2410\n",
      "Epoch [110/250], step [121/244], D_loss: 0.0561, M_loss: 0.6291, M_d_loss: 0.0410, M_r_loss: 588.0828\n",
      "Epoch [110/250], step [161/244], D_loss: 0.0444, M_loss: 0.6360, M_d_loss: 0.0432, M_r_loss: 592.7747\n",
      "Epoch [110/250], step [201/244], D_loss: 0.0586, M_loss: 0.6306, M_d_loss: 0.0401, M_r_loss: 590.5508\n",
      "Epoch [110/250], step [241/244], D_loss: 0.0591, M_loss: 0.6089, M_d_loss: 0.0446, M_r_loss: 564.2327\n",
      "Epoch [111/250], step [1/244], D_loss: 0.0629, M_loss: 0.6031, M_d_loss: 0.0410, M_r_loss: 562.0229\n",
      "Epoch [111/250], step [41/244], D_loss: 0.0408, M_loss: 0.5852, M_d_loss: 0.0439, M_r_loss: 541.3451\n",
      "Epoch [111/250], step [81/244], D_loss: 0.0620, M_loss: 0.5964, M_d_loss: 0.0379, M_r_loss: 558.5068\n",
      "Epoch [111/250], step [121/244], D_loss: 0.0606, M_loss: 0.5766, M_d_loss: 0.0472, M_r_loss: 529.4805\n",
      "Epoch [111/250], step [161/244], D_loss: 0.0574, M_loss: 0.6457, M_d_loss: 0.0543, M_r_loss: 591.4326\n",
      "Epoch [111/250], step [201/244], D_loss: 0.0592, M_loss: 0.5974, M_d_loss: 0.0493, M_r_loss: 548.0810\n",
      "Epoch [111/250], step [241/244], D_loss: 0.0503, M_loss: 0.5930, M_d_loss: 0.0401, M_r_loss: 552.8860\n",
      "Epoch [112/250], step [1/244], D_loss: 0.0564, M_loss: 0.5885, M_d_loss: 0.0443, M_r_loss: 544.2086\n",
      "Epoch [112/250], step [41/244], D_loss: 0.0459, M_loss: 0.5799, M_d_loss: 0.0357, M_r_loss: 544.1718\n",
      "Epoch [112/250], step [81/244], D_loss: 0.0957, M_loss: 0.5764, M_d_loss: 0.0423, M_r_loss: 534.1161\n",
      "Epoch [112/250], step [121/244], D_loss: 0.0668, M_loss: 0.5925, M_d_loss: 0.0559, M_r_loss: 536.6381\n",
      "Epoch [112/250], step [161/244], D_loss: 0.0720, M_loss: 0.5602, M_d_loss: 0.0413, M_r_loss: 518.9242\n",
      "Epoch [112/250], step [201/244], D_loss: 0.0504, M_loss: 0.5450, M_d_loss: 0.0371, M_r_loss: 507.9577\n",
      "Epoch [112/250], step [241/244], D_loss: 0.0497, M_loss: 0.5481, M_d_loss: 0.0362, M_r_loss: 511.8463\n",
      "Epoch [113/250], step [1/244], D_loss: 0.0470, M_loss: 0.5496, M_d_loss: 0.0431, M_r_loss: 506.4594\n",
      "Epoch [113/250], step [41/244], D_loss: 0.0536, M_loss: 0.5924, M_d_loss: 0.0408, M_r_loss: 551.5331\n",
      "Epoch [113/250], step [81/244], D_loss: 0.0522, M_loss: 0.5458, M_d_loss: 0.0366, M_r_loss: 509.2373\n",
      "Epoch [113/250], step [121/244], D_loss: 0.0492, M_loss: 0.5834, M_d_loss: 0.0379, M_r_loss: 545.5460\n",
      "Epoch [113/250], step [161/244], D_loss: 0.0512, M_loss: 0.5766, M_d_loss: 0.0372, M_r_loss: 539.4186\n",
      "Epoch [113/250], step [201/244], D_loss: 0.0596, M_loss: 0.6200, M_d_loss: 0.0427, M_r_loss: 577.3646\n",
      "Epoch [113/250], step [241/244], D_loss: 0.0578, M_loss: 0.5474, M_d_loss: 0.0378, M_r_loss: 509.6082\n",
      "Epoch [114/250], step [1/244], D_loss: 0.0525, M_loss: 0.6193, M_d_loss: 0.0446, M_r_loss: 574.7795\n",
      "Epoch [114/250], step [41/244], D_loss: 0.0593, M_loss: 0.5785, M_d_loss: 0.0405, M_r_loss: 538.0095\n",
      "Epoch [114/250], step [81/244], D_loss: 0.0662, M_loss: 0.5996, M_d_loss: 0.0471, M_r_loss: 552.5318\n",
      "Epoch [114/250], step [121/244], D_loss: 0.0494, M_loss: 0.6489, M_d_loss: 0.0419, M_r_loss: 606.9529\n",
      "Epoch [114/250], step [161/244], D_loss: 0.0662, M_loss: 0.5810, M_d_loss: 0.0441, M_r_loss: 536.9066\n",
      "Epoch [114/250], step [201/244], D_loss: 0.0622, M_loss: 0.6633, M_d_loss: 0.0552, M_r_loss: 608.1311\n",
      "Epoch [114/250], step [241/244], D_loss: 0.0646, M_loss: 0.5848, M_d_loss: 0.0374, M_r_loss: 547.3290\n",
      "Epoch [115/250], step [1/244], D_loss: 0.0609, M_loss: 0.5758, M_d_loss: 0.0403, M_r_loss: 535.4688\n",
      "Epoch [115/250], step [41/244], D_loss: 0.0621, M_loss: 0.5828, M_d_loss: 0.0386, M_r_loss: 544.1601\n",
      "Epoch [115/250], step [81/244], D_loss: 0.0482, M_loss: 0.5970, M_d_loss: 0.0395, M_r_loss: 557.5558\n",
      "Epoch [115/250], step [121/244], D_loss: 0.0690, M_loss: 0.5602, M_d_loss: 0.0351, M_r_loss: 525.1239\n",
      "Epoch [115/250], step [161/244], D_loss: 0.0630, M_loss: 0.5867, M_d_loss: 0.0425, M_r_loss: 544.1992\n",
      "Epoch [115/250], step [201/244], D_loss: 0.0555, M_loss: 0.6163, M_d_loss: 0.0402, M_r_loss: 576.1438\n",
      "Epoch [115/250], step [241/244], D_loss: 0.0441, M_loss: 0.6062, M_d_loss: 0.0416, M_r_loss: 564.5435\n",
      "Epoch [116/250], step [1/244], D_loss: 0.0523, M_loss: 0.6083, M_d_loss: 0.0447, M_r_loss: 563.5292\n",
      "Epoch [116/250], step [41/244], D_loss: 0.0646, M_loss: 0.6776, M_d_loss: 0.0504, M_r_loss: 627.2137\n",
      "Epoch [116/250], step [81/244], D_loss: 0.0662, M_loss: 0.6309, M_d_loss: 0.0535, M_r_loss: 577.4316\n",
      "Epoch [116/250], step [121/244], D_loss: 0.0668, M_loss: 0.6213, M_d_loss: 0.0374, M_r_loss: 583.8849\n",
      "Epoch [116/250], step [161/244], D_loss: 0.0469, M_loss: 0.5978, M_d_loss: 0.0366, M_r_loss: 561.1687\n",
      "Epoch [116/250], step [201/244], D_loss: 0.0452, M_loss: 0.6225, M_d_loss: 0.0406, M_r_loss: 581.9748\n",
      "Epoch [116/250], step [241/244], D_loss: 0.0446, M_loss: 0.5298, M_d_loss: 0.0367, M_r_loss: 493.1519\n",
      "Epoch [117/250], step [1/244], D_loss: 0.0485, M_loss: 0.5990, M_d_loss: 0.0430, M_r_loss: 555.9941\n",
      "Epoch [117/250], step [41/244], D_loss: 0.0539, M_loss: 0.5917, M_d_loss: 0.0449, M_r_loss: 546.8547\n",
      "Epoch [117/250], step [81/244], D_loss: 0.0518, M_loss: 0.5955, M_d_loss: 0.0542, M_r_loss: 541.2578\n",
      "Epoch [117/250], step [121/244], D_loss: 0.0721, M_loss: 0.6202, M_d_loss: 0.0443, M_r_loss: 575.8908\n",
      "Epoch [117/250], step [161/244], D_loss: 0.0482, M_loss: 0.6120, M_d_loss: 0.0410, M_r_loss: 571.0571\n",
      "Epoch [117/250], step [201/244], D_loss: 0.0528, M_loss: 0.5717, M_d_loss: 0.0364, M_r_loss: 535.3309\n",
      "Epoch [117/250], step [241/244], D_loss: 0.0583, M_loss: 0.5349, M_d_loss: 0.0392, M_r_loss: 495.7612\n",
      "Epoch [118/250], step [1/244], D_loss: 0.0534, M_loss: 0.5893, M_d_loss: 0.0411, M_r_loss: 548.2047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [118/250], step [41/244], D_loss: 0.0545, M_loss: 0.5817, M_d_loss: 0.0398, M_r_loss: 541.8337\n",
      "Epoch [118/250], step [81/244], D_loss: 0.0526, M_loss: 0.6296, M_d_loss: 0.0401, M_r_loss: 589.4688\n",
      "Epoch [118/250], step [121/244], D_loss: 0.0703, M_loss: 0.5632, M_d_loss: 0.0351, M_r_loss: 528.0790\n",
      "Epoch [118/250], step [161/244], D_loss: 0.0457, M_loss: 0.5574, M_d_loss: 0.0440, M_r_loss: 513.4528\n",
      "Epoch [118/250], step [201/244], D_loss: 0.0481, M_loss: 0.5920, M_d_loss: 0.0420, M_r_loss: 549.9451\n",
      "Epoch [118/250], step [241/244], D_loss: 0.0587, M_loss: 0.5796, M_d_loss: 0.0351, M_r_loss: 544.4786\n",
      "Epoch [119/250], step [1/244], D_loss: 0.0648, M_loss: 0.5863, M_d_loss: 0.0397, M_r_loss: 546.5895\n",
      "Epoch [119/250], step [41/244], D_loss: 0.0685, M_loss: 0.5743, M_d_loss: 0.0425, M_r_loss: 531.8420\n",
      "Epoch [119/250], step [81/244], D_loss: 0.0536, M_loss: 0.5755, M_d_loss: 0.0393, M_r_loss: 536.2272\n",
      "Epoch [119/250], step [121/244], D_loss: 0.0495, M_loss: 0.6078, M_d_loss: 0.0407, M_r_loss: 567.0342\n",
      "Epoch [119/250], step [161/244], D_loss: 0.0470, M_loss: 0.6345, M_d_loss: 0.0463, M_r_loss: 588.1930\n",
      "Epoch [119/250], step [201/244], D_loss: 0.0471, M_loss: 0.5710, M_d_loss: 0.0372, M_r_loss: 533.7745\n",
      "Epoch [119/250], step [241/244], D_loss: 0.0565, M_loss: 0.5771, M_d_loss: 0.0392, M_r_loss: 537.8766\n",
      "Epoch [120/250], step [1/244], D_loss: 0.0528, M_loss: 0.5660, M_d_loss: 0.0391, M_r_loss: 526.9142\n",
      "Epoch [120/250], step [41/244], D_loss: 0.0631, M_loss: 0.5680, M_d_loss: 0.0469, M_r_loss: 521.1500\n",
      "Epoch [120/250], step [81/244], D_loss: 0.0576, M_loss: 0.5559, M_d_loss: 0.0398, M_r_loss: 516.0867\n",
      "Epoch [120/250], step [121/244], D_loss: 0.0659, M_loss: 0.6060, M_d_loss: 0.0432, M_r_loss: 562.7304\n",
      "Epoch [120/250], step [161/244], D_loss: 0.0621, M_loss: 0.5647, M_d_loss: 0.0420, M_r_loss: 522.7264\n",
      "Epoch [120/250], step [201/244], D_loss: 0.0477, M_loss: 0.5500, M_d_loss: 0.0378, M_r_loss: 512.2247\n",
      "Epoch [120/250], step [241/244], D_loss: 0.0569, M_loss: 0.5680, M_d_loss: 0.0370, M_r_loss: 531.0097\n",
      "Epoch [121/250], step [1/244], D_loss: 0.0595, M_loss: 0.5793, M_d_loss: 0.0418, M_r_loss: 537.4822\n",
      "Epoch [121/250], step [41/244], D_loss: 0.0546, M_loss: 0.5760, M_d_loss: 0.0373, M_r_loss: 538.7812\n",
      "Epoch [121/250], step [81/244], D_loss: 0.0533, M_loss: 0.5649, M_d_loss: 0.0402, M_r_loss: 524.7135\n",
      "Epoch [121/250], step [121/244], D_loss: 0.0554, M_loss: 0.5593, M_d_loss: 0.0408, M_r_loss: 518.5986\n",
      "Epoch [121/250], step [161/244], D_loss: 0.0592, M_loss: 0.5917, M_d_loss: 0.0403, M_r_loss: 551.3895\n",
      "Epoch [121/250], step [201/244], D_loss: 0.0549, M_loss: 0.6126, M_d_loss: 0.0397, M_r_loss: 572.8525\n",
      "Epoch [121/250], step [241/244], D_loss: 0.0611, M_loss: 0.5949, M_d_loss: 0.0413, M_r_loss: 553.6064\n",
      "Epoch [122/250], step [1/244], D_loss: 0.0652, M_loss: 0.6057, M_d_loss: 0.0403, M_r_loss: 565.3307\n",
      "Epoch [122/250], step [41/244], D_loss: 0.0690, M_loss: 0.5141, M_d_loss: 0.0348, M_r_loss: 479.3425\n",
      "Epoch [122/250], step [81/244], D_loss: 0.0524, M_loss: 0.5725, M_d_loss: 0.0439, M_r_loss: 528.5597\n",
      "Epoch [122/250], step [121/244], D_loss: 0.0567, M_loss: 0.6006, M_d_loss: 0.0404, M_r_loss: 560.2288\n",
      "Epoch [122/250], step [161/244], D_loss: 0.0575, M_loss: 0.6432, M_d_loss: 0.0431, M_r_loss: 600.1041\n",
      "Epoch [122/250], step [201/244], D_loss: 0.0505, M_loss: 0.5833, M_d_loss: 0.0398, M_r_loss: 543.4255\n",
      "Epoch [122/250], step [241/244], D_loss: 0.0531, M_loss: 0.6031, M_d_loss: 0.0378, M_r_loss: 565.2982\n",
      "Epoch [123/250], step [1/244], D_loss: 0.0602, M_loss: 0.5704, M_d_loss: 0.0432, M_r_loss: 527.2069\n",
      "Epoch [123/250], step [41/244], D_loss: 0.0548, M_loss: 0.6187, M_d_loss: 0.0440, M_r_loss: 574.7322\n",
      "Epoch [123/250], step [81/244], D_loss: 0.0589, M_loss: 0.5763, M_d_loss: 0.0424, M_r_loss: 533.9380\n",
      "Epoch [123/250], step [121/244], D_loss: 0.0629, M_loss: 0.6079, M_d_loss: 0.0455, M_r_loss: 562.4481\n",
      "Epoch [123/250], step [161/244], D_loss: 0.0684, M_loss: 0.5483, M_d_loss: 0.0420, M_r_loss: 506.3306\n",
      "Epoch [123/250], step [201/244], D_loss: 0.0519, M_loss: 0.6176, M_d_loss: 0.0497, M_r_loss: 567.9117\n",
      "Epoch [123/250], step [241/244], D_loss: 0.0682, M_loss: 0.6275, M_d_loss: 0.0525, M_r_loss: 574.9960\n",
      "Epoch [124/250], step [1/244], D_loss: 0.0640, M_loss: 0.6082, M_d_loss: 0.0528, M_r_loss: 555.3701\n",
      "Epoch [124/250], step [41/244], D_loss: 0.0573, M_loss: 0.5760, M_d_loss: 0.0373, M_r_loss: 538.6889\n",
      "Epoch [124/250], step [81/244], D_loss: 0.0594, M_loss: 0.5795, M_d_loss: 0.0393, M_r_loss: 540.1655\n",
      "Epoch [124/250], step [121/244], D_loss: 0.0466, M_loss: 0.5781, M_d_loss: 0.0375, M_r_loss: 540.5771\n",
      "Epoch [124/250], step [161/244], D_loss: 0.0509, M_loss: 0.5713, M_d_loss: 0.0395, M_r_loss: 531.7388\n",
      "Epoch [124/250], step [201/244], D_loss: 0.0438, M_loss: 0.5945, M_d_loss: 0.0414, M_r_loss: 553.1072\n",
      "Epoch [124/250], step [241/244], D_loss: 0.0591, M_loss: 0.5894, M_d_loss: 0.0411, M_r_loss: 548.3748\n",
      "Epoch [125/250], step [1/244], D_loss: 0.0657, M_loss: 0.5811, M_d_loss: 0.0390, M_r_loss: 542.1520\n",
      "Epoch [125/250], step [41/244], D_loss: 0.0447, M_loss: 0.6130, M_d_loss: 0.0492, M_r_loss: 563.7618\n",
      "Epoch [125/250], step [81/244], D_loss: 0.0920, M_loss: 0.6282, M_d_loss: 0.0504, M_r_loss: 577.7639\n",
      "Epoch [125/250], step [121/244], D_loss: 0.0484, M_loss: 0.6093, M_d_loss: 0.0373, M_r_loss: 572.0379\n",
      "Epoch [125/250], step [161/244], D_loss: 0.0507, M_loss: 0.5870, M_d_loss: 0.0402, M_r_loss: 546.8223\n",
      "Epoch [125/250], step [201/244], D_loss: 0.0450, M_loss: 0.6333, M_d_loss: 0.0425, M_r_loss: 590.8042\n",
      "Epoch [125/250], step [241/244], D_loss: 0.0549, M_loss: 0.6050, M_d_loss: 0.0416, M_r_loss: 563.3363\n",
      "Epoch [126/250], step [1/244], D_loss: 0.0624, M_loss: 0.5763, M_d_loss: 0.0448, M_r_loss: 531.4537\n",
      "Epoch [126/250], step [41/244], D_loss: 0.0609, M_loss: 0.6055, M_d_loss: 0.0452, M_r_loss: 560.3386\n",
      "Epoch [126/250], step [81/244], D_loss: 0.0576, M_loss: 0.5796, M_d_loss: 0.0414, M_r_loss: 538.1275\n",
      "Epoch [126/250], step [121/244], D_loss: 0.0564, M_loss: 0.5677, M_d_loss: 0.0398, M_r_loss: 527.8928\n",
      "Epoch [126/250], step [161/244], D_loss: 0.0823, M_loss: 0.6337, M_d_loss: 0.0497, M_r_loss: 584.0157\n",
      "Epoch [126/250], step [201/244], D_loss: 0.0545, M_loss: 0.6252, M_d_loss: 0.0404, M_r_loss: 584.7839\n",
      "Epoch [126/250], step [241/244], D_loss: 0.0466, M_loss: 0.5817, M_d_loss: 0.0404, M_r_loss: 541.2725\n",
      "Epoch [127/250], step [1/244], D_loss: 0.0472, M_loss: 0.5850, M_d_loss: 0.0399, M_r_loss: 545.1316\n",
      "Epoch [127/250], step [41/244], D_loss: 0.0484, M_loss: 0.5929, M_d_loss: 0.0382, M_r_loss: 554.7151\n",
      "Epoch [127/250], step [81/244], D_loss: 0.0484, M_loss: 0.5708, M_d_loss: 0.0409, M_r_loss: 529.9355\n",
      "Epoch [127/250], step [121/244], D_loss: 0.0642, M_loss: 0.5733, M_d_loss: 0.0469, M_r_loss: 526.3391\n",
      "Epoch [127/250], step [161/244], D_loss: 0.0557, M_loss: 0.5687, M_d_loss: 0.0387, M_r_loss: 529.9710\n",
      "Epoch [127/250], step [201/244], D_loss: 0.0637, M_loss: 0.5462, M_d_loss: 0.0393, M_r_loss: 506.9645\n",
      "Epoch [127/250], step [241/244], D_loss: 0.0782, M_loss: 0.6328, M_d_loss: 0.0535, M_r_loss: 579.3461\n",
      "Epoch [128/250], step [1/244], D_loss: 0.0840, M_loss: 0.6101, M_d_loss: 0.0489, M_r_loss: 561.1531\n",
      "Epoch [128/250], step [41/244], D_loss: 0.0548, M_loss: 0.6358, M_d_loss: 0.0451, M_r_loss: 590.6851\n",
      "Epoch [128/250], step [81/244], D_loss: 0.0510, M_loss: 0.6139, M_d_loss: 0.0401, M_r_loss: 573.7937\n",
      "Epoch [128/250], step [121/244], D_loss: 0.0499, M_loss: 0.5719, M_d_loss: 0.0361, M_r_loss: 535.7701\n",
      "Epoch [128/250], step [161/244], D_loss: 0.0423, M_loss: 0.5704, M_d_loss: 0.0383, M_r_loss: 532.0825\n",
      "Epoch [128/250], step [201/244], D_loss: 0.0620, M_loss: 0.5761, M_d_loss: 0.0399, M_r_loss: 536.2031\n",
      "Epoch [128/250], step [241/244], D_loss: 0.0488, M_loss: 0.5961, M_d_loss: 0.0429, M_r_loss: 553.2165\n",
      "Epoch [129/250], step [1/244], D_loss: 0.0521, M_loss: 0.5814, M_d_loss: 0.0453, M_r_loss: 536.1326\n",
      "Epoch [129/250], step [41/244], D_loss: 0.0704, M_loss: 0.5834, M_d_loss: 0.0406, M_r_loss: 542.7570\n",
      "Epoch [129/250], step [81/244], D_loss: 0.0650, M_loss: 0.5861, M_d_loss: 0.0416, M_r_loss: 544.4852\n",
      "Epoch [129/250], step [121/244], D_loss: 0.0492, M_loss: 0.6219, M_d_loss: 0.0403, M_r_loss: 581.5874\n",
      "Epoch [129/250], step [161/244], D_loss: 0.0572, M_loss: 0.6102, M_d_loss: 0.0462, M_r_loss: 563.9536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [129/250], step [201/244], D_loss: 0.0544, M_loss: 0.5797, M_d_loss: 0.0429, M_r_loss: 536.7733\n",
      "Epoch [129/250], step [241/244], D_loss: 0.0565, M_loss: 0.5986, M_d_loss: 0.0433, M_r_loss: 555.3218\n",
      "Epoch [130/250], step [1/244], D_loss: 0.0513, M_loss: 0.5736, M_d_loss: 0.0443, M_r_loss: 529.3569\n",
      "Epoch [130/250], step [41/244], D_loss: 0.0598, M_loss: 0.6240, M_d_loss: 0.0460, M_r_loss: 578.0548\n",
      "Epoch [130/250], step [81/244], D_loss: 0.0577, M_loss: 0.5849, M_d_loss: 0.0491, M_r_loss: 535.8003\n",
      "Epoch [130/250], step [121/244], D_loss: 0.0395, M_loss: 0.6014, M_d_loss: 0.0419, M_r_loss: 559.4916\n",
      "Epoch [130/250], step [161/244], D_loss: 0.0500, M_loss: 0.5810, M_d_loss: 0.0380, M_r_loss: 542.9761\n",
      "Epoch [130/250], step [201/244], D_loss: 0.0615, M_loss: 0.6514, M_d_loss: 0.0467, M_r_loss: 604.6595\n",
      "Epoch [130/250], step [241/244], D_loss: 0.0531, M_loss: 0.5535, M_d_loss: 0.0371, M_r_loss: 516.3727\n",
      "Epoch [131/250], step [1/244], D_loss: 0.0604, M_loss: 0.5703, M_d_loss: 0.0401, M_r_loss: 530.2289\n",
      "Epoch [131/250], step [41/244], D_loss: 0.0574, M_loss: 0.5893, M_d_loss: 0.0463, M_r_loss: 543.0029\n",
      "Epoch [131/250], step [81/244], D_loss: 0.0651, M_loss: 0.5700, M_d_loss: 0.0387, M_r_loss: 531.2458\n",
      "Epoch [131/250], step [121/244], D_loss: 0.0529, M_loss: 0.5819, M_d_loss: 0.0415, M_r_loss: 540.3878\n",
      "Epoch [131/250], step [161/244], D_loss: 0.0610, M_loss: 0.6332, M_d_loss: 0.0621, M_r_loss: 571.1246\n",
      "Epoch [131/250], step [201/244], D_loss: 0.0787, M_loss: 0.6376, M_d_loss: 0.0411, M_r_loss: 596.5269\n",
      "Epoch [131/250], step [241/244], D_loss: 0.0804, M_loss: 0.5600, M_d_loss: 0.0478, M_r_loss: 512.2052\n",
      "Epoch [132/250], step [1/244], D_loss: 0.0732, M_loss: 0.6093, M_d_loss: 0.0522, M_r_loss: 557.0900\n",
      "Epoch [132/250], step [41/244], D_loss: 0.0590, M_loss: 0.5933, M_d_loss: 0.0346, M_r_loss: 558.6882\n",
      "Epoch [132/250], step [81/244], D_loss: 0.0421, M_loss: 0.5853, M_d_loss: 0.0416, M_r_loss: 543.7145\n",
      "Epoch [132/250], step [121/244], D_loss: 0.0500, M_loss: 0.5981, M_d_loss: 0.0415, M_r_loss: 556.5382\n",
      "Epoch [132/250], step [161/244], D_loss: 0.0617, M_loss: 0.6260, M_d_loss: 0.0463, M_r_loss: 579.6425\n",
      "Epoch [132/250], step [201/244], D_loss: 0.0506, M_loss: 0.5851, M_d_loss: 0.0396, M_r_loss: 545.4535\n",
      "Epoch [132/250], step [241/244], D_loss: 0.0635, M_loss: 0.5955, M_d_loss: 0.0349, M_r_loss: 560.5396\n",
      "Epoch [133/250], step [1/244], D_loss: 0.0608, M_loss: 0.5808, M_d_loss: 0.0408, M_r_loss: 540.0029\n",
      "Epoch [133/250], step [41/244], D_loss: 0.0610, M_loss: 0.5970, M_d_loss: 0.0450, M_r_loss: 551.9756\n",
      "Epoch [133/250], step [81/244], D_loss: 0.0542, M_loss: 0.6052, M_d_loss: 0.0473, M_r_loss: 557.8918\n",
      "Epoch [133/250], step [121/244], D_loss: 0.0575, M_loss: 0.6397, M_d_loss: 0.0469, M_r_loss: 592.8665\n",
      "Epoch [133/250], step [161/244], D_loss: 0.0532, M_loss: 0.5828, M_d_loss: 0.0396, M_r_loss: 543.1251\n",
      "Epoch [133/250], step [201/244], D_loss: 0.0505, M_loss: 0.6059, M_d_loss: 0.0430, M_r_loss: 562.8445\n",
      "Epoch [133/250], step [241/244], D_loss: 0.0576, M_loss: 0.5675, M_d_loss: 0.0321, M_r_loss: 535.3502\n",
      "Epoch [134/250], step [1/244], D_loss: 0.0521, M_loss: 0.5915, M_d_loss: 0.0404, M_r_loss: 551.0881\n",
      "Epoch [134/250], step [41/244], D_loss: 0.0503, M_loss: 0.5634, M_d_loss: 0.0375, M_r_loss: 525.8765\n",
      "Epoch [134/250], step [81/244], D_loss: 0.0455, M_loss: 0.5998, M_d_loss: 0.0394, M_r_loss: 560.3986\n",
      "Epoch [134/250], step [121/244], D_loss: 0.0595, M_loss: 0.5947, M_d_loss: 0.0452, M_r_loss: 549.5304\n",
      "Epoch [134/250], step [161/244], D_loss: 0.0591, M_loss: 0.5667, M_d_loss: 0.0390, M_r_loss: 527.7400\n",
      "Epoch [134/250], step [201/244], D_loss: 0.0584, M_loss: 0.5935, M_d_loss: 0.0400, M_r_loss: 553.4802\n",
      "Epoch [134/250], step [241/244], D_loss: 0.0432, M_loss: 0.6511, M_d_loss: 0.0392, M_r_loss: 611.8445\n",
      "Epoch [135/250], step [1/244], D_loss: 0.0515, M_loss: 0.5858, M_d_loss: 0.0420, M_r_loss: 543.8841\n",
      "Epoch [135/250], step [41/244], D_loss: 0.0618, M_loss: 0.6150, M_d_loss: 0.0424, M_r_loss: 572.5580\n",
      "Epoch [135/250], step [81/244], D_loss: 0.0564, M_loss: 0.5882, M_d_loss: 0.0365, M_r_loss: 551.7582\n",
      "Epoch [135/250], step [121/244], D_loss: 0.0503, M_loss: 0.6023, M_d_loss: 0.0426, M_r_loss: 559.6331\n",
      "Epoch [135/250], step [161/244], D_loss: 0.0657, M_loss: 0.5828, M_d_loss: 0.0442, M_r_loss: 538.6257\n",
      "Epoch [135/250], step [201/244], D_loss: 0.0631, M_loss: 0.5906, M_d_loss: 0.0373, M_r_loss: 553.2780\n",
      "Epoch [135/250], step [241/244], D_loss: 0.0748, M_loss: 0.5611, M_d_loss: 0.0325, M_r_loss: 528.6007\n",
      "Epoch [136/250], step [1/244], D_loss: 0.0777, M_loss: 0.6210, M_d_loss: 0.0404, M_r_loss: 580.5549\n",
      "Epoch [136/250], step [41/244], D_loss: 0.0650, M_loss: 0.5652, M_d_loss: 0.0419, M_r_loss: 523.2961\n",
      "Epoch [136/250], step [81/244], D_loss: 0.0443, M_loss: 0.5883, M_d_loss: 0.0416, M_r_loss: 546.7557\n",
      "Epoch [136/250], step [121/244], D_loss: 0.0617, M_loss: 0.5575, M_d_loss: 0.0391, M_r_loss: 518.3493\n",
      "Epoch [136/250], step [161/244], D_loss: 0.0667, M_loss: 0.5793, M_d_loss: 0.0423, M_r_loss: 537.0010\n",
      "Epoch [136/250], step [201/244], D_loss: 0.0851, M_loss: 0.6164, M_d_loss: 0.0436, M_r_loss: 572.8109\n",
      "Epoch [136/250], step [241/244], D_loss: 0.0536, M_loss: 0.6214, M_d_loss: 0.0413, M_r_loss: 580.0737\n",
      "Epoch [137/250], step [1/244], D_loss: 0.0540, M_loss: 0.5760, M_d_loss: 0.0383, M_r_loss: 537.7159\n",
      "Epoch [137/250], step [41/244], D_loss: 0.0533, M_loss: 0.5815, M_d_loss: 0.0366, M_r_loss: 544.9279\n",
      "Epoch [137/250], step [81/244], D_loss: 0.0508, M_loss: 0.5766, M_d_loss: 0.0413, M_r_loss: 535.3579\n",
      "Epoch [137/250], step [121/244], D_loss: 0.0413, M_loss: 0.5609, M_d_loss: 0.0354, M_r_loss: 525.4564\n",
      "Epoch [137/250], step [161/244], D_loss: 0.0459, M_loss: 0.6352, M_d_loss: 0.0411, M_r_loss: 594.0719\n",
      "Epoch [137/250], step [201/244], D_loss: 0.0475, M_loss: 0.5757, M_d_loss: 0.0386, M_r_loss: 537.0724\n",
      "Epoch [137/250], step [241/244], D_loss: 0.0597, M_loss: 0.6154, M_d_loss: 0.0492, M_r_loss: 566.2234\n",
      "Epoch [138/250], step [1/244], D_loss: 0.0562, M_loss: 0.6197, M_d_loss: 0.0547, M_r_loss: 564.9573\n",
      "Epoch [138/250], step [41/244], D_loss: 0.0620, M_loss: 0.6024, M_d_loss: 0.0459, M_r_loss: 556.4401\n",
      "Epoch [138/250], step [81/244], D_loss: 0.0592, M_loss: 0.6218, M_d_loss: 0.0427, M_r_loss: 579.0817\n",
      "Epoch [138/250], step [121/244], D_loss: 0.0517, M_loss: 0.5791, M_d_loss: 0.0414, M_r_loss: 537.7691\n",
      "Epoch [138/250], step [161/244], D_loss: 0.0537, M_loss: 0.5933, M_d_loss: 0.0422, M_r_loss: 551.0785\n",
      "Epoch [138/250], step [201/244], D_loss: 0.0532, M_loss: 0.5796, M_d_loss: 0.0403, M_r_loss: 539.2786\n",
      "Epoch [138/250], step [241/244], D_loss: 0.0674, M_loss: 0.5850, M_d_loss: 0.0411, M_r_loss: 543.8080\n",
      "Epoch [139/250], step [1/244], D_loss: 0.0727, M_loss: 0.6102, M_d_loss: 0.0444, M_r_loss: 565.8702\n",
      "Epoch [139/250], step [41/244], D_loss: 0.0544, M_loss: 0.5662, M_d_loss: 0.0421, M_r_loss: 524.1139\n",
      "Epoch [139/250], step [81/244], D_loss: 0.0655, M_loss: 0.5749, M_d_loss: 0.0400, M_r_loss: 534.9874\n",
      "Epoch [139/250], step [121/244], D_loss: 0.0511, M_loss: 0.5757, M_d_loss: 0.0397, M_r_loss: 536.0898\n",
      "Epoch [139/250], step [161/244], D_loss: 0.0599, M_loss: 0.5699, M_d_loss: 0.0391, M_r_loss: 530.8806\n",
      "Epoch [139/250], step [201/244], D_loss: 0.0624, M_loss: 0.6129, M_d_loss: 0.0426, M_r_loss: 570.2314\n",
      "Epoch [139/250], step [241/244], D_loss: 0.0467, M_loss: 0.5860, M_d_loss: 0.0447, M_r_loss: 541.2379\n",
      "Epoch [140/250], step [1/244], D_loss: 0.0515, M_loss: 0.5802, M_d_loss: 0.0505, M_r_loss: 529.7144\n",
      "Epoch [140/250], step [41/244], D_loss: 0.0744, M_loss: 0.5922, M_d_loss: 0.0466, M_r_loss: 545.5827\n",
      "Epoch [140/250], step [81/244], D_loss: 0.0593, M_loss: 0.6563, M_d_loss: 0.0452, M_r_loss: 611.1332\n",
      "Epoch [140/250], step [121/244], D_loss: 0.0543, M_loss: 0.6098, M_d_loss: 0.0378, M_r_loss: 572.0352\n",
      "Epoch [140/250], step [161/244], D_loss: 0.0536, M_loss: 0.5697, M_d_loss: 0.0366, M_r_loss: 533.0798\n",
      "Epoch [140/250], step [201/244], D_loss: 0.0504, M_loss: 0.5681, M_d_loss: 0.0388, M_r_loss: 529.2775\n",
      "Epoch [140/250], step [241/244], D_loss: 0.0743, M_loss: 0.5966, M_d_loss: 0.0397, M_r_loss: 556.8467\n",
      "Epoch [141/250], step [1/244], D_loss: 0.0743, M_loss: 0.6462, M_d_loss: 0.0484, M_r_loss: 597.7675\n",
      "Epoch [141/250], step [41/244], D_loss: 0.0552, M_loss: 0.5688, M_d_loss: 0.0368, M_r_loss: 532.0203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [141/250], step [81/244], D_loss: 0.0540, M_loss: 0.5690, M_d_loss: 0.0440, M_r_loss: 524.9833\n",
      "Epoch [141/250], step [121/244], D_loss: 0.0482, M_loss: 0.6004, M_d_loss: 0.0495, M_r_loss: 550.8872\n",
      "Epoch [141/250], step [161/244], D_loss: 0.0546, M_loss: 0.5806, M_d_loss: 0.0362, M_r_loss: 544.4622\n",
      "Epoch [141/250], step [201/244], D_loss: 0.0657, M_loss: 0.5815, M_d_loss: 0.0423, M_r_loss: 539.1539\n",
      "Epoch [141/250], step [241/244], D_loss: 0.0627, M_loss: 0.5753, M_d_loss: 0.0480, M_r_loss: 527.2341\n",
      "Epoch [142/250], step [1/244], D_loss: 0.0728, M_loss: 0.6042, M_d_loss: 0.0623, M_r_loss: 541.8722\n",
      "Epoch [142/250], step [41/244], D_loss: 0.0611, M_loss: 0.5551, M_d_loss: 0.0415, M_r_loss: 513.6218\n",
      "Epoch [142/250], step [81/244], D_loss: 0.0438, M_loss: 0.5861, M_d_loss: 0.0411, M_r_loss: 545.0190\n",
      "Epoch [142/250], step [121/244], D_loss: 0.0520, M_loss: 0.5455, M_d_loss: 0.0351, M_r_loss: 510.3665\n",
      "Epoch [142/250], step [161/244], D_loss: 0.0514, M_loss: 0.6027, M_d_loss: 0.0447, M_r_loss: 558.0085\n",
      "Epoch [142/250], step [201/244], D_loss: 0.0553, M_loss: 0.6078, M_d_loss: 0.0408, M_r_loss: 566.9721\n",
      "Epoch [142/250], step [241/244], D_loss: 0.0634, M_loss: 0.5860, M_d_loss: 0.0398, M_r_loss: 546.1857\n",
      "Epoch [143/250], step [1/244], D_loss: 0.0684, M_loss: 0.5724, M_d_loss: 0.0439, M_r_loss: 528.5039\n",
      "Epoch [143/250], step [41/244], D_loss: 0.0488, M_loss: 0.5636, M_d_loss: 0.0386, M_r_loss: 524.9100\n",
      "Epoch [143/250], step [81/244], D_loss: 0.0506, M_loss: 0.5870, M_d_loss: 0.0411, M_r_loss: 545.9275\n",
      "Epoch [143/250], step [121/244], D_loss: 0.0463, M_loss: 0.5812, M_d_loss: 0.0424, M_r_loss: 538.8731\n",
      "Epoch [143/250], step [161/244], D_loss: 0.0679, M_loss: 0.6261, M_d_loss: 0.0412, M_r_loss: 584.9470\n",
      "Epoch [143/250], step [201/244], D_loss: 0.0591, M_loss: 0.5837, M_d_loss: 0.0434, M_r_loss: 540.2517\n",
      "Epoch [143/250], step [241/244], D_loss: 0.0650, M_loss: 0.5938, M_d_loss: 0.0375, M_r_loss: 556.3350\n",
      "Epoch [144/250], step [1/244], D_loss: 0.0682, M_loss: 0.5720, M_d_loss: 0.0418, M_r_loss: 530.1964\n",
      "Epoch [144/250], step [41/244], D_loss: 0.0530, M_loss: 0.5839, M_d_loss: 0.0433, M_r_loss: 540.5544\n",
      "Epoch [144/250], step [81/244], D_loss: 0.0590, M_loss: 0.6208, M_d_loss: 0.0419, M_r_loss: 578.8454\n",
      "Epoch [144/250], step [121/244], D_loss: 0.0575, M_loss: 0.5920, M_d_loss: 0.0440, M_r_loss: 547.9781\n",
      "Epoch [144/250], step [161/244], D_loss: 0.0431, M_loss: 0.5540, M_d_loss: 0.0356, M_r_loss: 518.3918\n",
      "Epoch [144/250], step [201/244], D_loss: 0.0533, M_loss: 0.5925, M_d_loss: 0.0416, M_r_loss: 550.8245\n",
      "Epoch [144/250], step [241/244], D_loss: 0.0630, M_loss: 0.5679, M_d_loss: 0.0420, M_r_loss: 525.9316\n",
      "Epoch [145/250], step [1/244], D_loss: 0.0665, M_loss: 0.5631, M_d_loss: 0.0426, M_r_loss: 520.4750\n",
      "Epoch [145/250], step [41/244], D_loss: 0.0604, M_loss: 0.5913, M_d_loss: 0.0451, M_r_loss: 546.1713\n",
      "Epoch [145/250], step [81/244], D_loss: 0.0516, M_loss: 0.5683, M_d_loss: 0.0362, M_r_loss: 532.0629\n",
      "Epoch [145/250], step [121/244], D_loss: 0.0591, M_loss: 0.5845, M_d_loss: 0.0358, M_r_loss: 548.7454\n",
      "Epoch [145/250], step [161/244], D_loss: 0.0618, M_loss: 0.6083, M_d_loss: 0.0425, M_r_loss: 565.8241\n",
      "Epoch [145/250], step [201/244], D_loss: 0.0478, M_loss: 0.6128, M_d_loss: 0.0465, M_r_loss: 566.2980\n",
      "Epoch [145/250], step [241/244], D_loss: 0.0489, M_loss: 0.5489, M_d_loss: 0.0360, M_r_loss: 512.9527\n",
      "Epoch [146/250], step [1/244], D_loss: 0.0377, M_loss: 0.5666, M_d_loss: 0.0431, M_r_loss: 523.5332\n",
      "Epoch [146/250], step [41/244], D_loss: 0.0513, M_loss: 0.5865, M_d_loss: 0.0377, M_r_loss: 548.7722\n",
      "Epoch [146/250], step [81/244], D_loss: 0.0596, M_loss: 0.5810, M_d_loss: 0.0375, M_r_loss: 543.5182\n",
      "Epoch [146/250], step [121/244], D_loss: 0.0606, M_loss: 0.5856, M_d_loss: 0.0439, M_r_loss: 541.7600\n",
      "Epoch [146/250], step [161/244], D_loss: 0.0599, M_loss: 0.6263, M_d_loss: 0.0448, M_r_loss: 581.4732\n",
      "Epoch [146/250], step [201/244], D_loss: 0.0611, M_loss: 0.5805, M_d_loss: 0.0424, M_r_loss: 538.1104\n",
      "Epoch [146/250], step [241/244], D_loss: 0.0557, M_loss: 0.6223, M_d_loss: 0.0406, M_r_loss: 581.6879\n",
      "Epoch [147/250], step [1/244], D_loss: 0.0574, M_loss: 0.5816, M_d_loss: 0.0424, M_r_loss: 539.1819\n",
      "Epoch [147/250], step [41/244], D_loss: 0.0701, M_loss: 0.6128, M_d_loss: 0.0369, M_r_loss: 575.9113\n",
      "Epoch [147/250], step [81/244], D_loss: 0.0686, M_loss: 0.6227, M_d_loss: 0.0503, M_r_loss: 572.4089\n",
      "Epoch [147/250], step [121/244], D_loss: 0.0585, M_loss: 0.6591, M_d_loss: 0.0501, M_r_loss: 608.9811\n",
      "Epoch [147/250], step [161/244], D_loss: 0.0557, M_loss: 0.5972, M_d_loss: 0.0421, M_r_loss: 555.1125\n",
      "Epoch [147/250], step [201/244], D_loss: 0.0542, M_loss: 0.5894, M_d_loss: 0.0423, M_r_loss: 547.0612\n",
      "Epoch [147/250], step [241/244], D_loss: 0.0550, M_loss: 0.6040, M_d_loss: 0.0472, M_r_loss: 556.7730\n",
      "Epoch [148/250], step [1/244], D_loss: 0.0729, M_loss: 0.6297, M_d_loss: 0.0557, M_r_loss: 574.0500\n",
      "Epoch [148/250], step [41/244], D_loss: 0.0851, M_loss: 0.6778, M_d_loss: 0.0761, M_r_loss: 601.7335\n",
      "Epoch [148/250], step [81/244], D_loss: 0.0735, M_loss: 0.6251, M_d_loss: 0.0613, M_r_loss: 563.8690\n",
      "Epoch [148/250], step [121/244], D_loss: 0.0505, M_loss: 0.5801, M_d_loss: 0.0348, M_r_loss: 545.3736\n",
      "Epoch [148/250], step [161/244], D_loss: 0.0634, M_loss: 0.5735, M_d_loss: 0.0395, M_r_loss: 534.0797\n",
      "Epoch [148/250], step [201/244], D_loss: 0.0507, M_loss: 0.6100, M_d_loss: 0.0393, M_r_loss: 570.6838\n",
      "Epoch [148/250], step [241/244], D_loss: 0.0733, M_loss: 0.5795, M_d_loss: 0.0391, M_r_loss: 540.4556\n",
      "Epoch [149/250], step [1/244], D_loss: 0.0793, M_loss: 0.5886, M_d_loss: 0.0414, M_r_loss: 547.2814\n",
      "Epoch [149/250], step [41/244], D_loss: 0.0545, M_loss: 0.5504, M_d_loss: 0.0358, M_r_loss: 514.5894\n",
      "Epoch [149/250], step [81/244], D_loss: 0.0501, M_loss: 0.5688, M_d_loss: 0.0383, M_r_loss: 530.4954\n",
      "Epoch [149/250], step [121/244], D_loss: 0.0588, M_loss: 0.6069, M_d_loss: 0.0474, M_r_loss: 559.4730\n",
      "Epoch [149/250], step [161/244], D_loss: 0.0634, M_loss: 0.5741, M_d_loss: 0.0424, M_r_loss: 531.7181\n",
      "Epoch [149/250], step [201/244], D_loss: 0.0518, M_loss: 0.5854, M_d_loss: 0.0404, M_r_loss: 545.0262\n",
      "Epoch [149/250], step [241/244], D_loss: 0.0514, M_loss: 0.5807, M_d_loss: 0.0370, M_r_loss: 543.6830\n",
      "Epoch [150/250], step [1/244], D_loss: 0.0476, M_loss: 0.5825, M_d_loss: 0.0449, M_r_loss: 537.6431\n",
      "Epoch [150/250], step [41/244], D_loss: 0.0526, M_loss: 0.5653, M_d_loss: 0.0399, M_r_loss: 525.4366\n",
      "Epoch [150/250], step [81/244], D_loss: 0.0464, M_loss: 0.5645, M_d_loss: 0.0387, M_r_loss: 525.8198\n",
      "Epoch [150/250], step [121/244], D_loss: 0.0595, M_loss: 0.5964, M_d_loss: 0.0437, M_r_loss: 552.7565\n",
      "Epoch [150/250], step [161/244], D_loss: 0.0638, M_loss: 0.6199, M_d_loss: 0.0422, M_r_loss: 577.6353\n",
      "Epoch [150/250], step [201/244], D_loss: 0.0543, M_loss: 0.6215, M_d_loss: 0.0502, M_r_loss: 571.3058\n",
      "Epoch [150/250], step [241/244], D_loss: 0.0475, M_loss: 0.5766, M_d_loss: 0.0393, M_r_loss: 537.2274\n",
      "Epoch [151/250], step [1/244], D_loss: 0.0479, M_loss: 0.5987, M_d_loss: 0.0402, M_r_loss: 558.4561\n",
      "Epoch [151/250], step [41/244], D_loss: 0.0458, M_loss: 0.6202, M_d_loss: 0.0441, M_r_loss: 576.1506\n",
      "Epoch [151/250], step [81/244], D_loss: 0.0484, M_loss: 0.5947, M_d_loss: 0.0460, M_r_loss: 548.7427\n",
      "Epoch [151/250], step [121/244], D_loss: 0.0575, M_loss: 0.5933, M_d_loss: 0.0415, M_r_loss: 551.8507\n",
      "Epoch [151/250], step [161/244], D_loss: 0.0549, M_loss: 0.6081, M_d_loss: 0.0439, M_r_loss: 564.1710\n",
      "Epoch [151/250], step [201/244], D_loss: 0.0564, M_loss: 0.5840, M_d_loss: 0.0429, M_r_loss: 541.1031\n",
      "Epoch [151/250], step [241/244], D_loss: 0.0521, M_loss: 0.6239, M_d_loss: 0.0448, M_r_loss: 579.0923\n",
      "Epoch [152/250], step [1/244], D_loss: 0.0577, M_loss: 0.5740, M_d_loss: 0.0427, M_r_loss: 531.2576\n",
      "Epoch [152/250], step [41/244], D_loss: 0.0527, M_loss: 0.5905, M_d_loss: 0.0448, M_r_loss: 545.6798\n",
      "Epoch [152/250], step [81/244], D_loss: 0.0635, M_loss: 0.6315, M_d_loss: 0.0487, M_r_loss: 582.7849\n",
      "Epoch [152/250], step [121/244], D_loss: 0.0694, M_loss: 0.5810, M_d_loss: 0.0438, M_r_loss: 537.2175\n",
      "Epoch [152/250], step [161/244], D_loss: 0.0632, M_loss: 0.6756, M_d_loss: 0.0558, M_r_loss: 619.8837\n",
      "Epoch [152/250], step [201/244], D_loss: 0.0608, M_loss: 0.5621, M_d_loss: 0.0424, M_r_loss: 519.7648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [152/250], step [241/244], D_loss: 0.0498, M_loss: 0.6074, M_d_loss: 0.0445, M_r_loss: 562.8727\n",
      "Epoch [153/250], step [1/244], D_loss: 0.0593, M_loss: 0.5402, M_d_loss: 0.0412, M_r_loss: 499.0751\n",
      "Epoch [153/250], step [41/244], D_loss: 0.0550, M_loss: 0.5748, M_d_loss: 0.0393, M_r_loss: 535.4900\n",
      "Epoch [153/250], step [81/244], D_loss: 0.0507, M_loss: 0.5827, M_d_loss: 0.0382, M_r_loss: 544.5186\n",
      "Epoch [153/250], step [121/244], D_loss: 0.0467, M_loss: 0.5823, M_d_loss: 0.0428, M_r_loss: 539.5053\n",
      "Epoch [153/250], step [161/244], D_loss: 0.0524, M_loss: 0.5876, M_d_loss: 0.0399, M_r_loss: 547.6544\n",
      "Epoch [153/250], step [201/244], D_loss: 0.0623, M_loss: 0.5825, M_d_loss: 0.0396, M_r_loss: 542.8322\n",
      "Epoch [153/250], step [241/244], D_loss: 0.0536, M_loss: 0.5669, M_d_loss: 0.0396, M_r_loss: 527.2905\n",
      "Epoch [154/250], step [1/244], D_loss: 0.0570, M_loss: 0.5675, M_d_loss: 0.0394, M_r_loss: 528.0944\n",
      "Epoch [154/250], step [41/244], D_loss: 0.0546, M_loss: 0.5723, M_d_loss: 0.0411, M_r_loss: 531.1574\n",
      "Epoch [154/250], step [81/244], D_loss: 0.0542, M_loss: 0.5813, M_d_loss: 0.0425, M_r_loss: 538.8162\n",
      "Epoch [154/250], step [121/244], D_loss: 0.0448, M_loss: 0.5608, M_d_loss: 0.0404, M_r_loss: 520.4810\n",
      "Epoch [154/250], step [161/244], D_loss: 0.0457, M_loss: 0.5999, M_d_loss: 0.0425, M_r_loss: 557.3785\n",
      "Epoch [154/250], step [201/244], D_loss: 0.0488, M_loss: 0.6031, M_d_loss: 0.0413, M_r_loss: 561.7697\n",
      "Epoch [154/250], step [241/244], D_loss: 0.0449, M_loss: 0.5883, M_d_loss: 0.0411, M_r_loss: 547.1935\n",
      "Epoch [155/250], step [1/244], D_loss: 0.0495, M_loss: 0.6130, M_d_loss: 0.0443, M_r_loss: 568.6761\n",
      "Epoch [155/250], step [41/244], D_loss: 0.0528, M_loss: 0.6371, M_d_loss: 0.0439, M_r_loss: 593.1626\n",
      "Epoch [155/250], step [81/244], D_loss: 0.0536, M_loss: 0.5838, M_d_loss: 0.0401, M_r_loss: 543.6895\n",
      "Epoch [155/250], step [121/244], D_loss: 0.0523, M_loss: 0.5801, M_d_loss: 0.0401, M_r_loss: 540.0132\n",
      "Epoch [155/250], step [161/244], D_loss: 0.0553, M_loss: 0.5647, M_d_loss: 0.0353, M_r_loss: 529.4590\n",
      "Epoch [155/250], step [201/244], D_loss: 0.0480, M_loss: 0.6001, M_d_loss: 0.0397, M_r_loss: 560.4132\n",
      "Epoch [155/250], step [241/244], D_loss: 0.0478, M_loss: 0.5982, M_d_loss: 0.0403, M_r_loss: 557.8605\n",
      "Epoch [156/250], step [1/244], D_loss: 0.0573, M_loss: 0.5824, M_d_loss: 0.0401, M_r_loss: 542.3395\n",
      "Epoch [156/250], step [41/244], D_loss: 0.0515, M_loss: 0.5870, M_d_loss: 0.0386, M_r_loss: 548.3936\n",
      "Epoch [156/250], step [81/244], D_loss: 0.0622, M_loss: 0.5962, M_d_loss: 0.0430, M_r_loss: 553.2726\n",
      "Epoch [156/250], step [121/244], D_loss: 0.0675, M_loss: 0.6189, M_d_loss: 0.0476, M_r_loss: 571.2321\n",
      "Epoch [156/250], step [161/244], D_loss: 0.0517, M_loss: 0.6056, M_d_loss: 0.0451, M_r_loss: 560.5602\n",
      "Epoch [156/250], step [201/244], D_loss: 0.0677, M_loss: 0.5856, M_d_loss: 0.0406, M_r_loss: 545.0727\n",
      "Epoch [156/250], step [241/244], D_loss: 0.0659, M_loss: 0.6180, M_d_loss: 0.0438, M_r_loss: 574.1276\n",
      "Epoch [157/250], step [1/244], D_loss: 0.0688, M_loss: 0.5647, M_d_loss: 0.0434, M_r_loss: 521.2469\n",
      "Epoch [157/250], step [41/244], D_loss: 0.0673, M_loss: 0.6027, M_d_loss: 0.0435, M_r_loss: 559.1896\n",
      "Epoch [157/250], step [81/244], D_loss: 0.0588, M_loss: 0.6038, M_d_loss: 0.0381, M_r_loss: 565.6853\n",
      "Epoch [157/250], step [121/244], D_loss: 0.0506, M_loss: 0.5808, M_d_loss: 0.0373, M_r_loss: 543.5013\n",
      "Epoch [157/250], step [161/244], D_loss: 0.0513, M_loss: 0.5616, M_d_loss: 0.0380, M_r_loss: 523.6441\n",
      "Epoch [157/250], step [201/244], D_loss: 0.0439, M_loss: 0.6000, M_d_loss: 0.0397, M_r_loss: 560.3049\n",
      "Epoch [157/250], step [241/244], D_loss: 0.0480, M_loss: 0.5668, M_d_loss: 0.0382, M_r_loss: 528.5685\n",
      "Epoch [158/250], step [1/244], D_loss: 0.0488, M_loss: 0.6088, M_d_loss: 0.0415, M_r_loss: 567.2964\n",
      "Epoch [158/250], step [41/244], D_loss: 0.0508, M_loss: 0.5742, M_d_loss: 0.0408, M_r_loss: 533.4380\n",
      "Epoch [158/250], step [81/244], D_loss: 0.0495, M_loss: 0.5757, M_d_loss: 0.0396, M_r_loss: 536.0361\n",
      "Epoch [158/250], step [121/244], D_loss: 0.0546, M_loss: 0.5959, M_d_loss: 0.0406, M_r_loss: 555.2967\n",
      "Epoch [158/250], step [161/244], D_loss: 0.0540, M_loss: 0.5888, M_d_loss: 0.0419, M_r_loss: 546.8829\n",
      "Epoch [158/250], step [201/244], D_loss: 0.0647, M_loss: 0.5728, M_d_loss: 0.0395, M_r_loss: 533.2378\n",
      "Epoch [158/250], step [241/244], D_loss: 0.0595, M_loss: 0.6072, M_d_loss: 0.0424, M_r_loss: 564.8090\n",
      "Epoch [159/250], step [1/244], D_loss: 0.0590, M_loss: 0.6426, M_d_loss: 0.0518, M_r_loss: 590.7255\n",
      "Epoch [159/250], step [41/244], D_loss: 0.0494, M_loss: 0.5587, M_d_loss: 0.0376, M_r_loss: 521.0932\n",
      "Epoch [159/250], step [81/244], D_loss: 0.0514, M_loss: 0.5940, M_d_loss: 0.0374, M_r_loss: 556.6662\n",
      "Epoch [159/250], step [121/244], D_loss: 0.0567, M_loss: 0.6320, M_d_loss: 0.0371, M_r_loss: 594.9698\n",
      "Epoch [159/250], step [161/244], D_loss: 0.0562, M_loss: 0.5628, M_d_loss: 0.0383, M_r_loss: 524.4635\n",
      "Epoch [159/250], step [201/244], D_loss: 0.0419, M_loss: 0.6050, M_d_loss: 0.0448, M_r_loss: 560.2104\n",
      "Epoch [159/250], step [241/244], D_loss: 0.0507, M_loss: 0.5571, M_d_loss: 0.0373, M_r_loss: 519.8051\n",
      "Epoch [160/250], step [1/244], D_loss: 0.0538, M_loss: 0.5861, M_d_loss: 0.0445, M_r_loss: 541.6306\n",
      "Epoch [160/250], step [41/244], D_loss: 0.0600, M_loss: 0.5990, M_d_loss: 0.0489, M_r_loss: 550.0402\n",
      "Epoch [160/250], step [81/244], D_loss: 0.0541, M_loss: 0.5672, M_d_loss: 0.0453, M_r_loss: 521.8118\n",
      "Epoch [160/250], step [121/244], D_loss: 0.0482, M_loss: 0.6288, M_d_loss: 0.0448, M_r_loss: 583.9340\n",
      "Epoch [160/250], step [161/244], D_loss: 0.0500, M_loss: 0.5780, M_d_loss: 0.0421, M_r_loss: 535.9712\n",
      "Epoch [160/250], step [201/244], D_loss: 0.0508, M_loss: 0.6204, M_d_loss: 0.0434, M_r_loss: 576.9391\n",
      "Epoch [160/250], step [241/244], D_loss: 0.0495, M_loss: 0.6122, M_d_loss: 0.0428, M_r_loss: 569.4478\n",
      "Epoch [161/250], step [1/244], D_loss: 0.0575, M_loss: 0.5674, M_d_loss: 0.0426, M_r_loss: 524.7688\n",
      "Epoch [161/250], step [41/244], D_loss: 0.0529, M_loss: 0.6413, M_d_loss: 0.0433, M_r_loss: 597.9417\n",
      "Epoch [161/250], step [81/244], D_loss: 0.0460, M_loss: 0.6004, M_d_loss: 0.0428, M_r_loss: 557.6041\n",
      "Epoch [161/250], step [121/244], D_loss: 0.0557, M_loss: 0.6062, M_d_loss: 0.0407, M_r_loss: 565.4778\n",
      "Epoch [161/250], step [161/244], D_loss: 0.0524, M_loss: 0.5677, M_d_loss: 0.0422, M_r_loss: 525.4648\n",
      "Epoch [161/250], step [201/244], D_loss: 0.0563, M_loss: 0.5987, M_d_loss: 0.0398, M_r_loss: 558.9294\n",
      "Epoch [161/250], step [241/244], D_loss: 0.0546, M_loss: 0.6000, M_d_loss: 0.0442, M_r_loss: 555.8302\n",
      "Epoch [162/250], step [1/244], D_loss: 0.0545, M_loss: 0.5672, M_d_loss: 0.0414, M_r_loss: 525.7594\n",
      "Epoch [162/250], step [41/244], D_loss: 0.0572, M_loss: 0.5856, M_d_loss: 0.0421, M_r_loss: 543.5172\n",
      "Epoch [162/250], step [81/244], D_loss: 0.0491, M_loss: 0.5708, M_d_loss: 0.0410, M_r_loss: 529.8053\n",
      "Epoch [162/250], step [121/244], D_loss: 0.0497, M_loss: 0.6054, M_d_loss: 0.0436, M_r_loss: 561.8438\n",
      "Epoch [162/250], step [161/244], D_loss: 0.0504, M_loss: 0.5850, M_d_loss: 0.0435, M_r_loss: 541.4384\n",
      "Epoch [162/250], step [201/244], D_loss: 0.0506, M_loss: 0.5618, M_d_loss: 0.0426, M_r_loss: 519.2626\n",
      "Epoch [162/250], step [241/244], D_loss: 0.0551, M_loss: 0.6127, M_d_loss: 0.0412, M_r_loss: 571.5076\n",
      "Epoch [163/250], step [1/244], D_loss: 0.0510, M_loss: 0.6193, M_d_loss: 0.0484, M_r_loss: 570.8311\n",
      "Epoch [163/250], step [41/244], D_loss: 0.0527, M_loss: 0.5567, M_d_loss: 0.0424, M_r_loss: 514.3066\n",
      "Epoch [163/250], step [81/244], D_loss: 0.0501, M_loss: 0.5672, M_d_loss: 0.0391, M_r_loss: 528.0634\n",
      "Epoch [163/250], step [121/244], D_loss: 0.0606, M_loss: 0.6053, M_d_loss: 0.0379, M_r_loss: 567.4128\n",
      "Epoch [163/250], step [161/244], D_loss: 0.0619, M_loss: 0.6336, M_d_loss: 0.0495, M_r_loss: 584.1036\n",
      "Epoch [163/250], step [201/244], D_loss: 0.0529, M_loss: 0.5564, M_d_loss: 0.0391, M_r_loss: 517.2449\n",
      "Epoch [163/250], step [241/244], D_loss: 0.0487, M_loss: 0.6125, M_d_loss: 0.0455, M_r_loss: 567.0685\n",
      "Epoch [164/250], step [1/244], D_loss: 0.0563, M_loss: 0.6510, M_d_loss: 0.0507, M_r_loss: 600.2451\n",
      "Epoch [164/250], step [41/244], D_loss: 0.0594, M_loss: 0.6008, M_d_loss: 0.0412, M_r_loss: 559.5980\n",
      "Epoch [164/250], step [81/244], D_loss: 0.0540, M_loss: 0.5971, M_d_loss: 0.0471, M_r_loss: 550.0671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [164/250], step [121/244], D_loss: 0.0512, M_loss: 0.6140, M_d_loss: 0.0445, M_r_loss: 569.5128\n",
      "Epoch [164/250], step [161/244], D_loss: 0.0705, M_loss: 0.5407, M_d_loss: 0.0400, M_r_loss: 500.7095\n",
      "Epoch [164/250], step [201/244], D_loss: 0.0618, M_loss: 0.6104, M_d_loss: 0.0480, M_r_loss: 562.4831\n",
      "Epoch [164/250], step [241/244], D_loss: 0.0587, M_loss: 0.5826, M_d_loss: 0.0417, M_r_loss: 540.9694\n",
      "Epoch [165/250], step [1/244], D_loss: 0.0635, M_loss: 0.5947, M_d_loss: 0.0451, M_r_loss: 549.5192\n",
      "Epoch [165/250], step [41/244], D_loss: 0.0628, M_loss: 0.5872, M_d_loss: 0.0462, M_r_loss: 541.0328\n",
      "Epoch [165/250], step [81/244], D_loss: 0.0606, M_loss: 0.5829, M_d_loss: 0.0433, M_r_loss: 539.5864\n",
      "Epoch [165/250], step [121/244], D_loss: 0.0577, M_loss: 0.5437, M_d_loss: 0.0408, M_r_loss: 502.8646\n",
      "Epoch [165/250], step [161/244], D_loss: 0.0559, M_loss: 0.5882, M_d_loss: 0.0408, M_r_loss: 547.3882\n",
      "Epoch [165/250], step [201/244], D_loss: 0.0510, M_loss: 0.5884, M_d_loss: 0.0391, M_r_loss: 549.2913\n",
      "Epoch [165/250], step [241/244], D_loss: 0.0481, M_loss: 0.5862, M_d_loss: 0.0425, M_r_loss: 543.7039\n",
      "Epoch [166/250], step [1/244], D_loss: 0.0464, M_loss: 0.6158, M_d_loss: 0.0444, M_r_loss: 571.3478\n",
      "Epoch [166/250], step [41/244], D_loss: 0.0542, M_loss: 0.6089, M_d_loss: 0.0392, M_r_loss: 569.7288\n",
      "Epoch [166/250], step [81/244], D_loss: 0.0569, M_loss: 0.5893, M_d_loss: 0.0437, M_r_loss: 545.6276\n",
      "Epoch [166/250], step [121/244], D_loss: 0.0601, M_loss: 0.5741, M_d_loss: 0.0364, M_r_loss: 537.6536\n",
      "Epoch [166/250], step [161/244], D_loss: 0.0568, M_loss: 0.6103, M_d_loss: 0.0398, M_r_loss: 570.5076\n",
      "Epoch [166/250], step [201/244], D_loss: 0.0627, M_loss: 0.5874, M_d_loss: 0.0517, M_r_loss: 535.6328\n",
      "Epoch [166/250], step [241/244], D_loss: 0.0484, M_loss: 0.5927, M_d_loss: 0.0407, M_r_loss: 552.0037\n",
      "Epoch [167/250], step [1/244], D_loss: 0.0546, M_loss: 0.5787, M_d_loss: 0.0454, M_r_loss: 533.2687\n",
      "Epoch [167/250], step [41/244], D_loss: 0.0510, M_loss: 0.6308, M_d_loss: 0.0416, M_r_loss: 589.2177\n",
      "Epoch [167/250], step [81/244], D_loss: 0.0544, M_loss: 0.6135, M_d_loss: 0.0431, M_r_loss: 570.3731\n",
      "Epoch [167/250], step [121/244], D_loss: 0.0540, M_loss: 0.6260, M_d_loss: 0.0438, M_r_loss: 582.1803\n",
      "Epoch [167/250], step [161/244], D_loss: 0.0552, M_loss: 0.6082, M_d_loss: 0.0446, M_r_loss: 563.5925\n",
      "Epoch [167/250], step [201/244], D_loss: 0.0483, M_loss: 0.5935, M_d_loss: 0.0414, M_r_loss: 552.1066\n",
      "Epoch [167/250], step [241/244], D_loss: 0.0564, M_loss: 0.5793, M_d_loss: 0.0352, M_r_loss: 544.1257\n",
      "Epoch [168/250], step [1/244], D_loss: 0.0569, M_loss: 0.5522, M_d_loss: 0.0369, M_r_loss: 515.3362\n",
      "Epoch [168/250], step [41/244], D_loss: 0.0664, M_loss: 0.6190, M_d_loss: 0.0398, M_r_loss: 579.2418\n",
      "Epoch [168/250], step [81/244], D_loss: 0.0503, M_loss: 0.6351, M_d_loss: 0.0458, M_r_loss: 589.3152\n",
      "Epoch [168/250], step [121/244], D_loss: 0.0605, M_loss: 0.5780, M_d_loss: 0.0439, M_r_loss: 534.1157\n",
      "Epoch [168/250], step [161/244], D_loss: 0.0558, M_loss: 0.5649, M_d_loss: 0.0432, M_r_loss: 521.7703\n",
      "Epoch [168/250], step [201/244], D_loss: 0.0515, M_loss: 0.5712, M_d_loss: 0.0365, M_r_loss: 534.6300\n",
      "Epoch [168/250], step [241/244], D_loss: 0.0463, M_loss: 0.5415, M_d_loss: 0.0396, M_r_loss: 501.9144\n",
      "Epoch [169/250], step [1/244], D_loss: 0.0452, M_loss: 0.5760, M_d_loss: 0.0429, M_r_loss: 533.1042\n",
      "Epoch [169/250], step [41/244], D_loss: 0.0567, M_loss: 0.6237, M_d_loss: 0.0366, M_r_loss: 587.1437\n",
      "Epoch [169/250], step [81/244], D_loss: 0.0569, M_loss: 0.6128, M_d_loss: 0.0391, M_r_loss: 573.6699\n",
      "Epoch [169/250], step [121/244], D_loss: 0.0535, M_loss: 0.5645, M_d_loss: 0.0438, M_r_loss: 520.6697\n",
      "Epoch [169/250], step [161/244], D_loss: 0.0549, M_loss: 0.5514, M_d_loss: 0.0357, M_r_loss: 515.6967\n",
      "Epoch [169/250], step [201/244], D_loss: 0.0534, M_loss: 0.6070, M_d_loss: 0.0435, M_r_loss: 563.4987\n",
      "Epoch [169/250], step [241/244], D_loss: 0.0565, M_loss: 0.5716, M_d_loss: 0.0418, M_r_loss: 529.8271\n",
      "Epoch [170/250], step [1/244], D_loss: 0.0607, M_loss: 0.5831, M_d_loss: 0.0417, M_r_loss: 541.3987\n",
      "Epoch [170/250], step [41/244], D_loss: 0.0481, M_loss: 0.5794, M_d_loss: 0.0437, M_r_loss: 535.6729\n",
      "Epoch [170/250], step [81/244], D_loss: 0.0556, M_loss: 0.5870, M_d_loss: 0.0447, M_r_loss: 542.3375\n",
      "Epoch [170/250], step [121/244], D_loss: 0.0541, M_loss: 0.5817, M_d_loss: 0.0427, M_r_loss: 539.0571\n",
      "Epoch [170/250], step [161/244], D_loss: 0.0585, M_loss: 0.6363, M_d_loss: 0.0486, M_r_loss: 587.7281\n",
      "Epoch [170/250], step [201/244], D_loss: 0.0523, M_loss: 0.6236, M_d_loss: 0.0433, M_r_loss: 580.2830\n",
      "Epoch [170/250], step [241/244], D_loss: 0.0566, M_loss: 0.6237, M_d_loss: 0.0472, M_r_loss: 576.4535\n",
      "Epoch [171/250], step [1/244], D_loss: 0.0644, M_loss: 0.6051, M_d_loss: 0.0512, M_r_loss: 553.9110\n",
      "Epoch [171/250], step [41/244], D_loss: 0.0471, M_loss: 0.5590, M_d_loss: 0.0399, M_r_loss: 519.1220\n",
      "Epoch [171/250], step [81/244], D_loss: 0.0552, M_loss: 0.5723, M_d_loss: 0.0422, M_r_loss: 530.0927\n",
      "Epoch [171/250], step [121/244], D_loss: 0.0474, M_loss: 0.5829, M_d_loss: 0.0433, M_r_loss: 539.6295\n",
      "Epoch [171/250], step [161/244], D_loss: 0.0498, M_loss: 0.6070, M_d_loss: 0.0402, M_r_loss: 566.8055\n",
      "Epoch [171/250], step [201/244], D_loss: 0.0559, M_loss: 0.5757, M_d_loss: 0.0423, M_r_loss: 533.4194\n",
      "Epoch [171/250], step [241/244], D_loss: 0.0462, M_loss: 0.6189, M_d_loss: 0.0413, M_r_loss: 577.6001\n",
      "Epoch [172/250], step [1/244], D_loss: 0.0488, M_loss: 0.5387, M_d_loss: 0.0438, M_r_loss: 494.8994\n",
      "Epoch [172/250], step [41/244], D_loss: 0.0472, M_loss: 0.6011, M_d_loss: 0.0445, M_r_loss: 556.6183\n",
      "Epoch [172/250], step [81/244], D_loss: 0.0504, M_loss: 0.5602, M_d_loss: 0.0441, M_r_loss: 516.0825\n",
      "Epoch [172/250], step [121/244], D_loss: 0.0779, M_loss: 0.5874, M_d_loss: 0.0407, M_r_loss: 546.6826\n",
      "Epoch [172/250], step [161/244], D_loss: 0.0618, M_loss: 0.6044, M_d_loss: 0.0457, M_r_loss: 558.7238\n",
      "Epoch [172/250], step [201/244], D_loss: 0.0488, M_loss: 0.5882, M_d_loss: 0.0453, M_r_loss: 542.9742\n",
      "Epoch [172/250], step [241/244], D_loss: 0.0486, M_loss: 0.6318, M_d_loss: 0.0404, M_r_loss: 591.3708\n",
      "Epoch [173/250], step [1/244], D_loss: 0.0479, M_loss: 0.5912, M_d_loss: 0.0414, M_r_loss: 549.7609\n",
      "Epoch [173/250], step [41/244], D_loss: 0.0471, M_loss: 0.5692, M_d_loss: 0.0402, M_r_loss: 528.9641\n",
      "Epoch [173/250], step [81/244], D_loss: 0.0585, M_loss: 0.6002, M_d_loss: 0.0429, M_r_loss: 557.2037\n",
      "Epoch [173/250], step [121/244], D_loss: 0.0486, M_loss: 0.5622, M_d_loss: 0.0486, M_r_loss: 513.6082\n",
      "Epoch [173/250], step [161/244], D_loss: 0.0464, M_loss: 0.5936, M_d_loss: 0.0528, M_r_loss: 540.7422\n",
      "Epoch [173/250], step [201/244], D_loss: 0.0496, M_loss: 0.6167, M_d_loss: 0.0445, M_r_loss: 572.1831\n",
      "Epoch [173/250], step [241/244], D_loss: 0.0654, M_loss: 0.5746, M_d_loss: 0.0364, M_r_loss: 538.2759\n",
      "Epoch [174/250], step [1/244], D_loss: 0.0665, M_loss: 0.5616, M_d_loss: 0.0406, M_r_loss: 521.0237\n",
      "Epoch [174/250], step [41/244], D_loss: 0.0628, M_loss: 0.5971, M_d_loss: 0.0424, M_r_loss: 554.7851\n",
      "Epoch [174/250], step [81/244], D_loss: 0.0566, M_loss: 0.5952, M_d_loss: 0.0410, M_r_loss: 554.2130\n",
      "Epoch [174/250], step [121/244], D_loss: 0.0572, M_loss: 0.5888, M_d_loss: 0.0397, M_r_loss: 549.0508\n",
      "Epoch [174/250], step [161/244], D_loss: 0.0442, M_loss: 0.5988, M_d_loss: 0.0506, M_r_loss: 548.1711\n",
      "Epoch [174/250], step [201/244], D_loss: 0.0631, M_loss: 0.5323, M_d_loss: 0.0441, M_r_loss: 488.2318\n",
      "Epoch [174/250], step [241/244], D_loss: 0.0517, M_loss: 0.5893, M_d_loss: 0.0454, M_r_loss: 543.9248\n",
      "Epoch [175/250], step [1/244], D_loss: 0.0511, M_loss: 0.5714, M_d_loss: 0.0489, M_r_loss: 522.5295\n",
      "Epoch [175/250], step [41/244], D_loss: 0.0495, M_loss: 0.5918, M_d_loss: 0.0435, M_r_loss: 548.2859\n",
      "Epoch [175/250], step [81/244], D_loss: 0.0534, M_loss: 0.5831, M_d_loss: 0.0393, M_r_loss: 543.8353\n",
      "Epoch [175/250], step [121/244], D_loss: 0.0472, M_loss: 0.6205, M_d_loss: 0.0499, M_r_loss: 570.5303\n",
      "Epoch [175/250], step [161/244], D_loss: 0.0601, M_loss: 0.5949, M_d_loss: 0.0425, M_r_loss: 552.3975\n",
      "Epoch [175/250], step [201/244], D_loss: 0.0643, M_loss: 0.5818, M_d_loss: 0.0427, M_r_loss: 539.1591\n",
      "Epoch [175/250], step [241/244], D_loss: 0.0496, M_loss: 0.6047, M_d_loss: 0.0451, M_r_loss: 559.5782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [176/250], step [1/244], D_loss: 0.0521, M_loss: 0.5912, M_d_loss: 0.0479, M_r_loss: 543.2357\n",
      "Epoch [176/250], step [41/244], D_loss: 0.0512, M_loss: 0.5960, M_d_loss: 0.0380, M_r_loss: 557.9960\n",
      "Epoch [176/250], step [81/244], D_loss: 0.0506, M_loss: 0.5929, M_d_loss: 0.0425, M_r_loss: 550.4735\n",
      "Epoch [176/250], step [121/244], D_loss: 0.0609, M_loss: 0.5925, M_d_loss: 0.0515, M_r_loss: 541.0253\n",
      "Epoch [176/250], step [161/244], D_loss: 0.0599, M_loss: 0.6122, M_d_loss: 0.0496, M_r_loss: 562.5741\n",
      "Epoch [176/250], step [201/244], D_loss: 0.0676, M_loss: 0.6099, M_d_loss: 0.0406, M_r_loss: 569.3085\n",
      "Epoch [176/250], step [241/244], D_loss: 0.0562, M_loss: 0.6018, M_d_loss: 0.0461, M_r_loss: 555.7426\n",
      "Epoch [177/250], step [1/244], D_loss: 0.0628, M_loss: 0.6133, M_d_loss: 0.0483, M_r_loss: 565.0115\n",
      "Epoch [177/250], step [41/244], D_loss: 0.0608, M_loss: 0.5635, M_d_loss: 0.0427, M_r_loss: 520.8740\n",
      "Epoch [177/250], step [81/244], D_loss: 0.0572, M_loss: 0.5970, M_d_loss: 0.0366, M_r_loss: 560.4038\n",
      "Epoch [177/250], step [121/244], D_loss: 0.0577, M_loss: 0.5983, M_d_loss: 0.0410, M_r_loss: 557.2152\n",
      "Epoch [177/250], step [161/244], D_loss: 0.0455, M_loss: 0.6200, M_d_loss: 0.0445, M_r_loss: 575.4935\n",
      "Epoch [177/250], step [201/244], D_loss: 0.0477, M_loss: 0.5927, M_d_loss: 0.0424, M_r_loss: 550.2466\n",
      "Epoch [177/250], step [241/244], D_loss: 0.0631, M_loss: 0.6048, M_d_loss: 0.0421, M_r_loss: 562.6255\n",
      "Epoch [178/250], step [1/244], D_loss: 0.0659, M_loss: 0.6033, M_d_loss: 0.0449, M_r_loss: 558.4125\n",
      "Epoch [178/250], step [41/244], D_loss: 0.0690, M_loss: 0.5621, M_d_loss: 0.0384, M_r_loss: 523.7052\n",
      "Epoch [178/250], step [81/244], D_loss: 0.0569, M_loss: 0.5873, M_d_loss: 0.0435, M_r_loss: 543.8511\n",
      "Epoch [178/250], step [121/244], D_loss: 0.0517, M_loss: 0.5534, M_d_loss: 0.0394, M_r_loss: 513.9812\n",
      "Epoch [178/250], step [161/244], D_loss: 0.0562, M_loss: 0.5787, M_d_loss: 0.0419, M_r_loss: 536.8397\n",
      "Epoch [178/250], step [201/244], D_loss: 0.0588, M_loss: 0.5855, M_d_loss: 0.0482, M_r_loss: 537.2887\n",
      "Epoch [178/250], step [241/244], D_loss: 0.0533, M_loss: 0.6160, M_d_loss: 0.0467, M_r_loss: 569.2717\n",
      "Epoch [179/250], step [1/244], D_loss: 0.0480, M_loss: 0.6314, M_d_loss: 0.0530, M_r_loss: 578.3646\n",
      "Epoch [179/250], step [41/244], D_loss: 0.0534, M_loss: 0.5655, M_d_loss: 0.0410, M_r_loss: 524.4570\n",
      "Epoch [179/250], step [81/244], D_loss: 0.0568, M_loss: 0.5652, M_d_loss: 0.0411, M_r_loss: 524.0432\n",
      "Epoch [179/250], step [121/244], D_loss: 0.0595, M_loss: 0.6253, M_d_loss: 0.0459, M_r_loss: 579.4303\n",
      "Epoch [179/250], step [161/244], D_loss: 0.0572, M_loss: 0.6243, M_d_loss: 0.0432, M_r_loss: 581.0983\n",
      "Epoch [179/250], step [201/244], D_loss: 0.0678, M_loss: 0.5739, M_d_loss: 0.0391, M_r_loss: 534.8361\n",
      "Epoch [179/250], step [241/244], D_loss: 0.0515, M_loss: 0.5664, M_d_loss: 0.0395, M_r_loss: 526.8604\n",
      "Epoch [180/250], step [1/244], D_loss: 0.0506, M_loss: 0.6213, M_d_loss: 0.0479, M_r_loss: 573.4124\n",
      "Epoch [180/250], step [41/244], D_loss: 0.0494, M_loss: 0.5801, M_d_loss: 0.0430, M_r_loss: 537.1218\n",
      "Epoch [180/250], step [81/244], D_loss: 0.0355, M_loss: 0.6585, M_d_loss: 0.0433, M_r_loss: 615.1971\n",
      "Epoch [180/250], step [121/244], D_loss: 0.0446, M_loss: 0.5679, M_d_loss: 0.0381, M_r_loss: 529.7993\n",
      "Epoch [180/250], step [161/244], D_loss: 0.0592, M_loss: 0.5989, M_d_loss: 0.0405, M_r_loss: 558.3945\n",
      "Epoch [180/250], step [201/244], D_loss: 0.0511, M_loss: 0.5937, M_d_loss: 0.0385, M_r_loss: 555.2319\n",
      "Epoch [180/250], step [241/244], D_loss: 0.0512, M_loss: 0.5557, M_d_loss: 0.0389, M_r_loss: 516.8147\n",
      "Epoch [181/250], step [1/244], D_loss: 0.0504, M_loss: 0.5895, M_d_loss: 0.0422, M_r_loss: 547.2837\n",
      "Epoch [181/250], step [41/244], D_loss: 0.0587, M_loss: 0.5424, M_d_loss: 0.0379, M_r_loss: 504.4904\n",
      "Epoch [181/250], step [81/244], D_loss: 0.0606, M_loss: 0.6288, M_d_loss: 0.0461, M_r_loss: 582.6455\n",
      "Epoch [181/250], step [121/244], D_loss: 0.0564, M_loss: 0.5545, M_d_loss: 0.0425, M_r_loss: 511.9915\n",
      "Epoch [181/250], step [161/244], D_loss: 0.0530, M_loss: 0.5348, M_d_loss: 0.0385, M_r_loss: 496.2905\n",
      "Epoch [181/250], step [201/244], D_loss: 0.0504, M_loss: 0.5685, M_d_loss: 0.0392, M_r_loss: 529.2147\n",
      "Epoch [181/250], step [241/244], D_loss: 0.0514, M_loss: 0.6111, M_d_loss: 0.0441, M_r_loss: 567.0148\n",
      "Epoch [182/250], step [1/244], D_loss: 0.0534, M_loss: 0.5982, M_d_loss: 0.0389, M_r_loss: 559.2972\n",
      "Epoch [182/250], step [41/244], D_loss: 0.0512, M_loss: 0.5975, M_d_loss: 0.0468, M_r_loss: 550.6934\n",
      "Epoch [182/250], step [81/244], D_loss: 0.0632, M_loss: 0.6144, M_d_loss: 0.0458, M_r_loss: 568.5420\n",
      "Epoch [182/250], step [121/244], D_loss: 0.0542, M_loss: 0.6637, M_d_loss: 0.0518, M_r_loss: 611.9429\n",
      "Epoch [182/250], step [161/244], D_loss: 0.0542, M_loss: 0.5692, M_d_loss: 0.0473, M_r_loss: 521.8691\n",
      "Epoch [182/250], step [201/244], D_loss: 0.0660, M_loss: 0.6015, M_d_loss: 0.0450, M_r_loss: 556.4224\n",
      "Epoch [182/250], step [241/244], D_loss: 0.0572, M_loss: 0.5522, M_d_loss: 0.0396, M_r_loss: 512.6012\n",
      "Epoch [183/250], step [1/244], D_loss: 0.0500, M_loss: 0.6000, M_d_loss: 0.0441, M_r_loss: 555.8531\n",
      "Epoch [183/250], step [41/244], D_loss: 0.0500, M_loss: 0.6286, M_d_loss: 0.0441, M_r_loss: 584.4641\n",
      "Epoch [183/250], step [81/244], D_loss: 0.0529, M_loss: 0.5911, M_d_loss: 0.0394, M_r_loss: 551.6966\n",
      "Epoch [183/250], step [121/244], D_loss: 0.0586, M_loss: 0.5890, M_d_loss: 0.0404, M_r_loss: 548.6359\n",
      "Epoch [183/250], step [161/244], D_loss: 0.0553, M_loss: 0.5984, M_d_loss: 0.0433, M_r_loss: 555.0901\n",
      "Epoch [183/250], step [201/244], D_loss: 0.0574, M_loss: 0.5918, M_d_loss: 0.0405, M_r_loss: 551.2495\n",
      "Epoch [183/250], step [241/244], D_loss: 0.0517, M_loss: 0.5681, M_d_loss: 0.0441, M_r_loss: 524.0487\n",
      "Epoch [184/250], step [1/244], D_loss: 0.0543, M_loss: 0.5840, M_d_loss: 0.0427, M_r_loss: 541.2971\n",
      "Epoch [184/250], step [41/244], D_loss: 0.0552, M_loss: 0.6156, M_d_loss: 0.0394, M_r_loss: 576.1616\n",
      "Epoch [184/250], step [81/244], D_loss: 0.0527, M_loss: 0.5917, M_d_loss: 0.0416, M_r_loss: 550.0961\n",
      "Epoch [184/250], step [121/244], D_loss: 0.0520, M_loss: 0.5963, M_d_loss: 0.0428, M_r_loss: 553.4846\n",
      "Epoch [184/250], step [161/244], D_loss: 0.0630, M_loss: 0.6096, M_d_loss: 0.0470, M_r_loss: 562.6020\n",
      "Epoch [184/250], step [201/244], D_loss: 0.0588, M_loss: 0.5933, M_d_loss: 0.0428, M_r_loss: 550.4327\n",
      "Epoch [184/250], step [241/244], D_loss: 0.0520, M_loss: 0.6473, M_d_loss: 0.0469, M_r_loss: 600.3952\n",
      "Epoch [185/250], step [1/244], D_loss: 0.0591, M_loss: 0.6581, M_d_loss: 0.0502, M_r_loss: 607.9282\n",
      "Epoch [185/250], step [41/244], D_loss: 0.0483, M_loss: 0.6297, M_d_loss: 0.0478, M_r_loss: 581.8844\n",
      "Epoch [185/250], step [81/244], D_loss: 0.0494, M_loss: 0.5834, M_d_loss: 0.0379, M_r_loss: 545.5348\n",
      "Epoch [185/250], step [121/244], D_loss: 0.0464, M_loss: 0.5716, M_d_loss: 0.0393, M_r_loss: 532.2787\n",
      "Epoch [185/250], step [161/244], D_loss: 0.0429, M_loss: 0.5758, M_d_loss: 0.0426, M_r_loss: 533.2180\n",
      "Epoch [185/250], step [201/244], D_loss: 0.0579, M_loss: 0.5901, M_d_loss: 0.0424, M_r_loss: 547.6979\n",
      "Epoch [185/250], step [241/244], D_loss: 0.0663, M_loss: 0.6200, M_d_loss: 0.0460, M_r_loss: 574.0316\n",
      "Epoch [186/250], step [1/244], D_loss: 0.0712, M_loss: 0.5690, M_d_loss: 0.0457, M_r_loss: 523.2853\n",
      "Epoch [186/250], step [41/244], D_loss: 0.0534, M_loss: 0.5795, M_d_loss: 0.0441, M_r_loss: 535.3441\n",
      "Epoch [186/250], step [81/244], D_loss: 0.0486, M_loss: 0.5664, M_d_loss: 0.0412, M_r_loss: 525.1608\n",
      "Epoch [186/250], step [121/244], D_loss: 0.0501, M_loss: 0.5961, M_d_loss: 0.0380, M_r_loss: 558.1138\n",
      "Epoch [186/250], step [161/244], D_loss: 0.0565, M_loss: 0.6022, M_d_loss: 0.0454, M_r_loss: 556.7976\n",
      "Epoch [186/250], step [201/244], D_loss: 0.0579, M_loss: 0.5870, M_d_loss: 0.0429, M_r_loss: 544.1145\n",
      "Epoch [186/250], step [241/244], D_loss: 0.0614, M_loss: 0.5796, M_d_loss: 0.0473, M_r_loss: 532.3381\n",
      "Epoch [187/250], step [1/244], D_loss: 0.0571, M_loss: 0.6615, M_d_loss: 0.0546, M_r_loss: 606.9200\n",
      "Epoch [187/250], step [41/244], D_loss: 0.0647, M_loss: 0.5827, M_d_loss: 0.0446, M_r_loss: 538.1470\n",
      "Epoch [187/250], step [81/244], D_loss: 0.0631, M_loss: 0.6156, M_d_loss: 0.0435, M_r_loss: 572.1219\n",
      "Epoch [187/250], step [121/244], D_loss: 0.0634, M_loss: 0.6151, M_d_loss: 0.0419, M_r_loss: 573.1871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [187/250], step [161/244], D_loss: 0.0595, M_loss: 0.5625, M_d_loss: 0.0438, M_r_loss: 518.6321\n",
      "Epoch [187/250], step [201/244], D_loss: 0.0581, M_loss: 0.5660, M_d_loss: 0.0410, M_r_loss: 525.0203\n",
      "Epoch [187/250], step [241/244], D_loss: 0.0593, M_loss: 0.5767, M_d_loss: 0.0385, M_r_loss: 538.1666\n",
      "Epoch [188/250], step [1/244], D_loss: 0.0591, M_loss: 0.6275, M_d_loss: 0.0399, M_r_loss: 587.6102\n",
      "Epoch [188/250], step [41/244], D_loss: 0.0498, M_loss: 0.5450, M_d_loss: 0.0390, M_r_loss: 505.9753\n",
      "Epoch [188/250], step [81/244], D_loss: 0.0565, M_loss: 0.5845, M_d_loss: 0.0410, M_r_loss: 543.5083\n",
      "Epoch [188/250], step [121/244], D_loss: 0.0775, M_loss: 0.5882, M_d_loss: 0.0432, M_r_loss: 545.0168\n",
      "Epoch [188/250], step [161/244], D_loss: 0.0615, M_loss: 0.5827, M_d_loss: 0.0488, M_r_loss: 533.8512\n",
      "Epoch [188/250], step [201/244], D_loss: 0.0675, M_loss: 0.5964, M_d_loss: 0.0417, M_r_loss: 554.6987\n",
      "Epoch [188/250], step [241/244], D_loss: 0.0534, M_loss: 0.6136, M_d_loss: 0.0386, M_r_loss: 574.9163\n",
      "Epoch [189/250], step [1/244], D_loss: 0.0591, M_loss: 0.5896, M_d_loss: 0.0417, M_r_loss: 547.9460\n",
      "Epoch [189/250], step [41/244], D_loss: 0.0541, M_loss: 0.5714, M_d_loss: 0.0367, M_r_loss: 534.6408\n",
      "Epoch [189/250], step [81/244], D_loss: 0.0543, M_loss: 0.5938, M_d_loss: 0.0381, M_r_loss: 555.6307\n",
      "Epoch [189/250], step [121/244], D_loss: 0.0587, M_loss: 0.5823, M_d_loss: 0.0378, M_r_loss: 544.5114\n",
      "Epoch [189/250], step [161/244], D_loss: 0.0611, M_loss: 0.5886, M_d_loss: 0.0442, M_r_loss: 544.4091\n",
      "Epoch [189/250], step [201/244], D_loss: 0.0611, M_loss: 0.5711, M_d_loss: 0.0431, M_r_loss: 527.9876\n",
      "Epoch [189/250], step [241/244], D_loss: 0.0589, M_loss: 0.5640, M_d_loss: 0.0406, M_r_loss: 523.4465\n",
      "Epoch [190/250], step [1/244], D_loss: 0.0587, M_loss: 0.6007, M_d_loss: 0.0419, M_r_loss: 558.7881\n",
      "Epoch [190/250], step [41/244], D_loss: 0.0607, M_loss: 0.5857, M_d_loss: 0.0401, M_r_loss: 545.6587\n",
      "Epoch [190/250], step [81/244], D_loss: 0.0587, M_loss: 0.5727, M_d_loss: 0.0406, M_r_loss: 532.0443\n",
      "Epoch [190/250], step [121/244], D_loss: 0.0512, M_loss: 0.5869, M_d_loss: 0.0442, M_r_loss: 542.6292\n",
      "Epoch [190/250], step [161/244], D_loss: 0.0576, M_loss: 0.5964, M_d_loss: 0.0396, M_r_loss: 556.7564\n",
      "Epoch [190/250], step [201/244], D_loss: 0.0553, M_loss: 0.5855, M_d_loss: 0.0392, M_r_loss: 546.2988\n",
      "Epoch [190/250], step [241/244], D_loss: 0.0525, M_loss: 0.5805, M_d_loss: 0.0401, M_r_loss: 540.4404\n",
      "Epoch [191/250], step [1/244], D_loss: 0.0544, M_loss: 0.5717, M_d_loss: 0.0390, M_r_loss: 532.7051\n",
      "Epoch [191/250], step [41/244], D_loss: 0.0522, M_loss: 0.6073, M_d_loss: 0.0360, M_r_loss: 571.3052\n",
      "Epoch [191/250], step [81/244], D_loss: 0.0474, M_loss: 0.6025, M_d_loss: 0.0391, M_r_loss: 563.3839\n",
      "Epoch [191/250], step [121/244], D_loss: 0.0522, M_loss: 0.5879, M_d_loss: 0.0376, M_r_loss: 550.2357\n",
      "Epoch [191/250], step [161/244], D_loss: 0.0565, M_loss: 0.5923, M_d_loss: 0.0404, M_r_loss: 551.8890\n",
      "Epoch [191/250], step [201/244], D_loss: 0.0554, M_loss: 0.5860, M_d_loss: 0.0386, M_r_loss: 547.4286\n",
      "Epoch [191/250], step [241/244], D_loss: 0.0550, M_loss: 0.5876, M_d_loss: 0.0418, M_r_loss: 545.7209\n",
      "Epoch [192/250], step [1/244], D_loss: 0.0598, M_loss: 0.5500, M_d_loss: 0.0408, M_r_loss: 509.1530\n",
      "Epoch [192/250], step [41/244], D_loss: 0.0465, M_loss: 0.5779, M_d_loss: 0.0395, M_r_loss: 538.3931\n",
      "Epoch [192/250], step [81/244], D_loss: 0.0456, M_loss: 0.5806, M_d_loss: 0.0378, M_r_loss: 542.8262\n",
      "Epoch [192/250], step [121/244], D_loss: 0.0492, M_loss: 0.6020, M_d_loss: 0.0417, M_r_loss: 560.3264\n",
      "Epoch [192/250], step [161/244], D_loss: 0.0577, M_loss: 0.6339, M_d_loss: 0.0526, M_r_loss: 581.3098\n",
      "Epoch [192/250], step [201/244], D_loss: 0.0708, M_loss: 0.6031, M_d_loss: 0.0519, M_r_loss: 551.2517\n",
      "Epoch [192/250], step [241/244], D_loss: 0.0698, M_loss: 0.6135, M_d_loss: 0.0434, M_r_loss: 570.0752\n",
      "Epoch [193/250], step [1/244], D_loss: 0.0710, M_loss: 0.6185, M_d_loss: 0.0433, M_r_loss: 575.2365\n",
      "Epoch [193/250], step [41/244], D_loss: 0.0660, M_loss: 0.5888, M_d_loss: 0.0413, M_r_loss: 547.5345\n",
      "Epoch [193/250], step [81/244], D_loss: 0.0599, M_loss: 0.6039, M_d_loss: 0.0431, M_r_loss: 560.8444\n",
      "Epoch [193/250], step [121/244], D_loss: 0.0690, M_loss: 0.5682, M_d_loss: 0.0358, M_r_loss: 532.4438\n",
      "Epoch [193/250], step [161/244], D_loss: 0.0739, M_loss: 0.5611, M_d_loss: 0.0410, M_r_loss: 520.0450\n",
      "Epoch [193/250], step [201/244], D_loss: 0.0546, M_loss: 0.6026, M_d_loss: 0.0439, M_r_loss: 558.6796\n",
      "Epoch [193/250], step [241/244], D_loss: 0.0437, M_loss: 0.5791, M_d_loss: 0.0372, M_r_loss: 541.9535\n",
      "Epoch [194/250], step [1/244], D_loss: 0.0444, M_loss: 0.5798, M_d_loss: 0.0391, M_r_loss: 540.7365\n",
      "Epoch [194/250], step [41/244], D_loss: 0.0402, M_loss: 0.6097, M_d_loss: 0.0370, M_r_loss: 572.7163\n",
      "Epoch [194/250], step [81/244], D_loss: 0.0520, M_loss: 0.6034, M_d_loss: 0.0340, M_r_loss: 569.4240\n",
      "Epoch [194/250], step [121/244], D_loss: 0.0574, M_loss: 0.6216, M_d_loss: 0.0416, M_r_loss: 579.9753\n",
      "Epoch [194/250], step [161/244], D_loss: 0.0640, M_loss: 0.6259, M_d_loss: 0.0429, M_r_loss: 582.9510\n",
      "Epoch [194/250], step [201/244], D_loss: 0.0593, M_loss: 0.5810, M_d_loss: 0.0444, M_r_loss: 536.6195\n",
      "Epoch [194/250], step [241/244], D_loss: 0.0513, M_loss: 0.6323, M_d_loss: 0.0426, M_r_loss: 589.6556\n",
      "Epoch [195/250], step [1/244], D_loss: 0.0580, M_loss: 0.6065, M_d_loss: 0.0460, M_r_loss: 560.4703\n",
      "Epoch [195/250], step [41/244], D_loss: 0.0611, M_loss: 0.6453, M_d_loss: 0.0407, M_r_loss: 604.6162\n",
      "Epoch [195/250], step [81/244], D_loss: 0.0670, M_loss: 0.5612, M_d_loss: 0.0401, M_r_loss: 521.0852\n",
      "Epoch [195/250], step [121/244], D_loss: 0.0588, M_loss: 0.5679, M_d_loss: 0.0415, M_r_loss: 526.3531\n",
      "Epoch [195/250], step [161/244], D_loss: 0.0476, M_loss: 0.5968, M_d_loss: 0.0405, M_r_loss: 556.2600\n",
      "Epoch [195/250], step [201/244], D_loss: 0.0544, M_loss: 0.5995, M_d_loss: 0.0356, M_r_loss: 563.8540\n",
      "Epoch [195/250], step [241/244], D_loss: 0.0524, M_loss: 0.5877, M_d_loss: 0.0394, M_r_loss: 548.2615\n",
      "Epoch [196/250], step [1/244], D_loss: 0.0527, M_loss: 0.5845, M_d_loss: 0.0463, M_r_loss: 538.1810\n",
      "Epoch [196/250], step [41/244], D_loss: 0.0569, M_loss: 0.5601, M_d_loss: 0.0352, M_r_loss: 524.8727\n",
      "Epoch [196/250], step [81/244], D_loss: 0.0564, M_loss: 0.5898, M_d_loss: 0.0375, M_r_loss: 552.2870\n",
      "Epoch [196/250], step [121/244], D_loss: 0.0542, M_loss: 0.5477, M_d_loss: 0.0380, M_r_loss: 509.7452\n",
      "Epoch [196/250], step [161/244], D_loss: 0.0570, M_loss: 0.5972, M_d_loss: 0.0410, M_r_loss: 556.2184\n",
      "Epoch [196/250], step [201/244], D_loss: 0.0545, M_loss: 0.6063, M_d_loss: 0.0400, M_r_loss: 566.3678\n",
      "Epoch [196/250], step [241/244], D_loss: 0.0500, M_loss: 0.5990, M_d_loss: 0.0466, M_r_loss: 552.4297\n",
      "Epoch [197/250], step [1/244], D_loss: 0.0546, M_loss: 0.6267, M_d_loss: 0.0493, M_r_loss: 577.3841\n",
      "Epoch [197/250], step [41/244], D_loss: 0.0541, M_loss: 0.5895, M_d_loss: 0.0427, M_r_loss: 546.8463\n",
      "Epoch [197/250], step [81/244], D_loss: 0.0555, M_loss: 0.5700, M_d_loss: 0.0426, M_r_loss: 527.4053\n",
      "Epoch [197/250], step [121/244], D_loss: 0.0712, M_loss: 0.5839, M_d_loss: 0.0385, M_r_loss: 545.3466\n",
      "Epoch [197/250], step [161/244], D_loss: 0.0595, M_loss: 0.5972, M_d_loss: 0.0430, M_r_loss: 554.2234\n",
      "Epoch [197/250], step [201/244], D_loss: 0.0562, M_loss: 0.5578, M_d_loss: 0.0399, M_r_loss: 517.9041\n",
      "Epoch [197/250], step [241/244], D_loss: 0.0654, M_loss: 0.5679, M_d_loss: 0.0373, M_r_loss: 530.6049\n",
      "Epoch [198/250], step [1/244], D_loss: 0.0595, M_loss: 0.5899, M_d_loss: 0.0410, M_r_loss: 548.8564\n",
      "Epoch [198/250], step [41/244], D_loss: 0.0508, M_loss: 0.5712, M_d_loss: 0.0404, M_r_loss: 530.8376\n",
      "Epoch [198/250], step [81/244], D_loss: 0.0524, M_loss: 0.6329, M_d_loss: 0.0402, M_r_loss: 592.7306\n",
      "Epoch [198/250], step [121/244], D_loss: 0.0630, M_loss: 0.5926, M_d_loss: 0.0410, M_r_loss: 551.5175\n",
      "Epoch [198/250], step [161/244], D_loss: 0.0560, M_loss: 0.5692, M_d_loss: 0.0403, M_r_loss: 528.9315\n",
      "Epoch [198/250], step [201/244], D_loss: 0.0494, M_loss: 0.6023, M_d_loss: 0.0420, M_r_loss: 560.2472\n",
      "Epoch [198/250], step [241/244], D_loss: 0.0560, M_loss: 0.5891, M_d_loss: 0.0391, M_r_loss: 549.9763\n",
      "Epoch [199/250], step [1/244], D_loss: 0.0565, M_loss: 0.5919, M_d_loss: 0.0427, M_r_loss: 549.2259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [199/250], step [41/244], D_loss: 0.0525, M_loss: 0.5763, M_d_loss: 0.0442, M_r_loss: 532.1790\n",
      "Epoch [199/250], step [81/244], D_loss: 0.0528, M_loss: 0.6074, M_d_loss: 0.0407, M_r_loss: 566.7642\n",
      "Epoch [199/250], step [121/244], D_loss: 0.0561, M_loss: 0.5964, M_d_loss: 0.0391, M_r_loss: 557.3458\n",
      "Epoch [199/250], step [161/244], D_loss: 0.0489, M_loss: 0.5655, M_d_loss: 0.0372, M_r_loss: 528.3440\n",
      "Epoch [199/250], step [201/244], D_loss: 0.0533, M_loss: 0.5864, M_d_loss: 0.0413, M_r_loss: 545.1699\n",
      "Epoch [199/250], step [241/244], D_loss: 0.0606, M_loss: 0.5796, M_d_loss: 0.0389, M_r_loss: 540.6922\n",
      "Epoch [200/250], step [1/244], D_loss: 0.0634, M_loss: 0.5847, M_d_loss: 0.0399, M_r_loss: 544.7720\n",
      "Epoch [200/250], step [41/244], D_loss: 0.0565, M_loss: 0.5936, M_d_loss: 0.0386, M_r_loss: 555.0253\n",
      "Epoch [200/250], step [81/244], D_loss: 0.0474, M_loss: 0.6337, M_d_loss: 0.0432, M_r_loss: 590.5939\n",
      "Epoch [200/250], step [121/244], D_loss: 0.0526, M_loss: 0.5627, M_d_loss: 0.0375, M_r_loss: 525.1854\n",
      "Epoch [200/250], step [161/244], D_loss: 0.0500, M_loss: 0.5975, M_d_loss: 0.0408, M_r_loss: 556.6836\n",
      "Epoch [200/250], step [201/244], D_loss: 0.0593, M_loss: 0.5848, M_d_loss: 0.0439, M_r_loss: 540.9069\n",
      "Epoch [200/250], step [241/244], D_loss: 0.0716, M_loss: 0.5827, M_d_loss: 0.0449, M_r_loss: 537.8868\n",
      "Epoch [201/250], step [1/244], D_loss: 0.0781, M_loss: 0.5652, M_d_loss: 0.0485, M_r_loss: 516.6877\n",
      "Epoch [201/250], step [41/244], D_loss: 0.0626, M_loss: 0.6067, M_d_loss: 0.0436, M_r_loss: 563.0969\n",
      "Epoch [201/250], step [81/244], D_loss: 0.0511, M_loss: 0.6053, M_d_loss: 0.0431, M_r_loss: 562.2795\n",
      "Epoch [201/250], step [121/244], D_loss: 0.0574, M_loss: 0.5814, M_d_loss: 0.0388, M_r_loss: 542.6097\n",
      "Epoch [201/250], step [161/244], D_loss: 0.0589, M_loss: 0.5955, M_d_loss: 0.0361, M_r_loss: 559.3446\n",
      "Epoch [201/250], step [201/244], D_loss: 0.0557, M_loss: 0.5767, M_d_loss: 0.0372, M_r_loss: 539.4841\n",
      "Epoch [201/250], step [241/244], D_loss: 0.0518, M_loss: 0.5599, M_d_loss: 0.0378, M_r_loss: 522.0442\n",
      "Epoch [202/250], step [1/244], D_loss: 0.0478, M_loss: 0.5810, M_d_loss: 0.0405, M_r_loss: 540.4377\n",
      "Epoch [202/250], step [41/244], D_loss: 0.0451, M_loss: 0.6259, M_d_loss: 0.0395, M_r_loss: 586.3953\n",
      "Epoch [202/250], step [81/244], D_loss: 0.0456, M_loss: 0.5839, M_d_loss: 0.0388, M_r_loss: 545.0966\n",
      "Epoch [202/250], step [121/244], D_loss: 0.0437, M_loss: 0.5892, M_d_loss: 0.0381, M_r_loss: 551.0186\n",
      "Epoch [202/250], step [161/244], D_loss: 0.0467, M_loss: 0.5941, M_d_loss: 0.0375, M_r_loss: 556.5450\n",
      "Epoch [202/250], step [201/244], D_loss: 0.0466, M_loss: 0.5594, M_d_loss: 0.0355, M_r_loss: 523.9203\n",
      "Epoch [202/250], step [241/244], D_loss: 0.0504, M_loss: 0.6001, M_d_loss: 0.0414, M_r_loss: 558.6498\n",
      "Epoch [203/250], step [1/244], D_loss: 0.0519, M_loss: 0.6025, M_d_loss: 0.0421, M_r_loss: 560.4438\n",
      "Epoch [203/250], step [41/244], D_loss: 0.0576, M_loss: 0.5921, M_d_loss: 0.0412, M_r_loss: 550.8961\n",
      "Epoch [203/250], step [81/244], D_loss: 0.0587, M_loss: 0.6122, M_d_loss: 0.0442, M_r_loss: 568.0594\n",
      "Epoch [203/250], step [121/244], D_loss: 0.0535, M_loss: 0.5923, M_d_loss: 0.0457, M_r_loss: 546.6326\n",
      "Epoch [203/250], step [161/244], D_loss: 0.0471, M_loss: 0.6611, M_d_loss: 0.0460, M_r_loss: 615.1494\n",
      "Epoch [203/250], step [201/244], D_loss: 0.0479, M_loss: 0.6384, M_d_loss: 0.0433, M_r_loss: 595.0887\n",
      "Epoch [203/250], step [241/244], D_loss: 0.0513, M_loss: 0.5492, M_d_loss: 0.0398, M_r_loss: 509.3750\n",
      "Epoch [204/250], step [1/244], D_loss: 0.0455, M_loss: 0.5892, M_d_loss: 0.0415, M_r_loss: 547.6610\n",
      "Epoch [204/250], step [41/244], D_loss: 0.0443, M_loss: 0.5668, M_d_loss: 0.0378, M_r_loss: 529.0793\n",
      "Epoch [204/250], step [81/244], D_loss: 0.0481, M_loss: 0.5772, M_d_loss: 0.0401, M_r_loss: 537.1110\n",
      "Epoch [204/250], step [121/244], D_loss: 0.0555, M_loss: 0.5623, M_d_loss: 0.0405, M_r_loss: 521.7949\n",
      "Epoch [204/250], step [161/244], D_loss: 0.0546, M_loss: 0.5612, M_d_loss: 0.0384, M_r_loss: 522.7992\n",
      "Epoch [204/250], step [201/244], D_loss: 0.0494, M_loss: 0.5899, M_d_loss: 0.0395, M_r_loss: 550.3536\n",
      "Epoch [204/250], step [241/244], D_loss: 0.0509, M_loss: 0.5774, M_d_loss: 0.0417, M_r_loss: 535.7157\n",
      "Epoch [205/250], step [1/244], D_loss: 0.0534, M_loss: 0.5970, M_d_loss: 0.0422, M_r_loss: 554.8755\n",
      "Epoch [205/250], step [41/244], D_loss: 0.0564, M_loss: 0.6105, M_d_loss: 0.0393, M_r_loss: 571.2222\n",
      "Epoch [205/250], step [81/244], D_loss: 0.0564, M_loss: 0.6088, M_d_loss: 0.0396, M_r_loss: 569.2589\n",
      "Epoch [205/250], step [121/244], D_loss: 0.0510, M_loss: 0.6242, M_d_loss: 0.0411, M_r_loss: 583.1210\n",
      "Epoch [205/250], step [161/244], D_loss: 0.0537, M_loss: 0.6020, M_d_loss: 0.0434, M_r_loss: 558.5096\n",
      "Epoch [205/250], step [201/244], D_loss: 0.0497, M_loss: 0.5799, M_d_loss: 0.0468, M_r_loss: 533.1231\n",
      "Epoch [205/250], step [241/244], D_loss: 0.0566, M_loss: 0.5838, M_d_loss: 0.0469, M_r_loss: 536.8893\n",
      "Epoch [206/250], step [1/244], D_loss: 0.0496, M_loss: 0.6023, M_d_loss: 0.0424, M_r_loss: 559.9061\n",
      "Epoch [206/250], step [41/244], D_loss: 0.0604, M_loss: 0.5944, M_d_loss: 0.0420, M_r_loss: 552.3433\n",
      "Epoch [206/250], step [81/244], D_loss: 0.0568, M_loss: 0.6407, M_d_loss: 0.0436, M_r_loss: 597.1014\n",
      "Epoch [206/250], step [121/244], D_loss: 0.0561, M_loss: 0.5872, M_d_loss: 0.0425, M_r_loss: 544.7130\n",
      "Epoch [206/250], step [161/244], D_loss: 0.0580, M_loss: 0.6079, M_d_loss: 0.0399, M_r_loss: 567.9708\n",
      "Epoch [206/250], step [201/244], D_loss: 0.0605, M_loss: 0.5889, M_d_loss: 0.0410, M_r_loss: 547.9734\n",
      "Epoch [206/250], step [241/244], D_loss: 0.0521, M_loss: 0.5769, M_d_loss: 0.0439, M_r_loss: 533.0089\n",
      "Epoch [207/250], step [1/244], D_loss: 0.0510, M_loss: 0.5699, M_d_loss: 0.0477, M_r_loss: 522.1755\n",
      "Epoch [207/250], step [41/244], D_loss: 0.0469, M_loss: 0.5817, M_d_loss: 0.0412, M_r_loss: 540.4729\n",
      "Epoch [207/250], step [81/244], D_loss: 0.0432, M_loss: 0.5852, M_d_loss: 0.0444, M_r_loss: 540.7537\n",
      "Epoch [207/250], step [121/244], D_loss: 0.0534, M_loss: 0.6050, M_d_loss: 0.0438, M_r_loss: 561.1289\n",
      "Epoch [207/250], step [161/244], D_loss: 0.0583, M_loss: 0.5990, M_d_loss: 0.0420, M_r_loss: 557.0818\n",
      "Epoch [207/250], step [201/244], D_loss: 0.0710, M_loss: 0.5802, M_d_loss: 0.0374, M_r_loss: 542.8636\n",
      "Epoch [207/250], step [241/244], D_loss: 0.0574, M_loss: 0.5856, M_d_loss: 0.0409, M_r_loss: 544.7554\n",
      "Epoch [208/250], step [1/244], D_loss: 0.0594, M_loss: 0.6193, M_d_loss: 0.0438, M_r_loss: 575.5441\n",
      "Epoch [208/250], step [41/244], D_loss: 0.0509, M_loss: 0.5606, M_d_loss: 0.0398, M_r_loss: 520.8217\n",
      "Epoch [208/250], step [81/244], D_loss: 0.0467, M_loss: 0.6028, M_d_loss: 0.0421, M_r_loss: 560.7020\n",
      "Epoch [208/250], step [121/244], D_loss: 0.0528, M_loss: 0.6190, M_d_loss: 0.0444, M_r_loss: 574.6031\n",
      "Epoch [208/250], step [161/244], D_loss: 0.0524, M_loss: 0.5802, M_d_loss: 0.0421, M_r_loss: 538.0975\n",
      "Epoch [208/250], step [201/244], D_loss: 0.0517, M_loss: 0.5987, M_d_loss: 0.0446, M_r_loss: 554.1008\n",
      "Epoch [208/250], step [241/244], D_loss: 0.0468, M_loss: 0.5903, M_d_loss: 0.0440, M_r_loss: 546.2803\n",
      "Epoch [209/250], step [1/244], D_loss: 0.0462, M_loss: 0.6648, M_d_loss: 0.0493, M_r_loss: 615.4989\n",
      "Epoch [209/250], step [41/244], D_loss: 0.0451, M_loss: 0.5857, M_d_loss: 0.0416, M_r_loss: 544.1490\n",
      "Epoch [209/250], step [81/244], D_loss: 0.0476, M_loss: 0.6116, M_d_loss: 0.0411, M_r_loss: 570.4896\n",
      "Epoch [209/250], step [121/244], D_loss: 0.0502, M_loss: 0.5942, M_d_loss: 0.0426, M_r_loss: 551.6369\n",
      "Epoch [209/250], step [161/244], D_loss: 0.0547, M_loss: 0.5346, M_d_loss: 0.0381, M_r_loss: 496.5659\n",
      "Epoch [209/250], step [201/244], D_loss: 0.0534, M_loss: 0.6150, M_d_loss: 0.0478, M_r_loss: 567.1712\n",
      "Epoch [209/250], step [241/244], D_loss: 0.0530, M_loss: 0.5773, M_d_loss: 0.0469, M_r_loss: 530.4703\n",
      "Epoch [210/250], step [1/244], D_loss: 0.0534, M_loss: 0.6082, M_d_loss: 0.0516, M_r_loss: 556.5275\n",
      "Epoch [210/250], step [41/244], D_loss: 0.0478, M_loss: 0.5563, M_d_loss: 0.0487, M_r_loss: 507.6371\n",
      "Epoch [210/250], step [81/244], D_loss: 0.0465, M_loss: 0.6132, M_d_loss: 0.0458, M_r_loss: 567.3792\n",
      "Epoch [210/250], step [121/244], D_loss: 0.0552, M_loss: 0.5973, M_d_loss: 0.0423, M_r_loss: 554.9830\n",
      "Epoch [210/250], step [161/244], D_loss: 0.0569, M_loss: 0.5878, M_d_loss: 0.0415, M_r_loss: 546.2999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [210/250], step [201/244], D_loss: 0.0609, M_loss: 0.5580, M_d_loss: 0.0375, M_r_loss: 520.5202\n",
      "Epoch [210/250], step [241/244], D_loss: 0.0701, M_loss: 0.5915, M_d_loss: 0.0392, M_r_loss: 552.3030\n",
      "Epoch [211/250], step [1/244], D_loss: 0.0659, M_loss: 0.5837, M_d_loss: 0.0418, M_r_loss: 541.9391\n",
      "Epoch [211/250], step [41/244], D_loss: 0.0567, M_loss: 0.6435, M_d_loss: 0.0432, M_r_loss: 600.2681\n",
      "Epoch [211/250], step [81/244], D_loss: 0.0583, M_loss: 0.5905, M_d_loss: 0.0388, M_r_loss: 551.7493\n",
      "Epoch [211/250], step [121/244], D_loss: 0.0559, M_loss: 0.5483, M_d_loss: 0.0384, M_r_loss: 509.8801\n",
      "Epoch [211/250], step [161/244], D_loss: 0.0519, M_loss: 0.5830, M_d_loss: 0.0427, M_r_loss: 540.2804\n",
      "Epoch [211/250], step [201/244], D_loss: 0.0575, M_loss: 0.5687, M_d_loss: 0.0419, M_r_loss: 526.8436\n",
      "Epoch [211/250], step [241/244], D_loss: 0.0575, M_loss: 0.5830, M_d_loss: 0.0381, M_r_loss: 544.8868\n",
      "Epoch [212/250], step [1/244], D_loss: 0.0529, M_loss: 0.6403, M_d_loss: 0.0446, M_r_loss: 595.6588\n",
      "Epoch [212/250], step [41/244], D_loss: 0.0557, M_loss: 0.6236, M_d_loss: 0.0412, M_r_loss: 582.4257\n",
      "Epoch [212/250], step [81/244], D_loss: 0.0457, M_loss: 0.5934, M_d_loss: 0.0414, M_r_loss: 551.9490\n",
      "Epoch [212/250], step [121/244], D_loss: 0.0484, M_loss: 0.5564, M_d_loss: 0.0386, M_r_loss: 517.7466\n",
      "Epoch [212/250], step [161/244], D_loss: 0.0472, M_loss: 0.5702, M_d_loss: 0.0389, M_r_loss: 531.2971\n",
      "Epoch [212/250], step [201/244], D_loss: 0.0464, M_loss: 0.5695, M_d_loss: 0.0440, M_r_loss: 525.4708\n",
      "Epoch [212/250], step [241/244], D_loss: 0.0479, M_loss: 0.5845, M_d_loss: 0.0439, M_r_loss: 540.6514\n",
      "Epoch [213/250], step [1/244], D_loss: 0.0536, M_loss: 0.5511, M_d_loss: 0.0435, M_r_loss: 507.6417\n",
      "Epoch [213/250], step [41/244], D_loss: 0.0596, M_loss: 0.5263, M_d_loss: 0.0366, M_r_loss: 489.6502\n",
      "Epoch [213/250], step [81/244], D_loss: 0.0611, M_loss: 0.5805, M_d_loss: 0.0388, M_r_loss: 541.6982\n",
      "Epoch [213/250], step [121/244], D_loss: 0.0612, M_loss: 0.5749, M_d_loss: 0.0444, M_r_loss: 530.4259\n",
      "Epoch [213/250], step [161/244], D_loss: 0.0567, M_loss: 0.6005, M_d_loss: 0.0425, M_r_loss: 558.0583\n",
      "Epoch [213/250], step [201/244], D_loss: 0.0537, M_loss: 0.5804, M_d_loss: 0.0433, M_r_loss: 537.0256\n",
      "Epoch [213/250], step [241/244], D_loss: 0.0610, M_loss: 0.5812, M_d_loss: 0.0413, M_r_loss: 539.9174\n",
      "Epoch [214/250], step [1/244], D_loss: 0.0571, M_loss: 0.5762, M_d_loss: 0.0419, M_r_loss: 534.2733\n",
      "Epoch [214/250], step [41/244], D_loss: 0.0526, M_loss: 0.6192, M_d_loss: 0.0450, M_r_loss: 574.2512\n",
      "Epoch [214/250], step [81/244], D_loss: 0.0511, M_loss: 0.6145, M_d_loss: 0.0458, M_r_loss: 568.7078\n",
      "Epoch [214/250], step [121/244], D_loss: 0.0536, M_loss: 0.6042, M_d_loss: 0.0431, M_r_loss: 561.1897\n",
      "Epoch [214/250], step [161/244], D_loss: 0.0502, M_loss: 0.5731, M_d_loss: 0.0400, M_r_loss: 533.1539\n",
      "Epoch [214/250], step [201/244], D_loss: 0.0488, M_loss: 0.6086, M_d_loss: 0.0442, M_r_loss: 564.4159\n",
      "Epoch [214/250], step [241/244], D_loss: 0.0417, M_loss: 0.6182, M_d_loss: 0.0463, M_r_loss: 571.8507\n",
      "Epoch [215/250], step [1/244], D_loss: 0.0443, M_loss: 0.6146, M_d_loss: 0.0462, M_r_loss: 568.3674\n",
      "Epoch [215/250], step [41/244], D_loss: 0.0468, M_loss: 0.5565, M_d_loss: 0.0424, M_r_loss: 514.0706\n",
      "Epoch [215/250], step [81/244], D_loss: 0.0531, M_loss: 0.5713, M_d_loss: 0.0400, M_r_loss: 531.2341\n",
      "Epoch [215/250], step [121/244], D_loss: 0.0639, M_loss: 0.6088, M_d_loss: 0.0406, M_r_loss: 568.1447\n",
      "Epoch [215/250], step [161/244], D_loss: 0.0839, M_loss: 0.6406, M_d_loss: 0.0447, M_r_loss: 595.8336\n",
      "Epoch [215/250], step [201/244], D_loss: 0.0795, M_loss: 0.6316, M_d_loss: 0.0473, M_r_loss: 584.3628\n",
      "Epoch [215/250], step [241/244], D_loss: 0.0724, M_loss: 0.6018, M_d_loss: 0.0445, M_r_loss: 557.3492\n",
      "Epoch [216/250], step [1/244], D_loss: 0.0767, M_loss: 0.6539, M_d_loss: 0.0615, M_r_loss: 592.3368\n",
      "Epoch [216/250], step [41/244], D_loss: 0.0592, M_loss: 0.5846, M_d_loss: 0.0426, M_r_loss: 541.9747\n",
      "Epoch [216/250], step [81/244], D_loss: 0.0589, M_loss: 0.5844, M_d_loss: 0.0402, M_r_loss: 544.1816\n",
      "Epoch [216/250], step [121/244], D_loss: 0.0621, M_loss: 0.5927, M_d_loss: 0.0349, M_r_loss: 557.8506\n",
      "Epoch [216/250], step [161/244], D_loss: 0.0577, M_loss: 0.5239, M_d_loss: 0.0364, M_r_loss: 487.5426\n",
      "Epoch [216/250], step [201/244], D_loss: 0.0493, M_loss: 0.5782, M_d_loss: 0.0368, M_r_loss: 541.4258\n",
      "Epoch [216/250], step [241/244], D_loss: 0.0447, M_loss: 0.6160, M_d_loss: 0.0415, M_r_loss: 574.4916\n",
      "Epoch [217/250], step [1/244], D_loss: 0.0490, M_loss: 0.5735, M_d_loss: 0.0418, M_r_loss: 531.6340\n",
      "Epoch [217/250], step [41/244], D_loss: 0.0487, M_loss: 0.6078, M_d_loss: 0.0410, M_r_loss: 566.7694\n",
      "Epoch [217/250], step [81/244], D_loss: 0.0528, M_loss: 0.6306, M_d_loss: 0.0427, M_r_loss: 587.8508\n",
      "Epoch [217/250], step [121/244], D_loss: 0.0573, M_loss: 0.6113, M_d_loss: 0.0452, M_r_loss: 566.1726\n",
      "Epoch [217/250], step [161/244], D_loss: 0.0580, M_loss: 0.6004, M_d_loss: 0.0432, M_r_loss: 557.2169\n",
      "Epoch [217/250], step [201/244], D_loss: 0.0456, M_loss: 0.5839, M_d_loss: 0.0422, M_r_loss: 541.7829\n",
      "Epoch [217/250], step [241/244], D_loss: 0.0505, M_loss: 0.5912, M_d_loss: 0.0388, M_r_loss: 552.4132\n",
      "Epoch [218/250], step [1/244], D_loss: 0.0481, M_loss: 0.6354, M_d_loss: 0.0404, M_r_loss: 595.0623\n",
      "Epoch [218/250], step [41/244], D_loss: 0.0611, M_loss: 0.5570, M_d_loss: 0.0356, M_r_loss: 521.4543\n",
      "Epoch [218/250], step [81/244], D_loss: 0.0568, M_loss: 0.5872, M_d_loss: 0.0403, M_r_loss: 546.9501\n",
      "Epoch [218/250], step [121/244], D_loss: 0.0461, M_loss: 0.5905, M_d_loss: 0.0436, M_r_loss: 546.8805\n",
      "Epoch [218/250], step [161/244], D_loss: 0.0483, M_loss: 0.6036, M_d_loss: 0.0416, M_r_loss: 561.9913\n",
      "Epoch [218/250], step [201/244], D_loss: 0.0491, M_loss: 0.5751, M_d_loss: 0.0400, M_r_loss: 535.0634\n",
      "Epoch [218/250], step [241/244], D_loss: 0.0557, M_loss: 0.5880, M_d_loss: 0.0390, M_r_loss: 549.0214\n",
      "Epoch [219/250], step [1/244], D_loss: 0.0587, M_loss: 0.5553, M_d_loss: 0.0437, M_r_loss: 511.6151\n",
      "Epoch [219/250], step [41/244], D_loss: 0.0595, M_loss: 0.6062, M_d_loss: 0.0418, M_r_loss: 564.4378\n",
      "Epoch [219/250], step [81/244], D_loss: 0.0584, M_loss: 0.6038, M_d_loss: 0.0465, M_r_loss: 557.3323\n",
      "Epoch [219/250], step [121/244], D_loss: 0.0597, M_loss: 0.6188, M_d_loss: 0.0446, M_r_loss: 574.1227\n",
      "Epoch [219/250], step [161/244], D_loss: 0.0597, M_loss: 0.5871, M_d_loss: 0.0410, M_r_loss: 546.1191\n",
      "Epoch [219/250], step [201/244], D_loss: 0.0509, M_loss: 0.5941, M_d_loss: 0.0464, M_r_loss: 547.6478\n",
      "Epoch [219/250], step [241/244], D_loss: 0.0477, M_loss: 0.5761, M_d_loss: 0.0397, M_r_loss: 536.3710\n",
      "Epoch [220/250], step [1/244], D_loss: 0.0508, M_loss: 0.5955, M_d_loss: 0.0420, M_r_loss: 553.5406\n",
      "Epoch [220/250], step [41/244], D_loss: 0.0471, M_loss: 0.5831, M_d_loss: 0.0413, M_r_loss: 541.8600\n",
      "Epoch [220/250], step [81/244], D_loss: 0.0409, M_loss: 0.6153, M_d_loss: 0.0433, M_r_loss: 571.9290\n",
      "Epoch [220/250], step [121/244], D_loss: 0.0557, M_loss: 0.5333, M_d_loss: 0.0369, M_r_loss: 496.4261\n",
      "Epoch [220/250], step [161/244], D_loss: 0.0552, M_loss: 0.6052, M_d_loss: 0.0397, M_r_loss: 565.4480\n",
      "Epoch [220/250], step [201/244], D_loss: 0.0545, M_loss: 0.5902, M_d_loss: 0.0390, M_r_loss: 551.2540\n",
      "Epoch [220/250], step [241/244], D_loss: 0.0624, M_loss: 0.5838, M_d_loss: 0.0411, M_r_loss: 542.6366\n",
      "Epoch [221/250], step [1/244], D_loss: 0.0580, M_loss: 0.6066, M_d_loss: 0.0453, M_r_loss: 561.3300\n",
      "Epoch [221/250], step [41/244], D_loss: 0.0559, M_loss: 0.5827, M_d_loss: 0.0417, M_r_loss: 541.0018\n",
      "Epoch [221/250], step [81/244], D_loss: 0.0484, M_loss: 0.6333, M_d_loss: 0.0456, M_r_loss: 587.6709\n",
      "Epoch [221/250], step [121/244], D_loss: 0.0584, M_loss: 0.5863, M_d_loss: 0.0441, M_r_loss: 542.1959\n",
      "Epoch [221/250], step [161/244], D_loss: 0.0707, M_loss: 0.6039, M_d_loss: 0.0440, M_r_loss: 559.8789\n",
      "Epoch [221/250], step [201/244], D_loss: 0.0669, M_loss: 0.5685, M_d_loss: 0.0413, M_r_loss: 527.2349\n",
      "Epoch [221/250], step [241/244], D_loss: 0.0669, M_loss: 0.5736, M_d_loss: 0.0404, M_r_loss: 533.1464\n",
      "Epoch [222/250], step [1/244], D_loss: 0.0649, M_loss: 0.5788, M_d_loss: 0.0431, M_r_loss: 535.7209\n",
      "Epoch [222/250], step [41/244], D_loss: 0.0525, M_loss: 0.5683, M_d_loss: 0.0428, M_r_loss: 525.5511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [222/250], step [81/244], D_loss: 0.0490, M_loss: 0.5852, M_d_loss: 0.0434, M_r_loss: 541.7753\n",
      "Epoch [222/250], step [121/244], D_loss: 0.0504, M_loss: 0.5855, M_d_loss: 0.0438, M_r_loss: 541.7225\n",
      "Epoch [222/250], step [161/244], D_loss: 0.0547, M_loss: 0.5872, M_d_loss: 0.0403, M_r_loss: 546.9265\n",
      "Epoch [222/250], step [201/244], D_loss: 0.0536, M_loss: 0.5889, M_d_loss: 0.0398, M_r_loss: 549.1802\n",
      "Epoch [222/250], step [241/244], D_loss: 0.0635, M_loss: 0.5919, M_d_loss: 0.0375, M_r_loss: 554.3319\n",
      "Epoch [223/250], step [1/244], D_loss: 0.0667, M_loss: 0.5524, M_d_loss: 0.0411, M_r_loss: 511.2664\n",
      "Epoch [223/250], step [41/244], D_loss: 0.0653, M_loss: 0.5792, M_d_loss: 0.0401, M_r_loss: 539.0455\n",
      "Epoch [223/250], step [81/244], D_loss: 0.0600, M_loss: 0.5939, M_d_loss: 0.0429, M_r_loss: 550.9951\n",
      "Epoch [223/250], step [121/244], D_loss: 0.0577, M_loss: 0.5908, M_d_loss: 0.0425, M_r_loss: 548.3073\n",
      "Epoch [223/250], step [161/244], D_loss: 0.0493, M_loss: 0.5964, M_d_loss: 0.0451, M_r_loss: 551.3203\n",
      "Epoch [223/250], step [201/244], D_loss: 0.0506, M_loss: 0.5846, M_d_loss: 0.0426, M_r_loss: 541.9730\n",
      "Epoch [223/250], step [241/244], D_loss: 0.0490, M_loss: 0.5820, M_d_loss: 0.0434, M_r_loss: 538.6218\n",
      "Epoch [224/250], step [1/244], D_loss: 0.0514, M_loss: 0.5947, M_d_loss: 0.0417, M_r_loss: 552.9780\n",
      "Epoch [224/250], step [41/244], D_loss: 0.0517, M_loss: 0.5875, M_d_loss: 0.0394, M_r_loss: 548.0354\n",
      "Epoch [224/250], step [81/244], D_loss: 0.0568, M_loss: 0.5757, M_d_loss: 0.0406, M_r_loss: 535.0566\n",
      "Epoch [224/250], step [121/244], D_loss: 0.0607, M_loss: 0.5947, M_d_loss: 0.0415, M_r_loss: 553.1941\n",
      "Epoch [224/250], step [161/244], D_loss: 0.0557, M_loss: 0.5703, M_d_loss: 0.0434, M_r_loss: 526.9282\n",
      "Epoch [224/250], step [201/244], D_loss: 0.0536, M_loss: 0.6244, M_d_loss: 0.0459, M_r_loss: 578.5245\n",
      "Epoch [224/250], step [241/244], D_loss: 0.0607, M_loss: 0.6528, M_d_loss: 0.0486, M_r_loss: 604.1240\n",
      "Epoch [225/250], step [1/244], D_loss: 0.0616, M_loss: 0.5921, M_d_loss: 0.0486, M_r_loss: 543.5053\n",
      "Epoch [225/250], step [41/244], D_loss: 0.0653, M_loss: 0.5970, M_d_loss: 0.0446, M_r_loss: 552.4370\n",
      "Epoch [225/250], step [81/244], D_loss: 0.0552, M_loss: 0.6263, M_d_loss: 0.0503, M_r_loss: 576.0447\n",
      "Epoch [225/250], step [121/244], D_loss: 0.0579, M_loss: 0.5896, M_d_loss: 0.0453, M_r_loss: 544.3238\n",
      "Epoch [225/250], step [161/244], D_loss: 0.0564, M_loss: 0.5717, M_d_loss: 0.0449, M_r_loss: 526.7522\n",
      "Epoch [225/250], step [201/244], D_loss: 0.0560, M_loss: 0.5745, M_d_loss: 0.0431, M_r_loss: 531.4558\n",
      "Epoch [225/250], step [241/244], D_loss: 0.0493, M_loss: 0.5672, M_d_loss: 0.0388, M_r_loss: 528.4131\n",
      "Epoch [226/250], step [1/244], D_loss: 0.0479, M_loss: 0.6208, M_d_loss: 0.0444, M_r_loss: 576.4448\n",
      "Epoch [226/250], step [41/244], D_loss: 0.0547, M_loss: 0.5683, M_d_loss: 0.0381, M_r_loss: 530.1948\n",
      "Epoch [226/250], step [81/244], D_loss: 0.0481, M_loss: 0.6069, M_d_loss: 0.0419, M_r_loss: 565.0177\n",
      "Epoch [226/250], step [121/244], D_loss: 0.0467, M_loss: 0.5675, M_d_loss: 0.0431, M_r_loss: 524.4680\n",
      "Epoch [226/250], step [161/244], D_loss: 0.0534, M_loss: 0.5699, M_d_loss: 0.0426, M_r_loss: 527.2789\n",
      "Epoch [226/250], step [201/244], D_loss: 0.0561, M_loss: 0.5891, M_d_loss: 0.0423, M_r_loss: 546.8033\n",
      "Epoch [226/250], step [241/244], D_loss: 0.0505, M_loss: 0.6348, M_d_loss: 0.0485, M_r_loss: 586.3881\n",
      "Epoch [227/250], step [1/244], D_loss: 0.0537, M_loss: 0.5960, M_d_loss: 0.0478, M_r_loss: 548.1808\n",
      "Epoch [227/250], step [41/244], D_loss: 0.0653, M_loss: 0.6221, M_d_loss: 0.0412, M_r_loss: 580.8558\n",
      "Epoch [227/250], step [81/244], D_loss: 0.0626, M_loss: 0.6049, M_d_loss: 0.0418, M_r_loss: 563.0067\n",
      "Epoch [227/250], step [121/244], D_loss: 0.0585, M_loss: 0.6480, M_d_loss: 0.0461, M_r_loss: 601.8556\n",
      "Epoch [227/250], step [161/244], D_loss: 0.0558, M_loss: 0.5855, M_d_loss: 0.0433, M_r_loss: 542.1983\n",
      "Epoch [227/250], step [201/244], D_loss: 0.0546, M_loss: 0.5961, M_d_loss: 0.0406, M_r_loss: 555.5581\n",
      "Epoch [227/250], step [241/244], D_loss: 0.0527, M_loss: 0.5552, M_d_loss: 0.0384, M_r_loss: 516.7907\n",
      "Epoch [228/250], step [1/244], D_loss: 0.0539, M_loss: 0.5639, M_d_loss: 0.0399, M_r_loss: 523.9635\n",
      "Epoch [228/250], step [41/244], D_loss: 0.0493, M_loss: 0.6049, M_d_loss: 0.0401, M_r_loss: 564.7989\n",
      "Epoch [228/250], step [81/244], D_loss: 0.0456, M_loss: 0.5813, M_d_loss: 0.0388, M_r_loss: 542.4688\n",
      "Epoch [228/250], step [121/244], D_loss: 0.0406, M_loss: 0.6003, M_d_loss: 0.0424, M_r_loss: 557.8477\n",
      "Epoch [228/250], step [161/244], D_loss: 0.0436, M_loss: 0.5526, M_d_loss: 0.0393, M_r_loss: 513.2572\n",
      "Epoch [228/250], step [201/244], D_loss: 0.0465, M_loss: 0.5781, M_d_loss: 0.0405, M_r_loss: 537.6609\n",
      "Epoch [228/250], step [241/244], D_loss: 0.0491, M_loss: 0.6200, M_d_loss: 0.0396, M_r_loss: 580.3674\n",
      "Epoch [229/250], step [1/244], D_loss: 0.0558, M_loss: 0.6041, M_d_loss: 0.0437, M_r_loss: 560.3740\n",
      "Epoch [229/250], step [41/244], D_loss: 0.0524, M_loss: 0.5797, M_d_loss: 0.0402, M_r_loss: 539.5184\n",
      "Epoch [229/250], step [81/244], D_loss: 0.0541, M_loss: 0.5918, M_d_loss: 0.0405, M_r_loss: 551.3085\n",
      "Epoch [229/250], step [121/244], D_loss: 0.0536, M_loss: 0.5789, M_d_loss: 0.0419, M_r_loss: 536.9391\n",
      "Epoch [229/250], step [161/244], D_loss: 0.0504, M_loss: 0.6099, M_d_loss: 0.0426, M_r_loss: 567.2912\n",
      "Epoch [229/250], step [201/244], D_loss: 0.0546, M_loss: 0.5803, M_d_loss: 0.0420, M_r_loss: 538.3137\n",
      "Epoch [229/250], step [241/244], D_loss: 0.0555, M_loss: 0.6070, M_d_loss: 0.0402, M_r_loss: 566.8575\n",
      "Epoch [230/250], step [1/244], D_loss: 0.0620, M_loss: 0.5814, M_d_loss: 0.0425, M_r_loss: 538.9486\n",
      "Epoch [230/250], step [41/244], D_loss: 0.0510, M_loss: 0.5846, M_d_loss: 0.0441, M_r_loss: 540.5079\n",
      "Epoch [230/250], step [81/244], D_loss: 0.0460, M_loss: 0.6079, M_d_loss: 0.0428, M_r_loss: 565.1033\n",
      "Epoch [230/250], step [121/244], D_loss: 0.0477, M_loss: 0.6220, M_d_loss: 0.0488, M_r_loss: 573.1172\n",
      "Epoch [230/250], step [161/244], D_loss: 0.0572, M_loss: 0.5913, M_d_loss: 0.0447, M_r_loss: 546.5585\n",
      "Epoch [230/250], step [201/244], D_loss: 0.0650, M_loss: 0.6122, M_d_loss: 0.0447, M_r_loss: 567.4629\n",
      "Epoch [230/250], step [241/244], D_loss: 0.0556, M_loss: 0.5982, M_d_loss: 0.0436, M_r_loss: 554.6588\n",
      "Epoch [231/250], step [1/244], D_loss: 0.0616, M_loss: 0.5750, M_d_loss: 0.0436, M_r_loss: 531.3730\n",
      "Epoch [231/250], step [41/244], D_loss: 0.0517, M_loss: 0.5940, M_d_loss: 0.0449, M_r_loss: 549.0648\n",
      "Epoch [231/250], step [81/244], D_loss: 0.0539, M_loss: 0.6034, M_d_loss: 0.0459, M_r_loss: 557.4476\n",
      "Epoch [231/250], step [121/244], D_loss: 0.0596, M_loss: 0.5915, M_d_loss: 0.0420, M_r_loss: 549.5337\n",
      "Epoch [231/250], step [161/244], D_loss: 0.0570, M_loss: 0.5884, M_d_loss: 0.0415, M_r_loss: 546.8475\n",
      "Epoch [231/250], step [201/244], D_loss: 0.0521, M_loss: 0.5950, M_d_loss: 0.0458, M_r_loss: 549.2811\n",
      "Epoch [231/250], step [241/244], D_loss: 0.0511, M_loss: 0.6222, M_d_loss: 0.0433, M_r_loss: 578.9150\n",
      "Epoch [232/250], step [1/244], D_loss: 0.0557, M_loss: 0.5886, M_d_loss: 0.0410, M_r_loss: 547.6204\n",
      "Epoch [232/250], step [41/244], D_loss: 0.0484, M_loss: 0.5460, M_d_loss: 0.0408, M_r_loss: 505.1925\n",
      "Epoch [232/250], step [81/244], D_loss: 0.0506, M_loss: 0.5795, M_d_loss: 0.0430, M_r_loss: 536.4788\n",
      "Epoch [232/250], step [121/244], D_loss: 0.0454, M_loss: 0.6125, M_d_loss: 0.0480, M_r_loss: 564.5429\n",
      "Epoch [232/250], step [161/244], D_loss: 0.0498, M_loss: 0.6108, M_d_loss: 0.0436, M_r_loss: 567.1863\n",
      "Epoch [232/250], step [201/244], D_loss: 0.0606, M_loss: 0.5756, M_d_loss: 0.0422, M_r_loss: 533.3915\n",
      "Epoch [232/250], step [241/244], D_loss: 0.0562, M_loss: 0.6353, M_d_loss: 0.0492, M_r_loss: 586.1213\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 23.70 GiB total capacity; 15.25 GiB already allocated; 6.00 MiB free; 16.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     69\u001b[0m             z_mean \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(scvi_model\u001b[38;5;241m.\u001b[39mget_latent_representation())\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 70\u001b[0m             z_mean_transferred \u001b[38;5;241m=\u001b[39m \u001b[43mmapping_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz_mean\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m             z_record[epoch] \u001b[38;5;241m=\u001b[39m z_mean_transferred\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m#             break     \u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cell2loc_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mTransferMappingModule.forward\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, z):\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_transfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mTransferMappingModule.query_transfer\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquery_transfer\u001b[39m(\u001b[38;5;28mself\u001b[39m, z):\n\u001b[0;32m---> 27\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc11\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(h)\n\u001b[1;32m     29\u001b[0m     h \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc12(h))\n",
      "File \u001b[0;32m~/anaconda3/envs/cell2loc_env/lib/python3.9/site-packages/torch/nn/functional.py:1442\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1440\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1442\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 23.70 GiB total capacity; 15.25 GiB already allocated; 6.00 MiB free; 16.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "reconst_loss_rate = 0.001\n",
    "num_batches = len(train_loader)\n",
    "\n",
    "mapping_module.train()\n",
    "discriminator.train()\n",
    "\n",
    "z_record = {}\n",
    "for epoch in range(num_epoch):\n",
    "    # ref_latent to GPU\n",
    "    ref_latent_subset = sc.pp.subsample(ref_latent, n_obs = 10000, \n",
    "                                        random_state = epoch, copy=True)\n",
    "    ref_latent_tensor = torch.tensor(ref_latent_subset.X).to(device)\n",
    "    \n",
    "    for i, x in enumerate(train_loader):\n",
    "        query_x = x.to(device)\n",
    "        # encode and transfer\n",
    "        n_sample = query_x.shape[0]\n",
    "        batch_index = torch.full((n_sample, 1), batch_id)\n",
    "        inference_output = scvi_model.module.inference(\n",
    "            query_x, \n",
    "            batch_index=batch_index\n",
    "        )\n",
    "        query_latent = torch.squeeze(inference_output['z'])\n",
    "        query_latent_transferred = mapping_module(query_latent)\n",
    "        \n",
    "        ##############################\n",
    "        #   Training discriminator   #\n",
    "        ##############################\n",
    "        # forward\n",
    "        lossD = discriminator_loss(\n",
    "            discriminator, \n",
    "            ref_latent_tensor, \n",
    "            query_latent_transferred,\n",
    "            \"trainD\"\n",
    "        )\n",
    "        # backward\n",
    "        optimizerD.zero_grad()\n",
    "        lossD.backward()\n",
    "#         lossD.backward(retain_graph=True)\n",
    "        optimizerD.step()         \n",
    "        \n",
    "#         ##########################\n",
    "#         #   Training generator   #\n",
    "#         ##########################\n",
    "        inference_output = scvi_model.module.inference(\n",
    "            query_x, \n",
    "            batch_index=batch_index\n",
    "        )\n",
    "        query_latent = torch.squeeze(inference_output['z'])\n",
    "        query_latent_transferred = mapping_module(query_latent)\n",
    "        \n",
    "        # forward\n",
    "        discrim_loss = discriminator_loss(\n",
    "            discriminator, \n",
    "            ref_latent_tensor, \n",
    "            query_latent_transferred,\n",
    "            \"trainM\"\n",
    "        )\n",
    "        reconst_loss = reconstruction_loss(scvi_model, mapping_module, \n",
    "                                           torch.squeeze(query_x), batch_id)\n",
    "        lossM = reconst_loss_rate * reconst_loss + discrim_loss\n",
    "        \n",
    "        # backward\n",
    "        optimizerM.zero_grad()\n",
    "        lossM.backward()\n",
    "        optimizerM.step()\n",
    "        \n",
    "        if i == 0:\n",
    "            z_mean = torch.tensor(scvi_model.get_latent_representation()).to(device)\n",
    "            z_mean_transferred = mapping_module(z_mean)\n",
    "            z_record[epoch] = z_mean_transferred\n",
    "#             break     \n",
    "\n",
    "        if i%40 == 0:\n",
    "#             print(lossD.item())\n",
    "            print('Epoch [{}/{}], step [{}/{}], D_loss: {:.4f}, M_loss: {:.4f}, M_d_loss: {:.4f}, M_r_loss: {:.4f}'.format(\n",
    "                epoch+1, num_epoch, i+1, num_batches, \n",
    "                lossD.item(), lossM.item(), \n",
    "                discrim_loss.item(), reconst_loss.item()))\n",
    "    schedulerD.step()\n",
    "    schedulerM.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02f3a735",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './saves'\n",
    "ad = sc.AnnData(scvi_model.get_latent_representation())\n",
    "ad.obs[\"cell_type\"] = query_adata.obs[\"level1\"].tolist()\n",
    "ad.obs[\"batch\"] = query_adata.obs[\"batch\"].tolist()\n",
    "ad.raw = ad\n",
    "ad.write(save_path + \"/ad_sample_ramachandran_origin.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d0867ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wyh/anaconda3/envs/cell2loc_env/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:371: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.000625]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schedulerM.get_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c6becc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "model_path = './model'\n",
    "# os.makedirs(model_path, exist_ok=True)\n",
    "torch.save(discriminator.state_dict(), model_path + \"/sample_ramachandran_230.pth\")\n",
    "torch.save(mapping_module.state_dict(), model_path + \"/sample_ramachandran_230.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7dec68a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './saves'\n",
    "ad = sc.AnnData(z_record[0].detach().cpu().numpy())\n",
    "ad.obs[\"cell_type\"] = query_adata.obs[\"level1\"].tolist()\n",
    "ad.obs[\"batch\"] = query_adata.obs[\"batch\"].tolist()\n",
    "ad.raw = ad\n",
    "ad.write(save_path + \"/ad_sample_ramachandran_epo00.h5ad\")\n",
    "\n",
    "save_path = './saves'\n",
    "ad = sc.AnnData(z_record[99].detach().cpu().numpy())\n",
    "ad.obs[\"cell_type\"] = query_adata.obs[\"level1\"].tolist()\n",
    "ad.obs[\"batch\"] = query_adata.obs[\"batch\"].tolist()\n",
    "ad.raw = ad\n",
    "ad.write(save_path + \"/ad_sample_ramachandran_epo99.h5ad\")\n",
    "\n",
    "# ad = sc.AnnData(z_record[249].detach().cpu().numpy())\n",
    "ad = sc.AnnData(z_record[229].detach().cpu().numpy())\n",
    "ad.obs[\"cell_type\"] = query_adata.obs[\"level1\"].tolist()\n",
    "ad.obs[\"batch\"] = query_adata.obs[\"batch\"].tolist()\n",
    "ad.raw = ad\n",
    "ad.write(save_path + \"/ad_sample_ramachandran.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133ac8f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f198e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wyh/anaconda3/envs/cell2loc_env/lib/python3.9/site-packages/scvi/model/base/_base_model.py:150: UserWarning: Make sure the registered X field in anndata contains unnormalized count data.\n",
      "  warnings.warn(\n",
      "/home/wyh/anaconda3/envs/cell2loc_env/lib/python3.9/site-packages/scvi/model/base/_base_model.py:150: UserWarning: Make sure the registered X field in anndata contains unnormalized count data.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "save_path = './saves'\n",
    "# os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "reference_latent1 = sc.AnnData(scvi_model.get_latent_representation())\n",
    "reference_latent1.obs[\"cell_type\"] = query_adata.obs[\"level1\"].tolist()\n",
    "reference_latent1.obs[\"batch\"] = query_adata.obs[\"batch\"].tolist()\n",
    "reference_latent1.write(save_path + \"/latent_origin2.h5ad\")\n",
    "\n",
    "z_mean = torch.tensor(scvi_model.get_latent_representation()).to(device)\n",
    "z_mean_transferred = mapping_module(z_mean)\n",
    "reference_latent = sc.AnnData(z_mean_transferred.detach().cpu().numpy())\n",
    "reference_latent.obs[\"cell_type\"] = query_adata.obs[\"level1\"].tolist()\n",
    "reference_latent.obs[\"batch\"] = query_adata.obs[\"batch\"].tolist()\n",
    "reference_latent.write(save_path + \"/latent_transferred2.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad5bdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "save_path = './saves'\n",
    "query_latent_origin = sc.read_h5ad(save_path + \"/latent_origin2.h5ad\")\n",
    "query_latent_transferred = sc.read_h5ad(save_path + \"/latent_transferred2.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0b4331",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_latent = sc.read_h5ad(\"./reference_latent.h5ad\")\n",
    "sc.pp.subsample(ref_latent, n_obs = 10000)\n",
    "merged_adata = sc.AnnData.concatenate(ref_latent, query_latent_transferred,\n",
    "                                      batch_key=\"ref_query\")\n",
    "merged_adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ba5a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.neighbors(merged_adata, n_neighbors=4)\n",
    "sc.tl.umap(merged_adata)\n",
    "sc.pl.umap(merged_adata,\n",
    "           color=['cell_type', \"ref_query\"],\n",
    "           frameon=False,\n",
    "           wspace=0.6,\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79e5b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_adata0 = sc.AnnData.concatenate(ref_latent, query_latent_origin,\n",
    "                                      batch_key = \"ref_query\")\n",
    "\n",
    "sc.pp.neighbors(merged_adata0, n_neighbors=4)\n",
    "sc.tl.umap(merged_adata0)\n",
    "sc.pl.umap(merged_adata0,\n",
    "           color=['cell_type', \"ref_query\"],\n",
    "           frameon=False,\n",
    "           wspace=0.6,\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3403aae5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cell2loc",
   "language": "python",
   "name": "cell2loc_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
